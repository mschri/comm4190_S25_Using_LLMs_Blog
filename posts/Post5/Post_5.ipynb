{
 "cells": [
  {
   "cell_type": "raw",
   "id": "58448432-af2f-449e-8a46-49d5f8c8a3dd",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Fill in the Blank\"\n",
    "description: \"Testing Probabilistic Text Generation\"\n",
    "author: \"An LLM User\"\n",
    "date: \"2/17/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - probability\n",
    "  - embedding\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4300fa8-38a4-40b2-9b86-b6a973d530d8",
   "metadata": {},
   "source": [
    "## Probabilistic Text Generation (O'Reilly):\n",
    "\n",
    "A significant feature that drives LLM text generation is its ability to pick out patterns of text input (prompts you feed it) and provide text output according to those patterns. It does so through a process of assigning probabilities to certain words within texts and according to the data it's trained on, and spits out text that appears most likely. So in a process of election, each word in a sentence \"casts votes on the importance of other words for its meaning\" (Pheonix & Taylor, Chapter 2).\n",
    "\n",
    "> For reference: Phoenix, J. & Taylor, M.(2024) Prompt Engineering for Generative AI. Oâ€™Reilly Media, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3fe8d-9edb-48f9-b0f0-5d110e3c96ec",
   "metadata": {},
   "source": [
    "### Relatedly: Numerical Representations of Language:\n",
    "\n",
    "Further, word embeddings describe words (represented as word vectors) situated close to each other (within a space/plane), allowing the model to calculate and \"understand\" the relationship between these two words and hence its ability to follow along an input's context, syntax, and other broader language relationship features.\n",
    "\n",
    "It can be pictured as such (also referencing the O'Reilly textbook on: Prompt Engineering for Generative AI):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6cd6f-27bc-4d4b-9b00-9a22cb417da9",
   "metadata": {},
   "source": [
    "<img src=\"pic1.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6fae2-ba75-4f1b-8fc8-e0bbfecc357d",
   "metadata": {},
   "source": [
    "## Testing LLMs... \n",
    "### (model: anthropic/claude-3.5-sonnet)\n",
    "\n",
    "This led to a small experiment that tests the language model's ability to process certain texts/prompts, particularly when maybe they don't follow typical conventions.\n",
    "\n",
    "***One experiment:*** testing its probabilistic text feature to finish sentences with a particular rhyming structure:\n",
    "\n",
    "> We can see here it does a good job of completing the sentences according to the patterns within each text input (It guessed all of the words correctly that I had in mind for completing these sentences!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aedf350-9708-4e7a-a3cd-47be5810f2fe",
   "metadata": {},
   "source": [
    "<img src=\"Pic2.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d1a20-67c2-4c54-8595-c07d23ca7a68",
   "metadata": {},
   "source": [
    "### **Interestingly**\n",
    "the system inititally fails to guess correctly when colloquialisms are expected to finish a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e602129-ea39-4618-9a1b-620970d5f6f2",
   "metadata": {},
   "source": [
    "<img src=\"pic3.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b015ac-c17a-4bfa-9948-8c512f7ec32e",
   "metadata": {},
   "source": [
    "* **Or at least...** it was not its first guess to finish the sentence with a slang term. What's also interesting is that it chose to maintain strict/perfect rhyming patterns by default, and not neccessarily look to creating or maintaining assonance (as it notes upon correction in its response)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54a3df-2a21-4185-8b35-90772662d4ef",
   "metadata": {},
   "source": [
    "***A \"second\" experiment:*** examining how it learns from your rhyming patterns/speech tendencies based on the context of your previous prompts and inputs.\n",
    "* it assigns new probabilities to words based on previous inputs you give it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792da614-d627-4a46-bcc6-a833c686e70c",
   "metadata": {},
   "source": [
    "<img src=\"pic4.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41dcbe-6b0f-4c01-abd3-7cc4e4aa6590",
   "metadata": {},
   "source": [
    "* Still had a little bit of trouble getting exactly the word in mind, but was definitely on the right track!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b001c7-1b9a-4fa3-9244-c08c713c0b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfbca80b-714b-43df-9358-ccff326c386a",
   "metadata": {},
   "source": [
    "#### Another example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91570877-7d6a-4de3-bc50-e3e093f1e3b4",
   "metadata": {},
   "source": [
    "<img src=\"pic5.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac268b-d9eb-4065-b531-240c4b174a0f",
   "metadata": {},
   "source": [
    "* Here it got the question with ease (granted, the slang term is well-established/less obscure than the previous examples...); let's try to see if the model can predict a slang term that is more recent (from 2024-25):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e16c7-e256-4096-b990-6bc226171628",
   "metadata": {},
   "source": [
    "#### A third example: New slang term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33122704-7379-418b-a41d-fb4b0da0e22f",
   "metadata": {},
   "source": [
    "<img src=\"pic6.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff44ea-8ff8-431c-9c74-4f07d502fa45",
   "metadata": {},
   "source": [
    "* It picked up on the slang usage!\n",
    "\n",
    "> This shows just how adaptable LLMs are, and how it learns to pick up modern language particularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3be086-6cdf-4a9a-9fc0-60cbd88bdc16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
