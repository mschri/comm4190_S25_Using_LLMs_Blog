{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f024ae46-0599-4e2b-ad1b-4e7908949afb",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"How to Get the Best Response\"\n",
    "description: \"Prompt Engineering & Response Optimization\"\n",
    "author: \"An LLM User\"\n",
    "date: \"2/10/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - prompt engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089e007-ad98-47b5-82bc-5a1ff8653780",
   "metadata": {},
   "source": [
    "## Prompting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68048e87-264c-47fc-848b-5dc80220265f",
   "metadata": {},
   "source": [
    "AI has obviously come a long way in being able to generate accurate responses that fit the bill in terms of what the user is looking for. And, ceratinly, its responses can become even more complex and thorough, depending on just how extensive the user wants the answer to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd379db4-83bb-48c7-9a6f-9fa8022b74ae",
   "metadata": {},
   "source": [
    "<img src=\"Pic1.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4ac8f-8cd4-46c4-8378-1ed0f52a423b",
   "metadata": {},
   "source": [
    "Prompts like the one above (\"give me a grocery list\") provide enough detail and context for the model to spit out an answer in line with what a typical response might be, and sounds quite convincing to the average user. But what if you're alergic to, say, bell peppers? Well it wouldn't have the ability to know unless you specfied of course what it is you can't eat. And while it appers to be mostly magic in how much it can guestimate in order to generate accurate responses, it can only go so far unless provided the right context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c0cdf-6504-4736-a5d9-a756a0d84f80",
   "metadata": {},
   "source": [
    "<img src=\"pic2.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb6350-0656-4045-b2db-ff6990a2eb19",
   "metadata": {},
   "source": [
    "## Taking \"guesses\" based on context\n",
    "\n",
    "As the AI is fed more and more context, the responses become more and more aligned with what you're looking for. As per the image above, you can't just ask it to guess what you're allergic to (in this case, we decided bell peppers), because it would have no way of knowing otherwise, unless you provided it some sort of context or information (which it doesn't have, prior to you prompting it this request)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda8798-25f0-4867-be1d-21238e637a94",
   "metadata": {},
   "source": [
    "* Making it take guesses (by giving it some clues)\n",
    "\n",
    "> what is something you could tell the model to narrow its guesses, without outright telling the model?\n",
    "> Ex: I have an uncommon allergy; here's what I'm NOT allergic to, etc...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05535530-1fb6-4af8-a36a-6676b2ee5429",
   "metadata": {},
   "source": [
    "<img src=\"pic3.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0a805-e7ab-43ea-9ba8-efdabac8a0c8",
   "metadata": {},
   "source": [
    "> Still \"off the mark,\" as it puts it...but a step closer; it took guesses based on what are some common allergies, indicating both the power of context & data collection (from the internet), to generate more accurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9d851-9a6e-4e6f-abc6-172615ef6378",
   "metadata": {},
   "source": [
    "<img src=\"pic4.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da2448-0f31-4e54-aee2-174a9876e7e6",
   "metadata": {},
   "source": [
    "* You can continue offering more refined requests and prompts to further its guessing ability, i.e...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830e804-de4c-4c21-bb31-a85c18c19551",
   "metadata": {},
   "source": [
    "<img src=\"pic5.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00b3d9-4757-43b3-9343-3bd796e60036",
   "metadata": {},
   "source": [
    "### Okay...\n",
    "So it's still off...of course, models can only go far in their guesses! And depending on the model, different/successive ones will be more or less successful than the last. Further, depending on the data it's trained on (again, in this case it's the Internet), it will pull responses that are more probalistic or common than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d6daf-5943-4a3e-b29d-c91534e71754",
   "metadata": {},
   "source": [
    "<img src=\"pic6.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2ec076-0b6c-4088-979f-c0032c40961b",
   "metadata": {},
   "source": [
    "As shown above, it can only go so far depending on the context it's given! So how do we improve responses such that answers are more accurate off the bat? Enter here: prompt engineering, and it's five main associated principles:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92122fd2-7fb9-41b2-8bf0-1ba8f3e5351b",
   "metadata": {},
   "source": [
    "* **The Five Principles of Prompting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ee6f3-b6cc-4c2c-91b7-1874441d1fbe",
   "metadata": {},
   "source": [
    "> Give direction (describe desired style)\n",
    "\n",
    "> Specify format (define what rules to follow)\n",
    "\n",
    "> Provide examples (include a diverse set of cases)\n",
    "\n",
    "> Evaluate quality (identify errors)\n",
    "\n",
    "> Divide labor (split tasks into mulitple steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0236ae-66b8-4802-a551-b32a22232544",
   "metadata": {},
   "source": [
    "### Example: Divide labor\n",
    "\n",
    "This principle refers to the process by which you break up your prompt into several \"mini prompts\" or requests with more specificity. By chaining mulitple calls togther, it communicates more precisely the intent of your message and makes the responses more deterministic, and less elusive to the heart of the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73b604-0e75-4aae-956d-3612f3ef6b2b",
   "metadata": {},
   "source": [
    "* The goal is to prompt the model and have it list out its steps (**chain of thought reasoning**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f860fc-d817-4d49-beb2-0259db74689d",
   "metadata": {},
   "source": [
    "<img src=\"pic7.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475122c1-f2f6-46ad-8d9b-d8830891cb1a",
   "metadata": {},
   "source": [
    "* You then might ask the model to generate a new list based on this critera!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8399358-2644-4e88-b00f-1d91c45ffd30",
   "metadata": {},
   "source": [
    "<img src=\"pic8.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7941d-feab-46c8-bf46-e535e57c0242",
   "metadata": {},
   "source": [
    "> Noticed how it modified the list based on these new, more refined requests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03fb5a-1b85-4783-9593-87fa4e4d9121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
