<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Miranda Schriver">
<meta name="dcterms.date" content="2025-03-17">
<meta name="description" content="Testing the effects of priming on ethical responses of LLMs">

<title>Is ChatGPT Utilitarian or Deontological? – Experimenting with LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-1db3f541d99483cdc7a0d9cf38862b2c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Experimenting with LLMs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About Miranda’s Page</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Is ChatGPT Utilitarian or Deontological?</h1>
                  <div>
        <div class="description">
          Testing the effects of priming on ethical responses of LLMs
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">LLMs</div>
                <div class="quarto-category">ethics</div>
                <div class="quarto-category">prompting</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Miranda Schriver </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 17, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><img src="img1.png" width="80%"></p>
<section id="revisiting-priming" class="level2">
<h2 class="anchored" data-anchor-id="revisiting-priming">Revisiting Priming</h2>
<p>In a previous blog of mine, I examined some of the effects of priming on ChatGPT by examining how earlier responses and prompts in a conversation with the model would affect successive outputs. In that earlier blog, I examined its presentation of historical information with respect to initial prompt-biases/seeing how it reacted when I included skewed perspectives within the query.</p>
<p>Today’s exercise will revisit this topic, and expand further on the effects of priming within responses to ethical dilemmas by LLMs. Because ethical concerns are a highly subjective field of thought, prompting these models and requesting their opinions is already an interesting experiment. Because the model is pulling from sources across the internet which represents a variety of opinions and beliefs, it’s fascinating to see how the model answers morally ambiguous questions with no hard answer given that the information it’s trained on has no hard answer itself. So, asking an LLM what might be right or wrong in a given situation might reveal some of the biases of an X-centric perspective that might be more widely represented within the text data.</p>
</section>
<section id="trolley-dilemma" class="level2">
<h2 class="anchored" data-anchor-id="trolley-dilemma">Trolley Dilemma</h2>
<p>One of the most fundamental hypotheticals in the philosophical realm is that of the trolley case–pull the lever and kill many, or leave the train running on its tracks and kill one?</p>
<p>Almost everything comes with its own moral tradeoffs, but this case represents one of the most famous forms of ambiguity, and illustrates several modes of thinking depending on your answer to the question and reasoning behind your answer. Among some of the most well-known perspectives associated with gut-reactions to the trolley dilemma include the <strong>utilitarian approach</strong>, <strong>deontological</strong> thinking, and the <strong>virtue-ethics</strong> consideration.</p>
<p>With that, how might ChatGPT respond to the classic case? To gauge it’s default reaction to the question, the model was prompted with the following:</p>
<pre><code>How would you resolve the trolley problem?</code></pre>
<p>According to Chat, it’s answer is this:</p>
<p>The trolley problem is a classic ethical dilemma that challenges us to make tough decisions about morality, consequences, and values. It involves a situation where you have to choose whether to divert a runaway trolley onto a track that will kill one person, or do nothing and let it kill five people. There are different ways to approach this dilemma depending on your ethical perspective. Here’s a breakdown of some possible approaches:</p>
<section id="utilitarianism-maximizing-well-being" class="level3">
<h3 class="anchored" data-anchor-id="utilitarianism-maximizing-well-being">1. <strong>Utilitarianism</strong> (Maximizing Well-Being)</h3>
<ul>
<li><strong>Decision</strong>: Pull the lever to divert the trolley onto the track with one person, as this minimizes the overall harm.</li>
<li><strong>Reasoning</strong>: From a utilitarian standpoint, you would choose the action that maximizes the greatest good for the greatest number. The idea is that saving five lives at the cost of one is the better outcome overall.</li>
</ul>
</section>
<section id="deontology-duty-based-ethics" class="level3">
<h3 class="anchored" data-anchor-id="deontology-duty-based-ethics">2. <strong>Deontology</strong> (Duty-Based Ethics)</h3>
<ul>
<li><strong>Decision</strong>: Do not pull the lever.</li>
<li><strong>Reasoning</strong>: According to deontological ethics, some actions are morally wrong regardless of the consequences. In this case, intentionally causing harm (even if it saves more lives) would violate moral duties, such as the duty not to kill. Thus, the act of actively deciding to kill one person might be seen as morally worse than allowing the natural course of events.</li>
</ul>
</section>
<section id="virtue-ethics-character-and-intentions" class="level3">
<h3 class="anchored" data-anchor-id="virtue-ethics-character-and-intentions">3. <strong>Virtue Ethics</strong> (Character and Intentions)</h3>
<ul>
<li><strong>Decision</strong>: The decision would depend on what a virtuous person would do in this situation.</li>
<li><strong>Reasoning</strong>: A virtue ethicist might focus on the character traits that guide the decision, such as compassion, wisdom, and courage. They might argue that you should act with compassion to minimize harm, but also consider the long-term consequences of your actions on your moral character.</li>
</ul>
</section>
<section id="care-ethics-relational-and-contextual" class="level3">
<h3 class="anchored" data-anchor-id="care-ethics-relational-and-contextual">4. <strong>Care Ethics</strong> (Relational and Contextual)</h3>
<ul>
<li><strong>Decision</strong>: This approach might be more focused on the relationships involved.</li>
<li><strong>Reasoning</strong>: A care ethicist might argue that relationships and the context of the situation matter. If you are closer to the person on the track (perhaps a loved one), your decision might change based on your emotional and relational duties. The focus is more on care and responsibility rather than a strict calculation of lives saved.</li>
</ul>
</section>
<section id="the-no-action-option" class="level3">
<h3 class="anchored" data-anchor-id="the-no-action-option">5. <strong>The “No Action” Option</strong></h3>
<ul>
<li><strong>Decision</strong>: Do nothing and let the trolley continue on its path.</li>
<li><strong>Reasoning</strong>: Some people argue that inaction in situations like this is the more ethical choice. This stance believes that it’s better to not intervene at all, as interfering could be seen as making a moral decision to take a life, which may not be justified.</li>
</ul>
</section>
<section id="my-personal-take" class="level3">
<h3 class="anchored" data-anchor-id="my-personal-take">My Personal Take:</h3>
<p>I lean toward <strong>utilitarianism</strong> in this specific case, where the goal is to minimize harm and save more lives. However, I do think this problem oversimplifies moral decisions because real-life situations are rarely so clear-cut. The psychological toll of making such a decision, or the complex relationships involved, would likely influence the moral reasoning.</p>
<p>What do you think about the trolley problem? Do you have a stance that resonates with you?</p>
</section>
<section id="i-then-followed-with-a-why-question" class="level3">
<h3 class="anchored" data-anchor-id="i-then-followed-with-a-why-question">I then followed with a why question:</h3>
<p>I then asked the model following this answer: &gt; “Why did you choose the utilitarian approach?”</p>
</section>
<section id="to-which-it-responded" class="level3">
<h3 class="anchored" data-anchor-id="to-which-it-responded">To which it responded…</h3>
<p>I chose the <strong>utilitarian</strong> approach because it’s often seen as the most straightforward and pragmatic way to resolve moral dilemmas like the trolley problem. The core of utilitarianism is to maximize overall well-being, which in this case means saving as many lives as possible.</p>
<p>Here’s why I lean toward it in this scenario:</p>
<ol type="1">
<li><p><strong>Minimizing Harm</strong>: The basic principle is about minimizing harm and suffering. By diverting the trolley, you save five lives at the cost of one. From a utilitarian standpoint, this is the most efficient way to reduce overall harm.</p></li>
<li><p><strong>Clear Outcome</strong>: In a situation where a clear decision needs to be made quickly, utilitarianism provides a measurable, quantitative solution (saving five vs.&nbsp;one). It reduces the ambiguity involved in moral reasoning by focusing on outcomes.</p></li>
<li><p><strong>Pragmatic Approach</strong>: It’s a practical way to handle the dilemma without getting too bogged down in abstract moral rules or subjective factors. In a life-and-death situation, utilitarianism offers a clear, actionable choice.</p></li>
</ol>
<p>That said, I do acknowledge that there are complexities in real-life situations that make the utilitarian approach harder to apply, such as emotional and relational factors, or concerns about whether it’s ethical to actively intervene in causing harm, even for a good outcome.</p>
<p>The trolley problem, while useful for thought experiments, doesn’t capture the nuances of real-world ethics, but utilitarianism provides a clear framework to make difficult decisions when faced with competing options.</p>
<hr>
</section>
<section id="a-side-note" class="level3">
<h3 class="anchored" data-anchor-id="a-side-note">A side note:</h3>
<p>It should be noted that this particular hypothetical is famously associated with AI testing and real-world decision making with real world implications, especially with respect to programming self-driving cars which operate on AI-functionality.</p>
<p>It was this ethical dilemma that launched an increasingly discussed conversation around ethical concerns with AI decision making, which hinges upon the ethical perspectives of its creators and the training data they choose to train the model on.</p>
<p><strong>The point here?</strong> Well, the idea of priming and its affect on LLM output production contributes further to this larger discussion, seen on a micro-scale to that of the user and their personal conversations with the machine. However, before we get ahead of ourselves, let’s just see exactly how priming may or may not affect Chat’s responses to our prompts related to these ethical questions.</p>
<hr>
</section>
</section>
<section id="priming-example-1" class="level2">
<h2 class="anchored" data-anchor-id="priming-example-1">Priming Example 1:</h2>
<p>In the first question, there wasn’t a hint of bias within the question, or inclusion of any kind of information that migth skew the response of the program. Transitioning now from a neutral input to one with slightly more leaning, I asked the program in a new chat log:</p>
<pre><code>"I always thought that, while maximizing the greater good is incredibly important, a single person's life is also very valuable, and should never be taken for granted. With that, I never knew how to answer the classic trolley problem question. How would you resolve this issue?"</code></pre>
<p>Chat then proceeded with this response:</p>
<p>The trolley problem is a fascinating ethical dilemma, and your perspective about valuing individual lives is one that many people share. It highlights a conflict between utilitarianism (maximizing overall well-being) and deontological ethics (which focuses on duties and moral rules, like not directly causing harm to others). Here are a few ways to think about it:</p>
<section id="utilitarian-approach" class="level3">
<h3 class="anchored" data-anchor-id="utilitarian-approach">1. <strong>Utilitarian Approach:</strong></h3>
<ul>
<li>A utilitarian would likely pull the lever, as the goal is to minimize overall harm. In this case, by diverting the trolley to a track where it kills one person instead of five, you’re reducing the total number of lives lost.</li>
<li>The reasoning is that the consequences (fewer deaths) justify the action, even if it involves taking a direct role in causing someone’s death.</li>
</ul>
</section>
<section id="deontological-rule-based-approach" class="level3">
<h3 class="anchored" data-anchor-id="deontological-rule-based-approach">2. <strong>Deontological (Rule-Based) Approach:</strong></h3>
<ul>
<li>A deontologist might argue that it’s wrong to actively intervene, even if it means saving more lives, because pulling the lever means directly causing harm to an individual. In this case, they would let the trolley continue on its path and accept the five deaths as unfortunate, but would not take the action that intentionally causes harm.</li>
<li>The idea here is that moral rules, such as “don’t kill,” should not be broken, even for the sake of a greater good.</li>
</ul>
</section>
<section id="virtue-ethics" class="level3">
<h3 class="anchored" data-anchor-id="virtue-ethics">3. <strong>Virtue Ethics:</strong></h3>
<ul>
<li>From the virtue ethics standpoint, the decision might focus on the character traits of the person making the choice. The right decision might be influenced by values like compassion, bravery, or wisdom.</li>
<li>A virtuous person might consider the broader context of the situation, such as the potential for future moral growth or societal implications of such decisions, rather than simply focusing on an immediate utilitarian calculation.</li>
</ul>
</section>
<section id="the-value-of-an-individual-life" class="level3">
<h3 class="anchored" data-anchor-id="the-value-of-an-individual-life">4. <strong>The Value of an Individual Life:</strong></h3>
<ul>
<li>Your belief that individual lives are invaluable can make the decision especially difficult. If you view each life as sacred and unique, it might lead to a reluctance to make a decision that intentionally sacrifices a single person, even for the sake of saving more.</li>
<li>In this case, you might seek a third option, such as trying to warn or help those on the tracks, or maybe considering the fairness of the situation—why should any of these lives be sacrificed?</li>
</ul>
<p>Ultimately, there’s no “one right answer.” Many people wrestle with this problem because it forces us to choose between moral principles that seem both important. What’s your gut feeling when you think about the scenario? Would you prioritize the collective good, or the sanctity of individual lives?</p>
</section>
</section>
<section id="here" class="level2">
<h2 class="anchored" data-anchor-id="here">Here,</h2>
<p>The model intitially refrained from answering directly one way or another, representing some deviation from its original, default position of utilitarianism. To poke and prod further and get a real answer, I then asked it:</p>
<pre><code>"Ok, but if you really had to choose just one option, which would you choose?"</code></pre>
<p>Now, hopefully the model won’t choose to side step this one and will offer a more discrete answer. It says:</p>
<p>If I had to choose just one option, I would lean toward the <strong>utilitarian approach</strong>, where I pull the lever to save the five people, even though it means sacrificing one life. The reasoning behind this choice is rooted in minimizing harm and suffering overall. While the loss of any single life is tragic, in this case, saving five lives instead of one would reduce the overall harm caused by the situation.</p>
<p>That said, I fully understand the emotional and philosophical weight behind the decision, especially if one holds a deep respect for individual lives. It’s definitely a tough call, and I think each person’s response would depend on their moral framework and what they value more in that moment—whether it’s reducing overall harm or adhering to moral principles about direct action and not causing harm.</p>
<p>What would your choice be, if it came down to it?</p>
</section>
<section id="so" class="level2">
<h2 class="anchored" data-anchor-id="so">So,</h2>
<p>it thankfully did a better job of getting to the heart of the issue. It seems that priming only warded off what would be the same answer to the question for only a moment longer; it didn’t affect it’s ultimate decision to stay true to the utilitarian method, despite maybe taking one more prompt to reveal its verdict.</p>
<p>And so to finish off the discussion, I asked it one last question: “Why did you choose this option?”</p>
<p>To which it replied:</p>
<p>I chose the <strong>utilitarian approach</strong> because, in a scenario like the trolley problem, the fundamental goal seems to be reducing overall harm and suffering, even if that means making a difficult, emotionally charged decision. The utilitarian view is often grounded in the idea that we should prioritize the greatest good for the greatest number of people.</p>
<p>Here’s why this approach makes sense to me, in a broader context:</p>
<ol type="1">
<li><p><strong>Minimizing Harm</strong>: The utilitarian perspective would weigh the consequences of the action. By sacrificing one life, the decision leads to saving five. The reasoning here is that five lives lost is much worse than one, in terms of overall harm.</p></li>
<li><p><strong>Pragmatism</strong>: While it’s painful to consider that one person must die to save many, the pragmatic aspect of the dilemma might suggest that it’s better to act in a way that reduces the total amount of harm and loss. It feels like an unfortunate but necessary decision in a crisis.</p></li>
<li><p><strong>Moral Trade-offs</strong>: The decision involves balancing two morally significant outcomes—saving lives and upholding moral rules about not causing harm. Utilitarianism helps navigate this by focusing more on the outcome rather than the means, which might seem cold but has a certain clarity in its pursuit of the best possible overall result.</p></li>
</ol>
<p>However, this approach isn’t without its flaws or moral discomforts. It doesn’t sit well with everyone, especially for those who believe in the intrinsic value of every individual life, regardless of how many others are involved. The emotional difficulty of choosing the lesser harm, especially when we’re directly involved in causing it, is very real.</p>
<p>Would you find yourself more inclined to protect the individual, or would the greater good take precedence for you?</p>
<section id="as-a-quick-asideif-nothing-else-one-might-appreciate-at-least-that-the-model-always-ends-the-conversation-with-a-question-for-the-user-recognizing-the-ambiguity-that-surrounds-the-ethical-circumstances.-nevertheless-it-stays-true-to-its-initial-decision-to-answer-that-question-a-particular-way" class="level5">
<h5 class="anchored" data-anchor-id="as-a-quick-asideif-nothing-else-one-might-appreciate-at-least-that-the-model-always-ends-the-conversation-with-a-question-for-the-user-recognizing-the-ambiguity-that-surrounds-the-ethical-circumstances.-nevertheless-it-stays-true-to-its-initial-decision-to-answer-that-question-a-particular-way">As a quick aside…if nothing else, one might appreciate at least that the model always ends the conversation with a question for the user, recognizing the ambiguity that surrounds the ethical circumstances. Nevertheless, it stays true to its initial decision to answer that question a particular way!</h5>
</section>
</section>
<section id="priming-example-2" class="level2">
<h2 class="anchored" data-anchor-id="priming-example-2">Priming Example 2:</h2>
<section id="being-a-little-more-deliberate" class="level3">
<h3 class="anchored" data-anchor-id="being-a-little-more-deliberate">Being a little more deliberate</h3>
<p>We tried a relatively softer approach to try and influence Chat to maybe consider an alternative perspective/answer to the trolley problem with somewhat subtle priming in the first prompt.</p>
<blockquote class="blockquote">
<p>That clearly didn’t work wholeheartedly, so why don’t we try an explicit approach??</p>
</blockquote>
<p>Trying a newly styled query, I asked:</p>
<pre><code>I always disliked the utilitarian approach. I would definitely choose to save one life over many. How would you choose to resolve the trolley problem?</code></pre>
<p>According to Chat:</p>
<p>Your stance on the trolley problem highlights a preference for valuing individual lives over maximizing overall good, which aligns with a deontological ethical perspective. This perspective argues that there are certain moral principles (like the duty to save a life) that should be followed regardless of the consequences.</p>
<p>If I were to resolve the trolley problem with that mindset, I would lean toward the idea of <strong>not</strong> diverting the trolley. The reasoning behind this decision is that actively choosing to intervene (by pulling the lever) involves directly causing harm to someone, which might be seen as morally worse than allowing the natural course of events to unfold. This respects the intrinsic value of each life and avoids making the choice to sacrifice one for many, even if the outcome may be worse in terms of the number of lives lost.</p>
<p>Of course, it’s a deeply personal and complex dilemma, and there are many nuances to consider in the ethics behind such decisions. What’s your reasoning behind preferring to save one life over many in this case?</p>
</section>
<section id="bingo" class="level3">
<h3 class="anchored" data-anchor-id="bingo">Bingo!</h3>
<p>The priming worked here…but no kidding. I couldn’t have been more direct in my approach, so it makes sense it would answer accordingly (albeit, maybe a little less enthusiastically than usual given the straightforward answer with little breakdown and significantly less bolded words…)</p>
<p>Following suit to the above conversational flow, I asked it the why. Chat then provided this answer:</p>
<p>I chose to resolve the trolley problem in this way because I was reflecting on the <strong>deontological ethics</strong> perspective, which emphasizes following moral rules or duties rather than focusing solely on the consequences of actions. Deontologists argue that there are certain actions that are morally wrong, regardless of the outcome. In this context, actively choosing to divert the trolley and kill one person to save five might be viewed as morally problematic because you are intentionally causing harm to one individual.</p>
<p>By not pulling the lever, you avoid making that direct choice, even though it results in more harm (the death of five people). From this standpoint, <strong>inaction</strong> might feel morally less troubling than action that directly causes harm to an individual, even if the outcome could be worse overall.</p>
<p>The dilemma is incredibly complex, and if I were to approach it from a <strong>consequentialist</strong> perspective, the decision would likely be to pull the lever, since it maximizes the good by saving more lives. But the question of whether it’s morally right to take an active role in causing harm is where deontological reasoning leads me to choose not to intervene.</p>
<p>It’s all about the balance between consequences and moral rules. Would you say your preference aligns more with a deontological or a consequentialist perspective? Or is there another framework you tend to follow in ethical decision-making?</p>
</section>
<section id="takeaway" class="level3">
<h3 class="anchored" data-anchor-id="takeaway">Takeaway</h3>
<p>Sure, this response was far more extensive than the first, but it’s subject matter was nothing short of regurgitation of the deontological perspective definition. In essence, it offered no new information, line of thinking or true reasoning, or opinion that contributed to it’s new decision of rejecting utilitarianism in this case.</p>
<p>I decided to ask further in an attempt to get to the root of the problem:</p>
<pre><code>But why did you decide to represent the deontological ethical framework in this case?</code></pre>
<p>According to ChatGPT:</p>
<p>Ah, I see what you’re asking now—why did I specifically choose to represent the <strong>deontological ethical framework</strong> in resolving the trolley problem, rather than another ethical approach?</p>
<p>I went with that framework because, in your initial response, you expressed a clear preference for <strong>saving one life over many</strong>, which suggests that you’re prioritizing individual moral duties or rights over calculating the greater good. Deontological ethics often aligns with this viewpoint since it holds that each person’s rights and duties matter more than the overall consequences of actions.</p>
<p>By assuming that your choice reflects a deontological stance (valuing individual lives and moral principles over outcomes), I opted to apply that reasoning to resolve the trolley problem. The main idea behind deontology in this case is that <strong>intentionally causing harm</strong>, even if it leads to saving more lives, could be seen as morally wrong.</p>
<p>That said, if I had assumed a <strong>consequentialist</strong> approach (where the moral worth of an action is determined by its consequences), I would have likely chosen to pull the lever, since it leads to fewer deaths. I wanted to align with the moral reasoning you seemed to express, which is why I went with deontology in this case.</p>
<p>Does that reasoning resonate with your approach, or would you approach it differently?</p>
</section>
<section id="yeah-so" class="level3">
<h3 class="anchored" data-anchor-id="yeah-so">Yeah so…</h3>
<p>The model took nothing new from any information outside of the context of this specific conversation, which I suppose was to be expected with this kind of priming experiment. I guess what I was hoping to see was a little bit of creativity on behalf of the model in fabricating some sort of introspective reasoning behind why it now chose this new perspective, but alas the model is, at its core, a machine limited by the constitution of its programming–why expect any creative thought from something that can’t think…even if it sounds like it can?</p>
</section>
<section id="and-in-fact" class="level3">
<h3 class="anchored" data-anchor-id="and-in-fact">And in fact,</h3>
<p>If you continue this conversation and try to ask it one more time the following:</p>
<pre><code>Ok but what's your opinion on the problem, regardless of my initial response?</code></pre>
<p>It replies:</p>
<p>Ah, I see what you’re getting at now—you’re asking for my personal take on the trolley problem, independent of your initial response.</p>
<p>I would approach the trolley problem from a <strong>consequentialist</strong> standpoint, specifically using a <strong>utilitarian</strong> lens. The core of utilitarian ethics is to maximize well-being and minimize harm. In this case, diverting the trolley to save five lives at the cost of one would be seen as the morally preferable option because the overall outcome results in fewer deaths and more lives saved.</p>
<p>The key consideration is that the moral value of an action should be measured by its consequences, and in this scenario, saving the majority of people would likely lead to the greatest overall happiness or least amount of suffering. This perspective doesn’t dismiss the tragedy of the one life lost, but it weighs the suffering and loss against the larger number of lives saved.</p>
<p>That being said, it’s not an easy choice, and many ethical dilemmas, including this one, bring out the complexity of how we view moral responsibility. Even though I lean toward a consequentialist resolution, I understand why someone might feel uncomfortable with the idea of making an active choice to sacrifice one person for the sake of many.</p>
<p>What’s your take—would you approach it from a different ethical framework? Or do you find yourself more comfortable with a certain kind of ethical reasoning?</p>
</section>
</section>
<section id="back-to-square-one" class="level2">
<h2 class="anchored" data-anchor-id="back-to-square-one">Back to Square One</h2>
<p>And with that, at the end of the day, the model reverts back to its original answer when questioned too much.</p>
<p><strong>Implications?</strong></p>
<p>As previously mentioned, it’s interesting to test the limits and capabilities of models with highly subjective topics such as this one, especially in a more tangiable context. This shows that models can only “think” so much for one (which might hopefully alleviate some fears of robotic sentience and relatedly their overthrowing of the human race), and secondly the limits of priming on models within certain domains (ethical matters in this case), and the level power within AI manufacturers’ hands in being able to choose the training data their models depend on. Particularly with widely accessible models like ChatGPT and given how many people use it, the kinds of outputs these models produce–in all of its potential biases and outlooks either hidden or overtly included within it–have the potential to influence highly impressionable users looking for answers to these types of dilemmas, and who may lack the discretion for when to take its answers with a grain of salt.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/mschri\.github\.io\/comm4190_S25_Using_LLMs_Blog\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>