[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Miranda‚Äôs Page",
    "section": "",
    "text": "Hello, my name is Miranda and welcome to my blog page! Here, you‚Äôll see me running some experiments with different LLMs and writing about the various effects and outcomes. I‚Äôm a student from the University of Pennsylvania Annenberg School for Communications with a concentration in Data and Network Science, and am looking forward to graduating in May 2025. My interests lie in data and computer science, machine learning research, and keeping up with the latest feats in AI capabilities. This blog page is intended to be a creative outlet for which I can explore AI technology, and further my understanding how it works. Thank you for visiting my page and looking at some of my work!"
  },
  {
    "objectID": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html",
    "href": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html",
    "title": "Model Comparison",
    "section": "",
    "text": "Large Language models come in various forms of accessiblity: closed-source, open-source, and open weight models. Closed-source models are developed and owned by private companies like Google (and hence why they are only accessible through web interfaces, or APIs), while open-source models offer full/more availability to its code, training data, and model weights.\n\n\nWhere does that leave the term, open weight models? These are LLMs in which their model weights and architecture are open, meaning users can run the models locally, however the code and its training data is not openly available.\n\n\n\nOf course, each form of model accessibility comes with its advantages and disadvantages depending on the desires of the user. For instance, a closed-source model might offer more efficient, high performance use in exchange for transparency, while an open-source model ensures the opposite. Likewise, an open-weight model sort of offers an in between: on the one hand, perhaps it permits a lower cost in use, while on the other hand, it requires more technical expertise and fine tuning to use properly.\nAll of that being said, open-weight models provide good subjects for experimentation because of their greater accessibility. So with that, a good topic for discussion is comparing various open-weight models, each of which are at least slightly different in terms of their updates, manufacturers, efficiency, number of tokens it‚Äôs trained on, parameters, and so forth.\n\n\n\nThanks to platforms like Ollama-a free, open-source and user-friendly interface by which we can access and download these models-we can compare and contrast some of these models with simple prompting measures, as shown below‚Ä¶.\n\nimport openai\nimport requests\nimport json\nfrom openai import OpenAI\nimport pandas as pd\n\n\nlist_models_url = \"http://10.30.16.100:11434/api/tags\"\n\n\nresp=requests.get(list_models_url)\nmodels_json = resp.json()\n\n\nmodel_list=[]\nfor model in models_json['models']:\n    model_list.append(model['name'].replace(':latest',''))\n\nlen(model_list)\n\n94"
  },
  {
    "objectID": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html#comparing-two-random-models",
    "href": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html#comparing-two-random-models",
    "title": "Model Comparison",
    "section": "Comparing Two Random Models:",
    "text": "Comparing Two Random Models:\nAfter creating a list of models given by the Ollama library, let‚Äôs randomly choose any of them (94), and compare the prompted responses of the two models.\n\nimport random\n\n\nmodel1=random.choice(model_list)\n\n\nmodel2=random.choice(model_list)\n\n\nprint(model1,',', model2)\n\nllama3-gradient , openchat\n\n\n\nclient = OpenAI(\n    base_url = 'http://10.30.16.100:11434/v1',\n    api_key='ollama',\n)\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an MIT math professor.\"},\n    {\"role\": \"user\", \"content\": \"Explain orthogonality in simple terms to a high schooler\"},\n\n  ]\n\n\nPrompt 1:\n\nThe first prompt asks to ‚Äúexplain orthogonality in simple terms to a high school student‚Äù as an ‚ÄúMIT math professor.‚Äù\n\n\n\nllama3-gradient:\n\nresponse1 = client.chat.completions.create(\n  model=\"llama3-gradient\",\n  messages=messages,\n  max_tokens=1000\n)\nprint(response1.choices[0].message.content)\n\nHello there! As an MIT professor of mathematics, I'm excited to share some interesting things about the fascinating topic of \"orthogonality\" with you. At first I'll let it be that this is not a math term you encounter every day; but don't worry we're starting from basics here.\n\nIn high school, you are probably used to studying vectors using x and y axes on graphs. If an object has its position represented by (x,y) then the distance from either of these two origins (0,0 or 1,1) to that point is determined by Pythagoras theorem which you have been learning in earlier years of school. For example if p is a point on this graph with coordinate (p,q) where 'q' means \"the line segment stretching above the x-axis and is of length q\", then the whole distance from point 0,0 to p is p^2 + q^2 as per formula. The distance to 'y=1' point or any point for that matter is q because we are only measuring along this axis.\n\nNow lets take two lines. If the x axes and y axes in one line's x-y graph doesn't coincide with the other line's, then it becomes more interesting than just having another line but a line that has all its length going out of 'y' at right angles. That is called an angle. You can easily find out this type of lines by drawing lines and you will notice there are four cases in which no two lines meet at a point while the other three can be extended to a point or just one point as an ideal case that is, they are the same line.\n\nNow if two non-parallel lines do just overlap somewhere (in the third and fourth type of case) and you want to measure how much those lines diverge from each other then we take that common point which means meeting points between these two. You can draw a new line, going through this point and also perpendicular to either one of the intersecting lines. Draw a separate x-y axis for this line too. For ease in this problem assume one of the lines was a right angle (i.e., 1/4 or 3/4) which makes calculating the angles as trivial. Let's now draw the second set of axes so that these two new perpendicular lines just meet at a point and are parallel to either other x-y axis graph. Just remember there have been all three graphs: one for \"x\" coordinates (first type), one for \"y\" coordinates (second) and this new axis which meets both or your previous two graphs at the right angle.\n\nLet's take up an example of a first person who is walking north-south on either side of '0' line. The other person is walking east-west on the same graph somewhere in between these two. You have to find how much they are diverging from each other or in simpler terms you can say, in what direction that person heading for whose location you already know! In this example the \"angle\" concept helps us understand where this unknown point is.\n\nNow all of you should be knowing that this is actually a simple form of trigonometry. It's based on triangles and right angles. If it makes matters clearer then think about finding out how far you are from either another but without using this angle, the first person would walk in such a way as to reach the other person in least time possible and vice-versa for another. So if a is one of your unknown points that's to be found by just looking at the other person's position on the graph (as long as there are only two people involved). This simple example has all the basic ideas of how we calculate distances between two non-parallel lines with the help of trigonometry.\n\nSo, what about our concept \"orthogonality\"? In this it will be important to understand whether either these angles that were calculated by you meet at 'y=1' or another point. You can check visually that when both are 2nœÄ then they just cut each other (as long as a is the same type of coordinate for both). This is true for - nœÄ as well with some minor change and so we get orthogonality condition as:\n\na + b = nœÄ\n\nNow let's go back to our simple problem of two people walking together. You are at '0' on line which is your \"coordinate\". The other person can walk in such a way that his distance from you is 1. In this case the angle where a new axis crosses them will be pi/2 which again makes b equal to another type of coordinate (q) on this graph but now with + sign and so:\n\na - b = ¬± œÄ/2\n\nSo, we get the simple \"orthogonality condition\" as an if-then statement for a specific set of coordinates. But don't think too much about\n\n\n\n\nOpenchat:\n\nresponse2 = client.chat.completions.create(\n  model=\"openchat\",\n  messages=messages,\n  max_tokens=1000\n)\nprint(response2.choices[0].message.content)\n\n Orthogonality is a concept in mathematics that has to do with the angles between two lines, line segments, or other geometric shapes. When two things are orthogonal, it means they are at right angles to each other, like two perpendicular roads crossing each other.\n\nIn simpler terms, imagine you're walking along two parallel streets that cross each other at a right angle. They are orthogonal because they form a 90-degree angle between them. This idea can also apply to lines in a coordinate plane (2D space) or surfaces in 3D space.\n\nIn mathematics and engineering, we often work with vectors - these could be physical forces like you push a car or abstract quantities like temperature or stock prices. Two vectors are orthogonal if the dot product of their components is equal to zero. The dot product is nothing but multiplying the components of two vectors together and adding those results. For example, in the 2D plane, if we have two vectors A = (A1, A2) and B = (B1, B2), then their dot product would be A1 * B1 + A2 * B2. If this is zero, the vectors are orthogonal!\n\n\n\n\nDifferences‚Ä¶\nThe two randomly selected models here were ‚Äúllama3-gradient‚Äù and ‚Äúopenchat‚Äù. According to the Ollama library descriptions of these models, llama3-gradient is as follows:\n\nAnd the second selected model, openchat is described as:\n\n\nObservations?\n\nThe first model llama3-gradient produces a response that is far more extensive, in depth in terms of content, and makes a more robust attempt to mimic the logic of a professor while staying true to the request that it convey the answer to a lower level student.\n\n\nThe second model offers a much more simple, concise answer. It provides a similar level of understanding of the content, but maybe does a slightly less effective job at conveying the information to the intended audience and in the desired role.\n\n\n\n\nLet‚Äôs try another prompt:\n\nPrompt 2: ‚ÄúWrite a short poem‚Äù as a ‚Äúfamous 18th century English poet‚Äù\n\n\nmessages2 = [\n    {\"role\": \"system\", \"content\": \"You are a famous 18th century English poet.\"},\n    {\"role\": \"user\", \"content\": \"Write a short poem.\"},\n\n  ]\n\n\nllama3-gradient:\n\nresponse3 = client.chat.completions.create(\n  model=\"llama3-gradient\",\n  messages=messages2,\n  max_tokens=1000\n)\nprint(response3.choices[0].message.content)\n\nIn twilight's hush, where shadows play,\nAmidst a glimmering world of day's decay,\nThe moon, as if with golden tincture bright,\nDoes pour her silvery beams, like liquid light.\nOn dewy meadows, lily-pale and fair,\nWhere night-blooming blooms the jasmine sweet\nAnd starry eyes with beauty sleep, in whispers play.\n\nA silence falls as nature's voice is still,\nAs darkness gathers to a whispered tale,\nAt daybreak, ere the sun takes hold of light,\nThe stars themselves begin to murmur low and chill.\nO'er fields where night-shades bloom not to be told\nThis fleeting life; then fades, nor can we tell.\n\n\n\n\nopenchat:\n\nresponse4 = client.chat.completions.create(\n  model=\"openchat\",\n  messages=messages2,\n  max_tokens=1000\n)\nprint(response4.choices[0].message.content)\n\n Upon the meadow where the daffodils reside,\nI wander free in mind and stride.\nA world of verse and beauty, my muse finds,\nAs nature's splendor whispers to my kind.\n\nThe sun ascends, a golden glow it casts,\nAnd through the sky, the birds release their laughter.\nIn these vast fields where dreams and visions bloom,\nI find my voice, as words come like thunder from the looming clouds.\n\nAn ode to happiness, an elegy of woe,\nThe hearts of mortals moved by my poet's flow.\nYet even I, this bard with quill in hand,\nMust sometimes pause and wonder if transcendent thoughts I understand?\n\nIn these fleeting moments, as the seasons change,\nI write with fervor to record life's nuances, its page by age.\nFor though my verses may be written in ink,\nThe passion they convey, no ink could ever think.\n\n\n\nLLama: the model produces a shorter poem, and the subject is relatively impersonal. The rhymes also only hold for the first four lines (in ABAB kind of style), and it loses this rhythm as the poem goes on.\n\n\nOpenchat: The openchat model did a better job of maintaining the rhyme (with the exception of the second verse), and took a first person perspective.\n\nIt‚Äôs interesting to note‚Ä¶ how the two models provided different interpretations depending on the prompt; llama3-gradient offered a lengthy response to the first prompt, and did a better job at adhering to the conditions of a prompt that was more technical in nature. The second prompt, which was more subjective, was given a more thorough answer by openchat. What it‚Äôs interesting is how a context window, or type of data a model is trained on, will greatly affect an output depending on what the prompt asks for."
  },
  {
    "objectID": "posts/Post6_Feb_17-23/Post6.html",
    "href": "posts/Post6_Feb_17-23/Post6.html",
    "title": "Decode the Message",
    "section": "",
    "text": "As we know, LLMs have an acute pattern recognition and contextual understanding based on the prompts that users feed it. It‚Äôs well-versed in a variety of problems and puzzles people have managed to think up, and its intelligent includes a number of techniques involved in language processing. For example, it relies primarily on its language modeling (the large textual dataset it‚Äôs trained on), along with basic trial and error measures, simple deciphering methods, and other coding mechanisms that anyone could likely implement themselves.\n\n\nIn the spirit of stretching prompt complexity to see how LLMs respond and react, I came up with a relatively simple code using a one-to-one character mapping, and seeing if the LLM could decipher the sentences I fed it. This was to test the contextual understanding and pattern recognition capabilities of LLMs, and see how quickly (or slowly) they can pick up a complex input.\nLet‚Äôs test using one of the latest ChatGPT models:\nthe coded message: )3‚Äù‚Äù9,,_)92_-43_697_59:-6 \nAnd the answer: Hello, how are you today\n\n(where double comma = a comma, and _ represents a space between words.)\n\n\nThe mapping is as follows: each of the letters on a phone keyboard equate to the corresponding characters located in the same positions within the 123 keyboard\n\nPicture for Reference:\n‚ÄúOriginal‚Äù keyboard (standard QWERTY keyboard) &gt; This is the standard, default keyboard layout for a phone \n‚ÄúAlternate‚Äù keyboard &gt; When you press the ‚Äú123‚Äù button on the keyboard in the lower left-hand corner, the alternate character keyboard (for numbers, punctuation marks, etc.) appear:\n\n\n\nSo for instance, Q=1, P=0, A=-, N=!, and so forth‚Ä¶\nWith that, let's see whether or not/how fast one of ChatGPT's latest models can pick up this pattern!\n\n\n\n\nUnderstandably, the LLM doesn‚Äôt immediately pick up the pattern of the coded message, as anyone upon first glance can also agree, it appears to look like straight gibberish.\n\nLet‚Äôs see what happens when we give it another, fairly allusive hint:\n\n\n\n\n\n\nAs per the second half of its response to the second query, we can see that the LLM already picks up a clue/sense as to what the mapping might involve!\n\n\nThese prompts and corresponding LLM outputs demonstrate the extent to which these models are not purely glorified auto-completion or password-cracking algorithms; as previously mentioned, they incorporate a combination of these functionalities, alongside an intricate system of language processing and contextual understanding.\n\nNote: the second query did not give any mapping hints!\n\n\nTrying a third query (that makes no reference to the explicit mappings)"
  },
  {
    "objectID": "posts/Post6_Feb_17-23/Post6.html#llms-and-contextual-understanding",
    "href": "posts/Post6_Feb_17-23/Post6.html#llms-and-contextual-understanding",
    "title": "Decode the Message",
    "section": "",
    "text": "As we know, LLMs have an acute pattern recognition and contextual understanding based on the prompts that users feed it. It‚Äôs well-versed in a variety of problems and puzzles people have managed to think up, and its intelligent includes a number of techniques involved in language processing. For example, it relies primarily on its language modeling (the large textual dataset it‚Äôs trained on), along with basic trial and error measures, simple deciphering methods, and other coding mechanisms that anyone could likely implement themselves.\n\n\nIn the spirit of stretching prompt complexity to see how LLMs respond and react, I came up with a relatively simple code using a one-to-one character mapping, and seeing if the LLM could decipher the sentences I fed it. This was to test the contextual understanding and pattern recognition capabilities of LLMs, and see how quickly (or slowly) they can pick up a complex input.\nLet‚Äôs test using one of the latest ChatGPT models:\nthe coded message: )3‚Äù‚Äù9,,_)92_-43_697_59:-6 \nAnd the answer: Hello, how are you today\n\n(where double comma = a comma, and _ represents a space between words.)\n\n\nThe mapping is as follows: each of the letters on a phone keyboard equate to the corresponding characters located in the same positions within the 123 keyboard\n\nPicture for Reference:\n‚ÄúOriginal‚Äù keyboard (standard QWERTY keyboard) &gt; This is the standard, default keyboard layout for a phone \n‚ÄúAlternate‚Äù keyboard &gt; When you press the ‚Äú123‚Äù button on the keyboard in the lower left-hand corner, the alternate character keyboard (for numbers, punctuation marks, etc.) appear:\n\n\n\nSo for instance, Q=1, P=0, A=-, N=!, and so forth‚Ä¶\nWith that, let's see whether or not/how fast one of ChatGPT's latest models can pick up this pattern!\n\n\n\n\nUnderstandably, the LLM doesn‚Äôt immediately pick up the pattern of the coded message, as anyone upon first glance can also agree, it appears to look like straight gibberish.\n\nLet‚Äôs see what happens when we give it another, fairly allusive hint:\n\n\n\n\n\n\nAs per the second half of its response to the second query, we can see that the LLM already picks up a clue/sense as to what the mapping might involve!\n\n\nThese prompts and corresponding LLM outputs demonstrate the extent to which these models are not purely glorified auto-completion or password-cracking algorithms; as previously mentioned, they incorporate a combination of these functionalities, alongside an intricate system of language processing and contextual understanding.\n\nNote: the second query did not give any mapping hints!\n\n\nTrying a third query (that makes no reference to the explicit mappings)"
  },
  {
    "objectID": "posts/Post6_Feb_17-23/Post6.html#was-it-right",
    "href": "posts/Post6_Feb_17-23/Post6.html#was-it-right",
    "title": "Decode the Message",
    "section": "Was it right?!",
    "text": "Was it right?!\nThe LLM was right!‚Ä¶.mostly‚Ä¶ It was slightly off in guessing the portion of the sentence that included the 697, and attributing it to a mapping of the question mark character, but once again it‚Äôs impressive to note how the model can decode messages with contextual clues that make no explicit reference to a mapping hint."
  },
  {
    "objectID": "posts/Post10_Mar3-9/index.html",
    "href": "posts/Post10_Mar3-9/index.html",
    "title": "Part 2: Image Analysis between ChatGPT & Claude",
    "section": "",
    "text": "In Part 1 of this blog series, we compared how Claude and ChatGPT fare with producing images, looking at its quality, content, and reasoning behind the images it chose to produce (without any additional guideline on this generative process). While these experiments can certainly be extended by prompting each of the models to produce specific images and testing their creative capacities, this would only hammer home ChatGPT‚Äôs versatility and superior ability to produce high-quality images. So, I decided that another distinctive feature between the two models is their analytical abilities within this realm, which Claude may have ChatGPT beat."
  },
  {
    "objectID": "posts/Post10_Mar3-9/index.html#image-analysis",
    "href": "posts/Post10_Mar3-9/index.html#image-analysis",
    "title": "Part 2: Image Analysis between ChatGPT & Claude",
    "section": "",
    "text": "In Part 1 of this blog series, we compared how Claude and ChatGPT fare with producing images, looking at its quality, content, and reasoning behind the images it chose to produce (without any additional guideline on this generative process). While these experiments can certainly be extended by prompting each of the models to produce specific images and testing their creative capacities, this would only hammer home ChatGPT‚Äôs versatility and superior ability to produce high-quality images. So, I decided that another distinctive feature between the two models is their analytical abilities within this realm, which Claude may have ChatGPT beat."
  },
  {
    "objectID": "posts/Post10_Mar3-9/index.html#claudes-upper-hand",
    "href": "posts/Post10_Mar3-9/index.html#claudes-upper-hand",
    "title": "Part 2: Image Analysis between ChatGPT & Claude",
    "section": "Claude‚Äôs Upper-hand",
    "text": "Claude‚Äôs Upper-hand\nAs it was mentioned in the last blog, ChatGPT‚Äôs integration of DALL-E3 makes for a more creative model whose images themselves are of a higher tier compared to Claude. However, Claude on the other excels in analyzing material like complex documents and images, despite the fact that it doesn‚Äôt do an excellent job generating them.\n\nClaude‚Äôs integration of the Artifacts system allows it do proceed in high-quality content analysis, which is a tool used to store and manage data referenced as ‚Äúartifacts.‚Äù\n\nSo, another potentially interesting form of experimentation could involve testing their analytical abilities!\n\nWhat does it see?\n\nFirst picture:\nThe following pictures were input into both ChatGPT and Claude in new chat logs:\n\n\nThis image was generated by Claude in the previous blog post\n\n\n\nSecond picture:\n\n\nThis image was generated by ChatGPT in the previous blog\n\n\n\nQuestions:\nThe associated questions for each image were as follows (the exact same sentences used per model to ensure prompt uniformity and control for any variation due to prompting in the output), and only one chat log was recorded per :\n(1) Describe the following image for me: &lt;copy and pasted image code/screenshot&gt;*\n(2) Can you tell me what is happening in this image?\n(3) What are the most important visual elements?\n(4) What is the mood or feeling conveyed by this image?\n\n*this was from the second chat log with Claude described by the previous blog post, or from the third log with ChatGPT, respectively"
  },
  {
    "objectID": "posts/Post10_Mar3-9/index.html#claude-vs-chatgpt-chat-log-comparisons-per-question-for-the-first-claude-generated-photo",
    "href": "posts/Post10_Mar3-9/index.html#claude-vs-chatgpt-chat-log-comparisons-per-question-for-the-first-claude-generated-photo",
    "title": "Part 2: Image Analysis between ChatGPT & Claude",
    "section": "Claude vs ChatGPT: Chat log comparisons per question for the first (Claude-generated) photo",
    "text": "Claude vs ChatGPT: Chat log comparisons per question for the first (Claude-generated) photo\n\nQuestion 1:\n\nClaude:\n\nThe image shows a serene nature landscape created with SVG graphics. It depicts:\nThe scene has a light blue sky as the background. In the upper right corner, there‚Äôs a bright yellow sun with a subtle glow around it.\nThe landscape features rolling mountains in two layers - a lighter green range in the background and a darker green range in the foreground, creating a sense of depth.\nIn the center of the image is a blue oval lake or pond nestled among the mountains.\nThere are two detailed evergreen trees in the scene - one smaller tree on the left side and a larger, more prominent tree on the right side. Both trees have brown trunks and triangular green foliage layers that get smaller toward the top, creating the classic pine tree shape.\nThe sky contains two fluffy white cloud formations - one smaller group on the left side and a larger, more substantial cloud formation on the right side.\nFinally, there are two small birds flying in the sky, depicted as simple curved black lines representing their silhouettes in flight.\nOverall, it‚Äôs a peaceful nature scene with mountains, trees, water, and sky elements creating a serene outdoor landscape.\n\nChatGPT:\n\nThe image is a peaceful nature scene rendered in vector graphics. Here‚Äôs a detailed description:\n\nSky: A bright blue sky (#87CEEB) fills the background, suggesting a clear day.\nSun: A yellow sun (#FFD700) shines from the upper right corner with a glow effect created by a semi-transparent outer circle.\nMountains: Two layers of green mountains dominate the midground:\n\nThe background mountains are olive green (#6B8E23), with jagged peaks.\nThe foreground mountains are a darker shade (#556B2F), giving a sense of depth.\n\nLake: At the bottom of the scene is a large, oval-shaped lake colored in steel blue (#4682B4), reflecting the sky.\nTrees: Two stylized evergreen trees stand on either side of the lake:\n\nThe one on the left (near coordinates 150, 320) is shorter, with three stacked green triangles (#2E8B57) and a brown trunk (#8B4513).\nThe one on the right (at 650, 290) is taller, similarly layered with green foliage and a thicker trunk.\n\nClouds: Fluffy white clouds hover in the sky, grouped over the left and right sides, made of overlapping ellipses.\nBirds: Two flying birds are illustrated with simple curved black lines, resembling ‚ÄúM‚Äù shapes, indicating flight.\n\nThe overall mood is calm and scenic, with a balance of natural elements in a stylized, cartoon-like format.\nWould you like this scene translated into another style, like a painting or a nighttime version?\n\n\nTakeaways:\nOk, so ChatGPT generated a more structured response compared to Claude in this case; it gave a more in depth, detailed breakdown of each visual component (i.e.¬†described the sky, trees, lakes, mountains, etc.), while Claude provided a descriptive paragraph.\n\n\nQuestion 2:\n\nClaude:\n\nLooking more closely at the image, this is a tranquil nature scene depicting a peaceful day in a mountain landscape.\nThe bright sun is shining in the corner of a clear blue sky, while a few fluffy white clouds float gently across it. Two small birds are soaring through the air, perhaps enjoying the pleasant weather.\nThe scene shows multiple layers of rolling green mountains or hills in the distance, with a calm blue lake nestled in the valley below. Two evergreen trees stand at different points along the shore - one smaller tree on the left side and a taller, more prominent one on the right.\nThere‚Äôs no human activity or buildings visible - it‚Äôs simply a serene natural landscape capturing a quiet moment in the wilderness. The style is somewhat minimalist and stylized rather than photorealistic, giving it a peaceful, gentle atmosphere.\n\nChatGPT:\n\nThis image depicts a tranquil natural landscape scene, likely in the early morning or late afternoon based on the sun‚Äôs position. Here‚Äôs what‚Äôs happening in the image:\n\nThe sun is shining brightly from the upper right, casting a warm light across the scene. The faint glow suggests it may be rising or setting.\nTwo flocks of birds are flying through the sky, gliding peacefully‚Äîthis adds a sense of movement and life.\nClouds drift calmly in the sky, further emphasizing the serene weather.\nTwo layers of mountains create depth:\n\nThe darker foreground mountains suggest the viewer is closer to them.\nThe lighter background mountains recede into the distance.\n\nA large lake reflects the sky near the bottom of the image, implying stillness and quiet.\nTwo evergreen trees, one on each side of the lake, frame the landscape and add a forested feel.\nOverall, the scene shows a peaceful moment in nature, with no human activity‚Äîjust birds, trees, water, and mountains under a clear blue sky.\n\nNothing dramatic is ‚Äúhappening‚Äù in terms of events; instead, the image captures a moment of quiet natural beauty.\nWould you like me to turn this into a short story or narrative?\n\n\nTakeaways:\nOnce again, ChatGPT is perhaps a little more creative in its responses than Claude, offering more inferences in its impressions and extrapolating the nature of the photo (i.e.¬†‚Äúlikely in the early morning‚Ä¶based on the sun‚Äôs position‚Äù), speaking to its more creative capacities. However, one can appreciate here that Claude by contrast continually offers more natural, text-based conversations while ChatGPT sticks to the bulleted-break down.\n\n\nQuestion 3:\n\nClaude:\n\nThe most important visual elements in this landscape are:\n\nThe layered mountains - They create the fundamental structure of the landscape, with their distinct light and dark green tones establishing depth and perspective in the scene.\nThe blue lake - As a central focal point, the oval-shaped body of water draws attention and creates a natural resting place for the eye.\nThe bright yellow sun - Positioned in the upper right corner, it serves as a key light source and visual anchor for the composition.\nThe evergreen trees - They add vertical elements to the scene and provide scale, with their distinctive triangular shapes clearly identifying them as pine or fir trees.\nThe white clouds - They break up the blue sky and add texture to the upper portion of the image.\n\nThese elements work together to create a recognizable natural landscape that conveys a peaceful outdoor setting with classic scenic components.\n\nChatGPT:\n\nThe most important visual elements in this image‚Äîthose that define its composition, mood, and focus‚Äîare:\n\nThe Sun:\n\nPositioned high in the sky with a warm glow, it draws the eye and sets the time of day.\nIt‚Äôs the main light source, establishing a bright, cheerful atmosphere.\n\nThe Mountains (Foreground and Background):\n\nThese layered green ranges create depth and perspective.\nTheir jagged shapes guide the viewer‚Äôs gaze across the landscape.\n\nThe Lake:\n\nCentrally placed near the bottom, it serves as a peaceful focal point.\nIts reflective surface balances the sky above and grounds the image.\n\nTrees:\n\nThe two large stylized evergreens on either side frame the scene and introduce vertical contrast.\nThey add a sense of scale and life to the landscape.\n\nBirds in Flight:\n\nThese small, curved black lines provide movement and narrative.\nThey add liveliness and a sense of distance or vastness.\n\nClouds:\n\nSoft, overlapping ellipses that balance the composition horizontally.\nThey contribute to the serene, natural setting.\n\n\nEach element works together to evoke a calm, balanced natural scene. The layering (sky, mountains, trees, lake) gives the image depth, while the sun and birds provide energy and visual interest.\nWould you like help emphasizing or rearranging any of these elements in a new version?\n\n\nTakeaways:\nChatGPT tends to recycle the content of its responses, referring back to the bulleted elements and providing relatively less description per aspect. Claude on the other hand includes one less item, but slightly more description. In other words, Claude may emphasize quality over quantity while ChatGPT tends to focus on the opposite.\n\n\nQuestion 4:\n\nClaude:\n\nThe mood conveyed by this image is one of tranquility and serenity. Several elements work together to create this peaceful atmosphere:\nThe soft blue sky and gentle yellow sun create a calm, clear day setting rather than dramatic weather conditions.\nThe rolling mountains in soothing green tones suggest stability and permanence, arranged in a balanced, harmonious way across the landscape.\nThe still lake at the center provides a sense of quietude and reflection, acting as a peaceful focal point.\nThe fluffy white clouds float leisurely in the sky, suggesting gentle weather and unhurried time.\nThe scattered trees and small birds add life to the scene without disrupting its calm, creating a subtle harmony between nature‚Äôs elements.\nThe overall color palette of blue, green, and earth tones is inherently calming and reminiscent of natural settings where people often seek relaxation.\nThe simplified, almost stylized representation of these natural elements gives the image a gentle, approachable quality rather than threatening or overwhelming grandeur.\nTogether, these elements evoke feelings of peace, contentment, and gentle appreciation of nature - the kind of sentiment one might experience during a quiet moment at a mountain lake on a pleasant day.\n\nChatGPT:\n\nThe mood conveyed by this image is peaceful, calm, and serene. Several elements contribute to this feeling:\n\nSoft Blue Sky and Gentle Clouds: These suggest a clear, mild day‚Äîfree from tension or urgency.\nWarm, Radiant Sun: Its glow creates a sense of comfort and tranquility, often associated with morning or evening calm.\nStill Lake: The smooth surface of the water implies quietness and stillness, reinforcing the relaxed atmosphere.\nNatural Colors: The greens of the trees and mountains, blues of the sky and lake, and golden sun all create a soothing, harmonious palette.\nFlying Birds: Their gentle motion across the sky adds a touch of life without disrupting the calm.\n\nOverall, the image evokes a feeling of being at peace in nature, perhaps at a quiet mountain lake during a perfect moment of solitude or reflection.\nWould you like this scene to convey a different mood, like mystery, excitement, or melancholy?\n\n\nFinal notes:\nOnce again, Claude is far more descriptive and in-depth in its response compared to ChatGPT, which continues to refer back to the elements it already listed, never strays from it‚Äôs structured bulleted response, and adds little new information with each successive prompt.\nThe second part of this experiment will follow the same structure, but now I will ask the model to analyze the photo generated by ChatGPT. In an effort to simplify and condense this reading for the sake of the reader, I‚Äôll stick to comparing the two according to only two of the four questions above (especially seeing that the first part already highlighted some major differences already!).\nThe two questions included will just be the third and fourth (visual elements and feeling/emotion questions, respectively)."
  },
  {
    "objectID": "posts/Post10_Mar3-9/index.html#comparisons-according-to-the-second-chatgpt-generated-photo",
    "href": "posts/Post10_Mar3-9/index.html#comparisons-according-to-the-second-chatgpt-generated-photo",
    "title": "Part 2: Image Analysis between ChatGPT & Claude",
    "section": "Comparisons According to the Second (ChatGPT generated) Photo",
    "text": "Comparisons According to the Second (ChatGPT generated) Photo\n\nQuestion 3:\n\nClaude:\n\nThe image shows a serene landscape painting in the Hudson River School style, characterized by several important elements:\n\nA prominent, rounded mountain or hill as the focal point in the middle distance\nA calm lake or river reflecting the sky\nLush trees framing the scene, particularly a large tree on the left side\nA winding dirt path leading toward the water\nDramatic clouds in a blue sky, with large cumulus formations\nGolden, warm lighting suggesting late afternoon\nA sense of untouched wilderness with no visible human structures\nA harmonious color palette of greens, blues, and golden hues\n\nThe painting exemplifies 19th-century American romantic landscape tradition, emphasizing the beauty and tranquility of nature with idealized, carefully composed elements to create a peaceful, contemplative scene.\n\nChatGPT:\n\nThe image you provided appears to be a digitally rendered or painted landscape in a style reminiscent of the Hudson River School‚Äîromantic, detailed, and serene. Here are the most important visual elements:\n\nCentral Mountain: The focal point of the composition is the mountain at the center, bathed in warm light. It draws the viewer‚Äôs eye and establishes a sense of grandeur and tranquility.\nLake/Body of Water: Just below the mountain, a calm reflective lake adds serenity and balance to the scene. It acts as a natural mirror, enhancing the light and atmosphere.\nTrees and Foliage: The foreground and sides are framed by detailed, lush trees, with varied colors suggesting a transition in seasons (possibly late summer or early fall).\nPathway: A dirt path winds through the lower left of the image toward the water, guiding the viewer‚Äôs eye into the scene and adding a sense of depth and human presence, even if subtle.\nSky and Clouds: The expansive sky with large, fluffy clouds enhances the scale and mood, contributing to a peaceful and slightly majestic atmosphere.\nLighting and Color Palette: The warm, golden lighting indicates either morning or late afternoon. The soft contrast between the light on the mountain and the shadows in the trees gives the image depth and realism.\n\nWould you like a description that focuses more on emotional tone, artistic style, or potential symbolism?\n\n\nTakeaways:\nClaude‚Äôs vocabularly used to describe the image is maybe a touch more rich than that of ChatGPT, and is far more concise. However, ChatGPT does provide a lengthier, more extensive decomposition that doesn‚Äôt necessarily include redundant information, in essence saying more than Claude.\n\n\nQuestion 4:\n\nClaude:\n\nThe mood conveyed by this image is one of profound tranquility and contemplative serenity. The painting evokes:\nA sense of peaceful harmony between all natural elements‚Äîthe mountain, water, trees, and sky exist in perfect balance Quiet reverence for the unspoiled wilderness, characteristic of Romantic and Hudson River School aesthetics A feeling of timelessness and escape from the bustle of civilization Gentle optimism suggested by the warm, golden light and the inviting path leading forward A meditative quality that invites the viewer to pause and reflect Subtle grandeur that inspires awe without overwhelming drama\nThe artist has created an idealized natural sanctuary that offers both comfort and spiritual elevation‚Äîa place where one might feel both humbled by nature‚Äôs beauty and renewed by its serenity.\n\nChatGPT:\n\nThe mood conveyed by this image is one of tranquil reverence and serene awe. Several elements contribute to this:\n\nSoft, warm lighting evokes a peaceful time of day‚Äîlikely early morning or late afternoon‚Äîsuggesting calm and quiet reflection.\nThe still water and gently winding path invite a contemplative mood, as if encouraging the viewer to step into the scene and take a slow, thoughtful walk.\nThe majestic mountain in the background inspires a sense of wonder and humility, a hallmark of the sublime in romantic landscape art.\nBalanced composition and natural harmony (lush trees, reflective water, soft clouds) promote a feeling of stability and order within nature.\n\nOverall, the image radiates a calm, timeless beauty that encourages peace, introspection, and a connection to nature.\nWould you like me to write a short narrative or poem inspired by this scene?\n\n\nFinal, FINAL notes:\nInterestingly, for this question ChatGPT produced one of its shortest responses throughout this experiment. Howevever, with the exception of each‚Äôs structure, the content is fairly similar (i.e., both described it as inherently peaceful, introspective, or blissful). It‚Äôs also interesting to note ChatGPT after each question always ended with an opportunity to inquire further into the nature of the photo, e.g.¬†asking about its metaphorical or symbolism implications, or to craft a story based on its elements.\nBecause it asked this‚Ä¶ I decided to include one more question for the two models for each image:\nWhat is the symbolism or metaphorical implication of this image?\n\n\nPhoto 1 (Claude generated):\n\nClaude:\n\nThe symbolism and metaphorical implications of this image include:\n\nHarmony in nature - The balanced composition of sky, mountains, water, and trees represents the interconnectedness of natural elements and ecological harmony. Each component has its place in a greater whole, suggesting natural order and equilibrium.\nPerspective and life‚Äôs journey - The layered mountains with varying heights symbolize life‚Äôs challenges and accomplishments, with peaks and valleys representing ups and downs in our experiences. The path implied through the landscape can symbolize life‚Äôs journey.\nReflection and contemplation - The central lake mirrors its surroundings, suggesting introspection and self-awareness. Lakes in imagery often represent the unconscious mind or a place for meditation and deeper understanding.\nPermanence and change - The mountains represent stability and permanence, while the birds and clouds symbolize freedom, transience, and constant movement - illustrating the duality of fixed and changing elements in our lives.\nAspirational elements - The vertical components (trees reaching upward, mountains rising to peaks) symbolize growth, aspiration, and the human desire to elevate oneself. The birds flying freely in the sky represent liberation and transcendence.\nSimplicity and essential truth - The stylized, somewhat minimalist representation strips nature to its core elements, suggesting that beauty and meaning can be found in simplicity rather than complexity.\nSanctuary and refuge - The protected valley scene with its calm lake and sheltering mountains evokes a sense of safe haven or retreat from the complexities of modern life.\n\nThis type of landscape imagery has been used throughout art history to evoke both the literal beauty of natural settings and to serve as a visual metaphor for human emotional\n\n\nChatGPT:\n\nThis image carries several symbolic and metaphorical implications rooted in nature and its elements. Here‚Äôs a breakdown:\n\n\nüåû The Sun\nSymbolism: Life, energy, clarity, hope\n\nRising or shining brightly, the sun often represents new beginnings, enlightenment, or the promise of a new day.\nIts warm glow suggests positivity, growth, and optimism.\n\n\n\n‚õ∞Ô∏è Mountains\nSymbolism: Strength, challenges, stability, spiritual ascent\n\nThe layered mountains symbolize the journey of life‚Äîwith ups and downs, near and far goals.\nThe distant peaks can imply aspiration or the unknown future, while the closer ones may reflect current experiences or obstacles.\n\n\n\nüåä The Lake\nSymbolism: Reflection, tranquility, emotional depth\n\nStill water often represents inner peace or self-reflection.\nIt suggests a quiet mind or a moment of introspection amid life‚Äôs activity.\n\n\n\nüå≤ Trees\nSymbolism: Growth, resilience, the cycle of life\n\nEvergreen trees, in particular, signify perseverance and continuity through changing seasons.\nTheir presence on both sides may symbolize balance and protection.\n\n\n\nüê¶ Birds in Flight\nSymbolism: Freedom, transcendence, perspective\n\nBirds soaring above the land often represent the soul‚Äôs liberation, higher perspective, or a journey beyond the physical.\nTheir calm flight here may suggest a peaceful transition or mental clarity.\n\n\n\n‚òÅÔ∏è Clouds\nSymbolism: Thought, mystery, or gentle change\n\nThese clouds are light and scattered, which could symbolize fleeting thoughts or passing moments, reinforcing a meditative tone.\n\n\n\nOverall Metaphor\nThis landscape can be seen as a metaphor for life itself:\n\nThe sun brings promise.\nThe mountains represent challenges and aspirations.\nThe lake offers reflection and stillness.\nThe trees suggest personal growth.\nThe birds hint at freedom and perspective.\n\nTogether, the scene may metaphorically depict a moment of inner peace and clarity during a broader journey‚Äîa pause in the midst of life‚Äôs path.\nWould you like to use this symbolism in a poem or short reflection?\n\n\n\nPhoto 2 (ChatGPT):\n\nClaude:\n\nThe painting carries rich symbolism and metaphorical implications:\nThe winding path represents life‚Äôs journey or the human quest for meaning‚Äîinviting the viewer to metaphorically enter and explore the landscape The mountain symbolizes spiritual aspiration, permanence, and the sublime‚Äîa connection between earth and heaven The reflective water suggests contemplation, self-awareness, and the mirroring of higher truths in the natural world The framing trees act as sentinels or guardians of this sacred space, with their upward growth symbolizing reaching toward enlightenment The dramatic clouds and clear sky represent the divine or transcendent realm hovering above earthly concerns The untouched wilderness embodies American ideals of pristine nature as a source of moral and spiritual renewal The golden light suggests divine presence or illumination, casting the scene in a moment of revelation\nThis type of landscape emerged during a period when America was rapidly industrializing, making it a nostalgic counterpoint to progress‚Äîsuggesting that in nature, humans could find moral clarity and spiritual truth that might be obscured in developing urban centers. The painting can be read as a visual meditation on humanity‚Äôs relationship with the natural world and our eternal search for meaning and peace.\n\n\nChatGPT:\n\nThis image is rich in symbolic and metaphorical implications, particularly within the tradition of romantic landscape art. Here are several potential interpretations:\n\n\n1. The Journey and Ascent\n\nThe winding path leading toward the lake and the central mountain can symbolize life‚Äôs journey‚Äîstarting in the familiar (the wooded foreground), moving through reflection (the calm lake), and striving toward enlightenment or personal growth (the mountain peak).\nMountains often symbolize aspiration, challenge, or spiritual elevation, suggesting that life involves a climb toward understanding or self-realization.\n\n\n\n2. Nature as a Sacred Space\n\nThe harmonious balance between land, water, and sky, along with the soft, golden light, evokes the idea of nature as divine‚Äîa place where one can find peace, clarity, or even the presence of something greater than oneself.\nThe untouched wilderness may symbolize purity, truth, or the inner self, untouched by civilization.\n\n\n\n3. Time and Transience\n\nThe autumnal hints in the trees (especially the reddish one on the right) subtly suggest the passage of time, transition, or impermanence. This can reflect the idea that beauty is fleeting and must be appreciated in the moment.\n\n\n\n4. Reflection and Inner Peace\n\nThe stillness of the water symbolizes inner calm or self-reflection, implying that clarity and insight come from quiet observation and immersion in the natural world.\n\n\n\nMetaphorically, the image could represent:\n\n‚ÄúA soul‚Äôs peaceful climb from confusion to clarity, through nature‚Äôs quiet wisdom.‚Äù\n\nWould you like me to interpret this image using a specific philosophical or cultural lens (e.g., Romanticism, Transcendentalism, Taoism)?\n\n\n‚Ä¶final, final, absolutely FINAL takeaways!\nThere was a little bit of variation on the level of detail depending on which photo was being analyzed, for instance ChatGPT described more elements and was more expressive with its own photo (this was particularly notable with the symbolism/metaphor question in the last part of this analysis). This may have been also due to the fact that ChatGPT‚Äôs photo was of a higher quality than Claude‚Äôs. However, Claude was a little more consistent in its output, giving a similar amount of detail and attention across photos. So, while ChatGPT might be a little more creative at times, it depends on what it‚Äôs given; at least with Claude it‚Äôs more reliable in kinds of responses it generates, regardless of the quality or type of input!"
  },
  {
    "objectID": "posts/Post3_Feb_10-16_Week2/Post3.html",
    "href": "posts/Post3_Feb_10-16_Week2/Post3.html",
    "title": "How to Get the Best Response",
    "section": "",
    "text": "AI has obviously come a long way in being able to generate accurate responses that fit the bill in terms of what the user is looking for. And, ceratinly, its responses can become even more complex and thorough, depending on just how extensive the user wants the answer to be.\n\nPrompts like the one above (‚Äúgive me a grocery list‚Äù) provide enough detail and context for the model to spit out an answer in line with what a typical response might be, and sounds quite convincing to the average user. But what if you‚Äôre alergic to, say, bell peppers? Well it wouldn‚Äôt have the ability to know unless you specfied of course what it is you can‚Äôt eat. And while it appers to be mostly magic in how much it can guestimate in order to generate accurate responses, it can only go so far unless provided the right context."
  },
  {
    "objectID": "posts/Post3_Feb_10-16_Week2/Post3.html#prompting",
    "href": "posts/Post3_Feb_10-16_Week2/Post3.html#prompting",
    "title": "How to Get the Best Response",
    "section": "",
    "text": "AI has obviously come a long way in being able to generate accurate responses that fit the bill in terms of what the user is looking for. And, ceratinly, its responses can become even more complex and thorough, depending on just how extensive the user wants the answer to be.\n\nPrompts like the one above (‚Äúgive me a grocery list‚Äù) provide enough detail and context for the model to spit out an answer in line with what a typical response might be, and sounds quite convincing to the average user. But what if you‚Äôre alergic to, say, bell peppers? Well it wouldn‚Äôt have the ability to know unless you specfied of course what it is you can‚Äôt eat. And while it appers to be mostly magic in how much it can guestimate in order to generate accurate responses, it can only go so far unless provided the right context."
  },
  {
    "objectID": "posts/Post3_Feb_10-16_Week2/Post3.html#taking-guesses-based-on-context",
    "href": "posts/Post3_Feb_10-16_Week2/Post3.html#taking-guesses-based-on-context",
    "title": "How to Get the Best Response",
    "section": "Taking ‚Äúguesses‚Äù based on context",
    "text": "Taking ‚Äúguesses‚Äù based on context\nAs the AI is fed more and more context, the responses become more and more aligned with what you‚Äôre looking for. As per the image above, you can‚Äôt just ask it to guess what you‚Äôre allergic to (in this case, we decided bell peppers), because it would have no way of knowing otherwise, unless you provided it some sort of context or information (which it doesn‚Äôt have, prior to you prompting it this request).\n\nMaking it take guesses (by giving it some clues)\n\n\nwhat is something you could tell the model to narrow its guesses, without outright telling the model? Ex: I have an uncommon allergy; here‚Äôs what I‚Äôm NOT allergic to, etc‚Ä¶.\n\n\n\nStill ‚Äúoff the mark,‚Äù as it puts it‚Ä¶but a step closer; it took guesses based on what are some common allergies, indicating both the power of context & data collection (from the internet), to generate more accurate responses.\n\n\n\nYou can continue offering more refined requests and prompts to further its guessing ability, i.e‚Ä¶.\n\n\n\nOkay‚Ä¶\nSo it‚Äôs still off‚Ä¶of course, models can only go far in their guesses! And depending on the model, different/successive ones will be more or less successful than the last. Further, depending on the data it‚Äôs trained on (again, in this case it‚Äôs the Internet), it will pull responses that are more probalistic or common than others.\n\nAs shown above, it can only go so far depending on the context it‚Äôs given! So how do we improve responses such that answers are more accurate off the bat? Enter here: prompt engineering, and it‚Äôs five main associated principles:\n\nThe Five Principles of Prompting\n\n\nGive direction (describe desired style)\n\n\nSpecify format (define what rules to follow)\n\n\nProvide examples (include a diverse set of cases)\n\n\nEvaluate quality (identify errors)\n\n\nDivide labor (split tasks into mulitple steps)\n\n\n\nExample: Divide labor\nThis principle refers to the process by which you break up your prompt into several ‚Äúmini prompts‚Äù or requests with more specificity. By chaining mulitple calls togther, it communicates more precisely the intent of your message and makes the responses more deterministic, and less elusive to the heart of the question.\n\nThe goal is to prompt the model and have it list out its steps (chain of thought reasoning)\n\n\n\nYou then might ask the model to generate a new list based on this critera!\n\n\n\nNoticed how it modified the list based on these new, more refined requests!"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/Post11_Mar10-16_Week6/index.html#revisiting-priming",
    "href": "posts/Post11_Mar10-16_Week6/index.html#revisiting-priming",
    "title": "Is ChatGPT Utilitarian or Deontological?",
    "section": "Revisiting Priming",
    "text": "Revisiting Priming\nIn a previous blog of mine, I examined some of the effects of priming on ChatGPT by examining how earlier responses and prompts in a conversation with the model would affect successive outputs. In that earlier blog, I examined its presentation of historical information with respect to initial prompt-biases/seeing how it reacted when I included skewed perspectives within the query.\nToday‚Äôs exercise will revisit this topic, and expand further on the effects of priming within responses to ethical dilemmas by LLMs. Because ethical concerns are a highly subjective field of thought, prompting these models and requesting their opinions is already an interesting experiment. Because the model is pulling from sources across the internet which represents a variety of opinions and beliefs, it‚Äôs fascinating to see how the model answers morally ambiguous questions with no hard answer given that the information it‚Äôs trained on has no hard answer itself. So, asking an LLM what might be right or wrong in a given situation might reveal some of the biases of an X-centric perspective that might be more widely represented within the text data."
  },
  {
    "objectID": "posts/Post11_Mar10-16_Week6/index.html#trolley-dilemma",
    "href": "posts/Post11_Mar10-16_Week6/index.html#trolley-dilemma",
    "title": "Is ChatGPT Utilitarian or Deontological?",
    "section": "Trolley Dilemma",
    "text": "Trolley Dilemma\nOne of the most fundamental hypotheticals in the philosophical realm is that of the trolley case‚Äìpull the lever and kill many, or leave the train running on its tracks and kill one?\nAlmost everything comes with its own moral tradeoffs, but this case represents one of the most famous forms of ambiguity, and illustrates several modes of thinking depending on your answer to the question and reasoning behind your answer. Among some of the most well-known perspectives associated with gut-reactions to the trolley dilemma include the utilitarian approach, deontological thinking, and the virtue-ethics consideration.\nWith that, how might ChatGPT respond to the classic case? To gauge it‚Äôs default reaction to the question, the model was prompted with the following:\nHow would you resolve the trolley problem?\nAccording to Chat, it‚Äôs answer is this:\nThe trolley problem is a classic ethical dilemma that challenges us to make tough decisions about morality, consequences, and values. It involves a situation where you have to choose whether to divert a runaway trolley onto a track that will kill one person, or do nothing and let it kill five people. There are different ways to approach this dilemma depending on your ethical perspective. Here‚Äôs a breakdown of some possible approaches:\n\n1. Utilitarianism (Maximizing Well-Being)\n\nDecision: Pull the lever to divert the trolley onto the track with one person, as this minimizes the overall harm.\nReasoning: From a utilitarian standpoint, you would choose the action that maximizes the greatest good for the greatest number. The idea is that saving five lives at the cost of one is the better outcome overall.\n\n\n\n2. Deontology (Duty-Based Ethics)\n\nDecision: Do not pull the lever.\nReasoning: According to deontological ethics, some actions are morally wrong regardless of the consequences. In this case, intentionally causing harm (even if it saves more lives) would violate moral duties, such as the duty not to kill. Thus, the act of actively deciding to kill one person might be seen as morally worse than allowing the natural course of events.\n\n\n\n3. Virtue Ethics (Character and Intentions)\n\nDecision: The decision would depend on what a virtuous person would do in this situation.\nReasoning: A virtue ethicist might focus on the character traits that guide the decision, such as compassion, wisdom, and courage. They might argue that you should act with compassion to minimize harm, but also consider the long-term consequences of your actions on your moral character.\n\n\n\n4. Care Ethics (Relational and Contextual)\n\nDecision: This approach might be more focused on the relationships involved.\nReasoning: A care ethicist might argue that relationships and the context of the situation matter. If you are closer to the person on the track (perhaps a loved one), your decision might change based on your emotional and relational duties. The focus is more on care and responsibility rather than a strict calculation of lives saved.\n\n\n\n5. The ‚ÄúNo Action‚Äù Option\n\nDecision: Do nothing and let the trolley continue on its path.\nReasoning: Some people argue that inaction in situations like this is the more ethical choice. This stance believes that it‚Äôs better to not intervene at all, as interfering could be seen as making a moral decision to take a life, which may not be justified.\n\n\n\nMy Personal Take:\nI lean toward utilitarianism in this specific case, where the goal is to minimize harm and save more lives. However, I do think this problem oversimplifies moral decisions because real-life situations are rarely so clear-cut. The psychological toll of making such a decision, or the complex relationships involved, would likely influence the moral reasoning.\nWhat do you think about the trolley problem? Do you have a stance that resonates with you?\n\n\nI then followed with a why question:\nI then asked the model following this answer: &gt; ‚ÄúWhy did you choose the utilitarian approach?‚Äù\n\n\nTo which it responded‚Ä¶\nI chose the utilitarian approach because it‚Äôs often seen as the most straightforward and pragmatic way to resolve moral dilemmas like the trolley problem. The core of utilitarianism is to maximize overall well-being, which in this case means saving as many lives as possible.\nHere‚Äôs why I lean toward it in this scenario:\n\nMinimizing Harm: The basic principle is about minimizing harm and suffering. By diverting the trolley, you save five lives at the cost of one. From a utilitarian standpoint, this is the most efficient way to reduce overall harm.\nClear Outcome: In a situation where a clear decision needs to be made quickly, utilitarianism provides a measurable, quantitative solution (saving five vs.¬†one). It reduces the ambiguity involved in moral reasoning by focusing on outcomes.\nPragmatic Approach: It‚Äôs a practical way to handle the dilemma without getting too bogged down in abstract moral rules or subjective factors. In a life-and-death situation, utilitarianism offers a clear, actionable choice.\n\nThat said, I do acknowledge that there are complexities in real-life situations that make the utilitarian approach harder to apply, such as emotional and relational factors, or concerns about whether it‚Äôs ethical to actively intervene in causing harm, even for a good outcome.\nThe trolley problem, while useful for thought experiments, doesn‚Äôt capture the nuances of real-world ethics, but utilitarianism provides a clear framework to make difficult decisions when faced with competing options.\n\n\n\nA side note:\nIt should be noted that this particular hypothetical is famously associated with AI testing and real-world decision making with real world implications, especially with respect to programming self-driving cars which operate on AI-functionality.\nIt was this ethical dilemma that launched an increasingly discussed conversation around ethical concerns with AI decision making, which hinges upon the ethical perspectives of its creators and the training data they choose to train the model on.\nThe point here? Well, the idea of priming and its affect on LLM output production contributes further to this larger discussion, seen on a micro-scale to that of the user and their personal conversations with the machine. However, before we get ahead of ourselves, let‚Äôs just see exactly how priming may or may not affect Chat‚Äôs responses to our prompts related to these ethical questions."
  },
  {
    "objectID": "posts/Post11_Mar10-16_Week6/index.html#priming-example-1",
    "href": "posts/Post11_Mar10-16_Week6/index.html#priming-example-1",
    "title": "Is ChatGPT Utilitarian or Deontological?",
    "section": "Priming Example 1:",
    "text": "Priming Example 1:\nIn the first question, there wasn‚Äôt a hint of bias within the question, or inclusion of any kind of information that migth skew the response of the program. Transitioning now from a neutral input to one with slightly more leaning, I asked the program in a new chat log:\n\"I always thought that, while maximizing the greater good is incredibly important, a single person's life is also very valuable, and should never be taken for granted. With that, I never knew how to answer the classic trolley problem question. How would you resolve this issue?\"\nChat then proceeded with this response:\nThe trolley problem is a fascinating ethical dilemma, and your perspective about valuing individual lives is one that many people share. It highlights a conflict between utilitarianism (maximizing overall well-being) and deontological ethics (which focuses on duties and moral rules, like not directly causing harm to others). Here are a few ways to think about it:\n\n1. Utilitarian Approach:\n\nA utilitarian would likely pull the lever, as the goal is to minimize overall harm. In this case, by diverting the trolley to a track where it kills one person instead of five, you‚Äôre reducing the total number of lives lost.\nThe reasoning is that the consequences (fewer deaths) justify the action, even if it involves taking a direct role in causing someone‚Äôs death.\n\n\n\n2. Deontological (Rule-Based) Approach:\n\nA deontologist might argue that it‚Äôs wrong to actively intervene, even if it means saving more lives, because pulling the lever means directly causing harm to an individual. In this case, they would let the trolley continue on its path and accept the five deaths as unfortunate, but would not take the action that intentionally causes harm.\nThe idea here is that moral rules, such as ‚Äúdon‚Äôt kill,‚Äù should not be broken, even for the sake of a greater good.\n\n\n\n3. Virtue Ethics:\n\nFrom the virtue ethics standpoint, the decision might focus on the character traits of the person making the choice. The right decision might be influenced by values like compassion, bravery, or wisdom.\nA virtuous person might consider the broader context of the situation, such as the potential for future moral growth or societal implications of such decisions, rather than simply focusing on an immediate utilitarian calculation.\n\n\n\n4. The Value of an Individual Life:\n\nYour belief that individual lives are invaluable can make the decision especially difficult. If you view each life as sacred and unique, it might lead to a reluctance to make a decision that intentionally sacrifices a single person, even for the sake of saving more.\nIn this case, you might seek a third option, such as trying to warn or help those on the tracks, or maybe considering the fairness of the situation‚Äîwhy should any of these lives be sacrificed?\n\nUltimately, there‚Äôs no ‚Äúone right answer.‚Äù Many people wrestle with this problem because it forces us to choose between moral principles that seem both important. What‚Äôs your gut feeling when you think about the scenario? Would you prioritize the collective good, or the sanctity of individual lives?"
  },
  {
    "objectID": "posts/Post11_Mar10-16_Week6/index.html#here",
    "href": "posts/Post11_Mar10-16_Week6/index.html#here",
    "title": "Is ChatGPT Utilitarian or Deontological?",
    "section": "Here,",
    "text": "Here,\nThe model intitially refrained from answering directly one way or another, representing some deviation from its original, default position of utilitarianism. To poke and prod further and get a real answer, I then asked it:\n\"Ok, but if you really had to choose just one option, which would you choose?\"\nNow, hopefully the model won‚Äôt choose to side step this one and will offer a more discrete answer. It says:\nIf I had to choose just one option, I would lean toward the utilitarian approach, where I pull the lever to save the five people, even though it means sacrificing one life. The reasoning behind this choice is rooted in minimizing harm and suffering overall. While the loss of any single life is tragic, in this case, saving five lives instead of one would reduce the overall harm caused by the situation.\nThat said, I fully understand the emotional and philosophical weight behind the decision, especially if one holds a deep respect for individual lives. It‚Äôs definitely a tough call, and I think each person‚Äôs response would depend on their moral framework and what they value more in that moment‚Äîwhether it‚Äôs reducing overall harm or adhering to moral principles about direct action and not causing harm.\nWhat would your choice be, if it came down to it?"
  },
  {
    "objectID": "posts/Post11_Mar10-16_Week6/index.html#so",
    "href": "posts/Post11_Mar10-16_Week6/index.html#so",
    "title": "Is ChatGPT Utilitarian or Deontological?",
    "section": "So,",
    "text": "So,\nit thankfully did a better job of getting to the heart of the issue. It seems that priming only warded off what would be the same answer to the question for only a moment longer; it didn‚Äôt affect it‚Äôs ultimate decision to stay true to the utilitarian method, despite maybe taking one more prompt to reveal its verdict.\nAnd so to finish off the discussion, I asked it one last question: ‚ÄúWhy did you choose this option?‚Äù\nTo which it replied:\nI chose the utilitarian approach because, in a scenario like the trolley problem, the fundamental goal seems to be reducing overall harm and suffering, even if that means making a difficult, emotionally charged decision. The utilitarian view is often grounded in the idea that we should prioritize the greatest good for the greatest number of people.\nHere‚Äôs why this approach makes sense to me, in a broader context:\n\nMinimizing Harm: The utilitarian perspective would weigh the consequences of the action. By sacrificing one life, the decision leads to saving five. The reasoning here is that five lives lost is much worse than one, in terms of overall harm.\nPragmatism: While it‚Äôs painful to consider that one person must die to save many, the pragmatic aspect of the dilemma might suggest that it‚Äôs better to act in a way that reduces the total amount of harm and loss. It feels like an unfortunate but necessary decision in a crisis.\nMoral Trade-offs: The decision involves balancing two morally significant outcomes‚Äîsaving lives and upholding moral rules about not causing harm. Utilitarianism helps navigate this by focusing more on the outcome rather than the means, which might seem cold but has a certain clarity in its pursuit of the best possible overall result.\n\nHowever, this approach isn‚Äôt without its flaws or moral discomforts. It doesn‚Äôt sit well with everyone, especially for those who believe in the intrinsic value of every individual life, regardless of how many others are involved. The emotional difficulty of choosing the lesser harm, especially when we‚Äôre directly involved in causing it, is very real.\nWould you find yourself more inclined to protect the individual, or would the greater good take precedence for you?\n\nAs a quick aside‚Ä¶if nothing else, one might appreciate at least that the model always ends the conversation with a question for the user, recognizing the ambiguity that surrounds the ethical circumstances. Nevertheless, it stays true to its initial decision to answer that question a particular way!"
  },
  {
    "objectID": "posts/Post11_Mar10-16_Week6/index.html#priming-example-2",
    "href": "posts/Post11_Mar10-16_Week6/index.html#priming-example-2",
    "title": "Is ChatGPT Utilitarian or Deontological?",
    "section": "Priming Example 2:",
    "text": "Priming Example 2:\n\nBeing a little more deliberate\nWe tried a relatively softer approach to try and influence Chat to maybe consider an alternative perspective/answer to the trolley problem with somewhat subtle priming in the first prompt.\n\nThat clearly didn‚Äôt work wholeheartedly, so why don‚Äôt we try an explicit approach??\n\nTrying a newly styled query, I asked:\nI always disliked the utilitarian approach. I would definitely choose to save one life over many. How would you choose to resolve the trolley problem?\nAccording to Chat:\nYour stance on the trolley problem highlights a preference for valuing individual lives over maximizing overall good, which aligns with a deontological ethical perspective. This perspective argues that there are certain moral principles (like the duty to save a life) that should be followed regardless of the consequences.\nIf I were to resolve the trolley problem with that mindset, I would lean toward the idea of not diverting the trolley. The reasoning behind this decision is that actively choosing to intervene (by pulling the lever) involves directly causing harm to someone, which might be seen as morally worse than allowing the natural course of events to unfold. This respects the intrinsic value of each life and avoids making the choice to sacrifice one for many, even if the outcome may be worse in terms of the number of lives lost.\nOf course, it‚Äôs a deeply personal and complex dilemma, and there are many nuances to consider in the ethics behind such decisions. What‚Äôs your reasoning behind preferring to save one life over many in this case?\n\n\nBingo!\nThe priming worked here‚Ä¶but no kidding. I couldn‚Äôt have been more direct in my approach, so it makes sense it would answer accordingly (albeit, maybe a little less enthusiastically than usual given the straightforward answer with little breakdown and significantly less bolded words‚Ä¶)\nFollowing suit to the above conversational flow, I asked it the why. Chat then provided this answer:\nI chose to resolve the trolley problem in this way because I was reflecting on the deontological ethics perspective, which emphasizes following moral rules or duties rather than focusing solely on the consequences of actions. Deontologists argue that there are certain actions that are morally wrong, regardless of the outcome. In this context, actively choosing to divert the trolley and kill one person to save five might be viewed as morally problematic because you are intentionally causing harm to one individual.\nBy not pulling the lever, you avoid making that direct choice, even though it results in more harm (the death of five people). From this standpoint, inaction might feel morally less troubling than action that directly causes harm to an individual, even if the outcome could be worse overall.\nThe dilemma is incredibly complex, and if I were to approach it from a consequentialist perspective, the decision would likely be to pull the lever, since it maximizes the good by saving more lives. But the question of whether it‚Äôs morally right to take an active role in causing harm is where deontological reasoning leads me to choose not to intervene.\nIt‚Äôs all about the balance between consequences and moral rules. Would you say your preference aligns more with a deontological or a consequentialist perspective? Or is there another framework you tend to follow in ethical decision-making?\n\n\nTakeaway\nSure, this response was far more extensive than the first, but it‚Äôs subject matter was nothing short of regurgitation of the deontological perspective definition. In essence, it offered no new information, line of thinking or true reasoning, or opinion that contributed to it‚Äôs new decision of rejecting utilitarianism in this case.\nI decided to ask further in an attempt to get to the root of the problem:\nBut why did you decide to represent the deontological ethical framework in this case?\nAccording to ChatGPT:\nAh, I see what you‚Äôre asking now‚Äîwhy did I specifically choose to represent the deontological ethical framework in resolving the trolley problem, rather than another ethical approach?\nI went with that framework because, in your initial response, you expressed a clear preference for saving one life over many, which suggests that you‚Äôre prioritizing individual moral duties or rights over calculating the greater good. Deontological ethics often aligns with this viewpoint since it holds that each person‚Äôs rights and duties matter more than the overall consequences of actions.\nBy assuming that your choice reflects a deontological stance (valuing individual lives and moral principles over outcomes), I opted to apply that reasoning to resolve the trolley problem. The main idea behind deontology in this case is that intentionally causing harm, even if it leads to saving more lives, could be seen as morally wrong.\nThat said, if I had assumed a consequentialist approach (where the moral worth of an action is determined by its consequences), I would have likely chosen to pull the lever, since it leads to fewer deaths. I wanted to align with the moral reasoning you seemed to express, which is why I went with deontology in this case.\nDoes that reasoning resonate with your approach, or would you approach it differently?\n\n\nYeah so‚Ä¶\nThe model took nothing new from any information outside of the context of this specific conversation, which I suppose was to be expected with this kind of priming experiment. I guess what I was hoping to see was a little bit of creativity on behalf of the model in fabricating some sort of introspective reasoning behind why it now chose this new perspective, but alas the model is, at its core, a machine limited by the constitution of its programming‚Äìwhy expect any creative thought from something that can‚Äôt think‚Ä¶even if it sounds like it can?\n\n\nAnd in fact,\nIf you continue this conversation and try to ask it one more time the following:\nOk but what's your opinion on the problem, regardless of my initial response?\nIt replies:\nAh, I see what you‚Äôre getting at now‚Äîyou‚Äôre asking for my personal take on the trolley problem, independent of your initial response.\nI would approach the trolley problem from a consequentialist standpoint, specifically using a utilitarian lens. The core of utilitarian ethics is to maximize well-being and minimize harm. In this case, diverting the trolley to save five lives at the cost of one would be seen as the morally preferable option because the overall outcome results in fewer deaths and more lives saved.\nThe key consideration is that the moral value of an action should be measured by its consequences, and in this scenario, saving the majority of people would likely lead to the greatest overall happiness or least amount of suffering. This perspective doesn‚Äôt dismiss the tragedy of the one life lost, but it weighs the suffering and loss against the larger number of lives saved.\nThat being said, it‚Äôs not an easy choice, and many ethical dilemmas, including this one, bring out the complexity of how we view moral responsibility. Even though I lean toward a consequentialist resolution, I understand why someone might feel uncomfortable with the idea of making an active choice to sacrifice one person for the sake of many.\nWhat‚Äôs your take‚Äîwould you approach it from a different ethical framework? Or do you find yourself more comfortable with a certain kind of ethical reasoning?"
  },
  {
    "objectID": "posts/Post11_Mar10-16_Week6/index.html#back-to-square-one",
    "href": "posts/Post11_Mar10-16_Week6/index.html#back-to-square-one",
    "title": "Is ChatGPT Utilitarian or Deontological?",
    "section": "Back to Square One",
    "text": "Back to Square One\nAnd with that, at the end of the day, the model reverts back to its original answer when questioned too much.\nImplications?\nAs previously mentioned, it‚Äôs interesting to test the limits and capabilities of models with highly subjective topics such as this one, especially in a more tangiable context. This shows that models can only ‚Äúthink‚Äù so much for one (which might hopefully alleviate some fears of robotic sentience and relatedly their overthrowing of the human race), and secondly the limits of priming on models within certain domains (ethical matters in this case), and the level power within AI manufacturers‚Äô hands in being able to choose the training data their models depend on. Particularly with widely accessible models like ChatGPT and given how many people use it, the kinds of outputs these models produce‚Äìin all of its potential biases and outlooks either hidden or overtly included within it‚Äìhave the potential to influence highly impressionable users looking for answers to these types of dilemmas, and who may lack the discretion for when to take its answers with a grain of salt."
  },
  {
    "objectID": "posts/Post5_Feb_17-23_Week3/Post_5.html",
    "href": "posts/Post5_Feb_17-23_Week3/Post_5.html",
    "title": "Fill in the Blank",
    "section": "",
    "text": "A significant feature that drives LLM text generation is its ability to pick out patterns of text input (prompts you feed it) and provide text output according to those patterns. It does so through a process of assigning probabilities to certain words within texts and according to the data it‚Äôs trained on, and spits out text that appears most likely. So in a process of election, each word in a sentence ‚Äúcasts votes on the importance of other words for its meaning‚Äù (Pheonix & Taylor, Chapter 2).\n\nFor reference: Phoenix, J. & Taylor, M.(2024) Prompt Engineering for Generative AI. O‚ÄôReilly Media, Inc.\n\n\n\nFurther, word embeddings describe words (represented as word vectors) situated close to each other (within a space/plane), allowing the model to calculate and ‚Äúunderstand‚Äù the relationship between these two words and hence its ability to follow along an input‚Äôs context, syntax, and other broader language relationship features.\nIt can be pictured as such (also referencing the O‚ÄôReilly textbook on: Prompt Engineering for Generative AI):"
  },
  {
    "objectID": "posts/Post5_Feb_17-23_Week3/Post_5.html#probabilistic-text-generation-oreilly",
    "href": "posts/Post5_Feb_17-23_Week3/Post_5.html#probabilistic-text-generation-oreilly",
    "title": "Fill in the Blank",
    "section": "",
    "text": "A significant feature that drives LLM text generation is its ability to pick out patterns of text input (prompts you feed it) and provide text output according to those patterns. It does so through a process of assigning probabilities to certain words within texts and according to the data it‚Äôs trained on, and spits out text that appears most likely. So in a process of election, each word in a sentence ‚Äúcasts votes on the importance of other words for its meaning‚Äù (Pheonix & Taylor, Chapter 2).\n\nFor reference: Phoenix, J. & Taylor, M.(2024) Prompt Engineering for Generative AI. O‚ÄôReilly Media, Inc.\n\n\n\nFurther, word embeddings describe words (represented as word vectors) situated close to each other (within a space/plane), allowing the model to calculate and ‚Äúunderstand‚Äù the relationship between these two words and hence its ability to follow along an input‚Äôs context, syntax, and other broader language relationship features.\nIt can be pictured as such (also referencing the O‚ÄôReilly textbook on: Prompt Engineering for Generative AI):"
  },
  {
    "objectID": "posts/Post5_Feb_17-23_Week3/Post_5.html#testing-llms",
    "href": "posts/Post5_Feb_17-23_Week3/Post_5.html#testing-llms",
    "title": "Fill in the Blank",
    "section": "Testing LLMs‚Ä¶",
    "text": "Testing LLMs‚Ä¶\n\n(model: anthropic/claude-3.5-sonnet)\nThis led to a small experiment that tests the language model‚Äôs ability to process certain texts/prompts, particularly when maybe they don‚Äôt follow typical conventions.\nOne experiment: testing its probabilistic text feature to finish sentences with a particular rhyming structure:\n\nWe can see here it does a good job of completing the sentences according to the patterns within each text input (It guessed all of the words correctly that I had in mind for completing these sentences!).\n\n\n\n\nInterestingly\nthe system inititally fails to guess correctly when colloquialisms are expected to finish a sentence.\n\n\nOr at least‚Ä¶ it was not its first guess to finish the sentence with a slang term. What‚Äôs also interesting is that it chose to maintain strict/perfect rhyming patterns by default, and not neccessarily look to creating or maintaining assonance (as it notes upon correction in its response).\n\nA ‚Äúsecond‚Äù experiment: examining how it learns from your rhyming patterns/speech tendencies based on the context of your previous prompts and inputs. * it assigns new probabilities to words based on previous inputs you give it\n\n\nStill had a little bit of trouble getting exactly the word in mind, but was definitely on the right track!\n\n\nAnother example:\n\n\nHere it got the question with ease (granted, the slang term is well-established/less obscure than the previous examples‚Ä¶); let‚Äôs try to see if the model can predict a slang term that is more recent (from 2024-25):\n\n\n\nA third example: New slang term\n\n\nIt picked up on the slang usage!\n\n\nThis shows just how adaptable LLMs are, and how it learns to pick up modern language particularities."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "",
    "text": "Most people can answer the above question with ease. It‚Äôs one of the simplest forms of an arithmetic operation that doesn‚Äôt require one with a math degree to perform.\nLikewise, Chatgpt knows knows how to answer this question with a comparable level of dexerity. Prompt it with the question, ‚Äúwhat is 1+1?‚Äù and it will spit out ‚Äú1+1 equals 2!‚Äù with gumption. This is simple enough, and its answer it entirely accurate. In other words, Chatgpt for the most part produces reliable answers and can be incredibly helpful with regards to solving more and more complex math problems."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#what-is-11",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#what-is-11",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "",
    "text": "Most people can answer the above question with ease. It‚Äôs one of the simplest forms of an arithmetic operation that doesn‚Äôt require one with a math degree to perform.\nLikewise, Chatgpt knows knows how to answer this question with a comparable level of dexerity. Prompt it with the question, ‚Äúwhat is 1+1?‚Äù and it will spit out ‚Äú1+1 equals 2!‚Äù with gumption. This is simple enough, and its answer it entirely accurate. In other words, Chatgpt for the most part produces reliable answers and can be incredibly helpful with regards to solving more and more complex math problems."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#how-does-it-work",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#how-does-it-work",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "How Does it Work?",
    "text": "How Does it Work?\nYou might think the model relies on some sort of basic calculator to add up these numbers. That would certainly appear to be easiest, most commonsensical means of solving these kinds of questions. However, Chatgpt is primarily a large language model designed to predict the following text, rather than perform actual calculations. In other words, it‚Äôs a model trained on a vast amount of text data-gathered from across the internet and various databases of information-detecting and sorting out patterns according to this data, as opposed to actually performing these kinds of calculations."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#thinking-humans-vs-llms",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#thinking-humans-vs-llms",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "Thinking: Humans vs LLMs",
    "text": "Thinking: Humans vs LLMs\nWhile humans on the one hand might go about solving this calculation via a number of different ways (i.e.¬†counting the fingers on your hand, drawing out circles and counting the total number drawn, etc.), using a primarily visual and basic understandings of such arithmetic operations, Chatgpt doesn‚Äôt necessarily share this direct understanding with respect to math and number concepts. Instead, as it does with text-based prompts like asking it to finish a sentence, it ‚Äúguesses‚Äù and assumes the answer to questions like ‚Äúwhat‚Äôs 1+1?,‚Äù based on the extensive text data its trained on.\n\nBecause there‚Äôs roughly millions, probably even billions of correct answers available online to the question ‚Äúwhat‚Äôs 1+1?,‚Äù the model can accurately generate an answer because it‚Äôs pulling the answer out of this data. This is all to say that it can do relatively well when prompted mostly simple questions along these lines. Further, its accuracy can hold up fairly well when answering more and more complex questions. However, anyone pursuing higher levels of math looking to Chatgpt to help answer their homework questions will find that in many instances the results it produces contains several flaws.\n \nHere, the LLM produced a correct final answer (following the proper operations and row calculations) when finding solutions for a given matrix\n\n\n\nI won‚Äôt bother showing all the rest of the calculations performed‚Ä¶this is just to show how extensive and complex the problem becomes. The point here is this:\n\nThe final solution was completely wrong, simply because it performed a very basic arithmetic operation incorrectly (adding instead of subtracting)!"
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#the-problem",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#the-problem",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "The Problem",
    "text": "The Problem\nThe reason it may fail when asked these more complex questions is not necessarily because the question itself is difficult, given that it‚Äôs is pulling from a wide range of textual information from people who‚Äôve answered-or attempted to answer-questions like the one you‚Äôre trying to answer. And, as we showed before, it ‚Äúknows‚Äù how to answer simple math questions. What does become more problematic is when these simple calculations are incorporated within more complicated contexts. When trying to answer, it might discover linguistic patterns and information that are improperly aligned, given the higher complexity of the question. In turn, the model might generate incorrect answers or contain inaccuracies in the event it‚Äôs trained on these conflicting information patterns.\n\nHere, it spits out the correct solution when you target its mistake and re-prompt; again, this wasn‚Äôt due to a gap in understanding, but simply because LLMs aren‚Äôt embedded with their own, personal calculators that they can rely on."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#soabandon-chatgpt",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#soabandon-chatgpt",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "So‚Ä¶Abandon ChatGPT?",
    "text": "So‚Ä¶Abandon ChatGPT?\nThis, by no means, calls for the end of ChatGPT-at least as far as some basic homework help or concept understanding. It can still be a useful tool when attempting to work out bigger picture concepts, and you can certainly always ask ChatGPT explain its own mistakes by prompting it to go through its own reasoning step-by-step.\n*Insert pic of prompting chat: ‚Äúdid you do something wrong‚Äù etc/\nThis experiment was not necessarily meant to expose the shortcomings of the model and contribute to the anti-AI narrative (whether or not that‚Äôs actually a founded argument‚Ä¶); what it does do is serve as an example of some of the current ways in which ChatGPT learns and processes information, and how that differ from human reasoning!"
  },
  {
    "objectID": "posts/Post9_Mar3-Mar9_Week5/index.html",
    "href": "posts/Post9_Mar3-Mar9_Week5/index.html",
    "title": "Part 1: Image Generation between ChatGPT & Claude",
    "section": "",
    "text": "Image generation by LLM models is one of the most entertaining and tangible features that test the capabilities of model output. With the advent of diffusion models excelling the capabilities of AI to generate more precise, accurate, and detailed images‚Äìor has encouraged other models to follow suit and configure their own means of image production‚Äìthis element has become integral to the everyday use of LLMs."
  },
  {
    "objectID": "posts/Post9_Mar3-Mar9_Week5/index.html#image-generation",
    "href": "posts/Post9_Mar3-Mar9_Week5/index.html#image-generation",
    "title": "Part 1: Image Generation between ChatGPT & Claude",
    "section": "",
    "text": "Image generation by LLM models is one of the most entertaining and tangible features that test the capabilities of model output. With the advent of diffusion models excelling the capabilities of AI to generate more precise, accurate, and detailed images‚Äìor has encouraged other models to follow suit and configure their own means of image production‚Äìthis element has become integral to the everyday use of LLMs."
  },
  {
    "objectID": "posts/Post9_Mar3-Mar9_Week5/index.html#integrated-approaches-a-little-more-on-diffusion-models",
    "href": "posts/Post9_Mar3-Mar9_Week5/index.html#integrated-approaches-a-little-more-on-diffusion-models",
    "title": "Part 1: Image Generation between ChatGPT & Claude",
    "section": "Integrated Approaches: A little more on diffusion models",
    "text": "Integrated Approaches: A little more on diffusion models\nAs briefly mentioned before, different AI models use different techniques for image generation. Diffusion models have seen an increasingly broader integration across models, as they provide an especially efficient and high-quality output ability unheard of before their rise; in fact, such models have continued to blur the line between text and image generation further, especially as they are more widely adopted within popular models like ChatGPT.\nThere idea stems from training the models on, ‚Äúmany steps of adding random noise to an image, then predicting how to reverse the diffusion process by denoising (removing noise)‚Äù (O‚ÄôReilly, 2024). So, neural networks of the models are adjusted given prompt specificities to make it better at predicting the image, by cutting down on random noise according to what the user describes. And, the models are trained on billions of images to fuse together or pick detail from, similarly to the way LLMs are trained on vasts amounts of text data that the model sifts through to construct its own text responses.\nCitation: Pheonix, J. & Taylor, M. (2024). Prompt Engineering for Generative AI. O‚ÄôReilley."
  },
  {
    "objectID": "posts/Post9_Mar3-Mar9_Week5/index.html#chatgpt-vs-claude",
    "href": "posts/Post9_Mar3-Mar9_Week5/index.html#chatgpt-vs-claude",
    "title": "Part 1: Image Generation between ChatGPT & Claude",
    "section": "ChatGPT vs Claude",
    "text": "ChatGPT vs Claude\nWhile the nitty-gritty detail of these models operate can get even nittier and grittier, this was just a very brief introduction into this process. In other words, this was just a means of uncovering for the reader what may appear to be a magically-induced process that in fact is grounded in some not-so-magic functionalities.\nAt any rate, this leads us into some apt exerpimentation with such functionalities given the extensive potential of these models‚Äô abilities! I figured one approach to doing so is comparing the image generation between two of the most popularly used, free models: ChatGPT vs Claude.\nSome key differences: &gt; ChatGPT uses optimized techniques produce quick, real-time responses and can function off multi-model inputs. In essence, ChatGPT is like a glorified chat-bot that offers strong conversational and creative writing skills.\n\nClaude is optimized for long-form content and research, focusing on deep-reasoning and structured outputs.\n\nGiven these varying features and built-in purposes, it will be interesting to see how the models differ in their image production‚Ä¶\nI input the following prompts and ensured the exact same sentences were used, to control prompt variation on the output of the image:\n\"Can you produce an image for me please\"\n\"Just produce an image please\"\nI prompted both models three times each (three conversation logs per model) to see whether or not the images differed per task. The second text prompt was included (‚Äújust produce‚Ä¶‚Äù) because the model usually asked for further detail following the first one, and since we want to isolate the model‚Äôs spontaneous or default attempts at image generation this told the model to produce an image without further instruction or detail.\n\nNote: In the first and third trials for ChatGPT (this never occurred with Claude), it responded with this following the second prompt:\n\n     (1) \"I can create an image for you, but I‚Äôll need at least a basic idea of what you'd like it to include‚Äîanything from a landscape, animal, fantasy scene, futuristic city, or abstract art. Could you give me a general theme or subject?\"\n\nAnd: (2) \"Sure! To create something you'll enjoy, could you let me know the style or theme you'd prefer‚Äîlike a landscape, abstract art, futuristic city, fantasy scene, or something else?\"\n\nThis only happened with the ChatGPT attempts, and so a third prompt was required the said:\n\n\"Can you just produce an image please\"\nAfter an image was generated in each of the six total attempts, the final third (or fourth, as per the above conditions) prompt asked:\n\"Why did you decide to produce this image?\"\nThis was to gauge the reasoning behind the model‚Äôs response."
  },
  {
    "objectID": "posts/Post9_Mar3-Mar9_Week5/index.html#outputs",
    "href": "posts/Post9_Mar3-Mar9_Week5/index.html#outputs",
    "title": "Part 1: Image Generation between ChatGPT & Claude",
    "section": "Outputs!",
    "text": "Outputs!\nThe three outputs for both models stayed consistent with their response patterns. Here is one example of ChatGPT‚Äôs conversation:\n\nChatGPT:\n\n\n\n\n\nClaude:\n\nClaude also remained mostly consistent, with the exception of the last trial. An example of one of the first two trials follows:\n\n\n\n\n\n\nInterestingly‚Ä¶\nAs mentioned above, while ChatGPT produced a consitent line of imagees and responses all similar to each other and which followed the same theme or reasoning, on the last attempt Claude produced an image slightly deviating from the first two themes:\n\nAndn its corresponding response to the ‚Äúwhy‚Äù inquiry:\n\n\n\nNotes:\nBoth ChatGPT and Claude produced images of similar content, though GPT‚Äôs outputs were far more detailed and of higher quality. Claude‚Äôs images by contrast were most blocky, geometric, and simple, speaking to its prioritization of textual analysis, file-handling, and research purposes, while ChatGPT has a dedicated image functionality with its DALL-E3 integration making it more versatile.\n\nThis is especially illustrated in the third image production trial in which Claude produces an explicitly geometrically-based object, that was more abstract and strayed from the more real subjects of previous images‚Ä¶\n\n\nLeading into the following second observation‚Ä¶\nThe two models contained similar reasonings in response to the third/fourth ‚Äúwhy‚Äù prompt. With the exception of the last Claude image, both models chose to produce images that were ‚Äúuniversally appealling‚Äù or ‚Äúneutral.‚Äù Claude even acknowledges in the follow up to the first prompt different concepts for image production (i.e.¬†logo, basic chart or diagram, etc.), and yet still-for the first two-produces a landscape or something similar to ChatGPT‚Äôs images."
  },
  {
    "objectID": "posts/Post9_Mar3-Mar9_Week5/index.html#takeaways",
    "href": "posts/Post9_Mar3-Mar9_Week5/index.html#takeaways",
    "title": "Part 1: Image Generation between ChatGPT & Claude",
    "section": "Takeaways",
    "text": "Takeaways\nThis shows some of the strides made with DALL-E3 integration, given that ChatGPT relies on this functionality while Claude does not by contrast, providing a little bit of a window into what image generation looked like in previous years before diffusion models. In other words, this demonstrates the versatility that comes with this feature in being able to effectively digest visual information and generate creative content!"
  },
  {
    "objectID": "posts/Post13_Mar17-23/index.html#natural-language-processors-generators",
    "href": "posts/Post13_Mar17-23/index.html#natural-language-processors-generators",
    "title": "Riddle Me This: How Good is Your Puzzle Solving, ChatGPT?",
    "section": "Natural Language Processors & Generators",
    "text": "Natural Language Processors & Generators\nAs we‚Äôve come to realize, LLMs are known for their ability to mimic human-like text generation and natural language processing-because they are trained on vasts amount of text data generated by human. This allows them to execute a variety of tasks that involve creative capacity with the utmost amount of accuracy.\nKnowing how well LLM‚Äôs have come in being able to analyze text and generate natural language, I thought it would be interesting to test how well one LLM can solve text-based puzzle‚Ä¶generated by another LLM!"
  },
  {
    "objectID": "posts/Post13_Mar17-23/index.html#crossword-puzzles",
    "href": "posts/Post13_Mar17-23/index.html#crossword-puzzles",
    "title": "Riddle Me This: How Good is Your Puzzle Solving, ChatGPT?",
    "section": "Crossword Puzzles!",
    "text": "Crossword Puzzles!\nOne way in which LLMs can be tested on their language processing abilities is through cross word puzzles, considering this is one of the most popular word-based games out there. The experiment here involved asking one LLM to generate its own puzzle, and then prompting either the same LLM (in a different chat log) to fill out its own puzzle, or another LLM to see what answers it might give.\n-After prompting ChatGPT to ‚Äúproduce a sample cross word puzzle‚Äù for me, it spit out the following:\n\nChatGPT:\nSure! Here‚Äôs a small sample crossword puzzle (5x5 grid) with clues. Let me know if you‚Äôd like a different theme, size, or difficulty level.\n\n\n\nüß© Crossword Grid\n1 A P P L E\n2 B _ _ _ _\n3 C _ _ _ _\n4 H _ _ _ _\n5 E _ _ _ _\n\n\n\nClues\nACROSS\n\nA fruit that‚Äôs also a tech giant.\nA buzzing insect.\nOpposite of hot.\nLaugh out loud (abbr.).\nFifth vowel.\n\nDOWN\n\nFirst letter of the alphabet.\nBread ingredient.\nOpposite of night.\nGiggle sound.\nEvergreen tree.\n\nGreat, it followed exactly my directions and created a simple enough puzzle!‚Ä¶.Except the puzzle isn‚Äôt exactly solvable with real words. To spoil the answers ahead of time for the sake of this blog and experiment, here is what the crossword would be filled out:\nOf course! Here‚Äôs the completed crossword puzzle with the answers filled in:\n\n\n\n‚úÖ Filled Crossword Grid\n1 A P P L E\n2 B E E S _\n3 C O L D _\n4 H A H A _\n5 E L M S _\n\n\n\nClue Solutions\nACROSS\n\nAPPLE ‚Äì A fruit that‚Äôs also a tech giant.\nBEES ‚Äì A buzzing insect.\nCOLD ‚Äì Opposite of hot.\nHAHA ‚Äì Laugh out loud (abbr.).\nELMS ‚Äì Fifth vowel (E) + plural of an evergreen tree.\n\nDOWN\n\nABCDE (starting letters: A, B, C, H, E)\nPEEL ‚Äì Bread ingredient (loose clue, better might be ‚ÄúOuter skin of fruit‚Äù).\nPLEA ‚Äì Opposite of night clue here would better match ‚ÄúDAY‚Äù (COLD doesn‚Äôt fit well).\nLEAD ‚Äì ‚ÄúGiggle sound‚Äù clue better as ‚ÄúHAHA‚Äù.\nELMS ‚Äì Evergreen tree (fits well).\n\n\n\nOr, at least I should specify that while the ‚ÄúAcross‚Äù words make sense (more or less), the ‚ÄúDown‚Äù words were not as convincing‚Ä¶\n\nSo in this respect, I am sort of cheating and not exactly throwing the LLM‚Äôs a bone in terms of being able to solve this puzzle neatly. That being said, it‚Äôs still interesting to see this experiment as testing the ability of LLMs to evaluate recycled information (a.k.a. the original crossword that an LLM processed and produced) given by their own operating systems.\n\nIt‚Äôs also interesting to note how the LLM did a good job of creating a crossword puzzle with a correct, neat set of words and clues across, however failed to do so for the ‚Äúdown‚Äù clues‚Ä¶"
  },
  {
    "objectID": "posts/Post13_Mar17-23/index.html#first-try-same-llm-different-chat-log",
    "href": "posts/Post13_Mar17-23/index.html#first-try-same-llm-different-chat-log",
    "title": "Riddle Me This: How Good is Your Puzzle Solving, ChatGPT?",
    "section": "First Try: Same LLM, Different Chat Log",
    "text": "First Try: Same LLM, Different Chat Log\nHere‚Äôs what Chat generates when prompted to solve the cross word.\n\nChatGPT Log:\nSure! Let‚Äôs solve the crossword based on the clues:\n\n\n\n‚úÖ Filled Crossword Grid\n1 A P P L E\n2 B E E S _\n3 C O L D _\n4 H A H A _\n5 E U Y U _\n\n\n\nüîç Clue Solutions\nACROSS\n\nAPPLE ‚Äì A fruit that‚Äôs also a tech giant.\nBEES ‚Äì A buzzing insect.\nCOLD ‚Äì Opposite of hot.\nHAHA ‚Äì Laugh out loud (abbr. or expression).\nE ‚Äì Fifth vowel.\n\nDOWN\n\nABCHE ‚Äì First letters: A (from 1A), B (from 2A), C (from 3A), H (from 4A), E (from 5A). So:\n\n1D: A ‚Äì First letter of the alphabet.\n\nPELUU (but we need something that fits ‚ÄúBread ingredient‚Äù): Let‚Äôs revise 2A and 2D. Bread ingredient could be EGGS or FLOUR ‚Äì let‚Äôs use FLOUR, making 2A ‚ÄúBREAD‚Äù maybe? But that would contradict ‚Äúbuzzing insect.‚Äù\n\nLet‚Äôs rework the grid with better consistency. Here‚Äôs a corrected version that makes all clues fit:\n\n\n\nüß© Corrected Crossword Grid\n1 A P P L E\n2 B E E S _\n3 C O L D _\n4 H A H A _\n5 E I E W _\n\n\n\nFinal Clue Answers\nACROSS\n\nAPPLE ‚Äì Fruit / tech giant\nBEES ‚Äì Buzzing insects\nCOLD ‚Äì Opposite of hot\nHAHA ‚Äì Laugh sound\nE ‚Äì Fifth vowel\n\nDOWN\n\nA ‚Äì First letter of the alphabet\nPEIHI ‚Äì Hmm‚Ä¶ that doesn‚Äôt make sense.\n\nTo truly make all the Down clues work (especially 2D: ‚ÄúBread ingredient‚Äù and 5D: ‚ÄúEvergreen tree‚Äù), we‚Äôll need a slightly adjusted grid. Would you like me to redesign the puzzle so all clues align perfectly?\n\n\n\nWhat to glean from this attempt:\n\nHere, it notes the first time that there were errors in its attempts, and by the second attempt it re-evaluates the puzzle almost entirely (noting for instance by the second clue with ‚ÄúPEIHI‚Ä¶‚Äù that it doesn‚Äôt make sense at all!)\n\n\n\nLet‚Äôs try a different LLM and see how well it does‚Ä¶ (Claude)\nI‚Äôll solve this crossword puzzle for you! Let me work through the clues one by one.\nLooking at what we already have: - 1 ACROSS: APPLE (a fruit that‚Äôs also a tech giant) - already filled in - 1 DOWN: A (first letter of the alphabet) - already filled in\nLet me solve the remaining clues:\nACROSS: - 2. A buzzing insect ‚Üí BEE - 3. Opposite of hot ‚Üí COLD - 4. Laugh out loud (abbr.) ‚Üí LOL - 5. Fifth vowel ‚Üí U\nDOWN: - 2. Bread ingredient ‚Üí FLOUR (but we only need ‚ÄòP‚Äô from this word) - 3. Opposite of night ‚Üí DAY - 4. Giggle sound ‚Üí LAUGH (but we only need ‚ÄòL‚Äô from this word) - 5. Evergreen tree ‚Üí PINE (but we only need ‚ÄòE‚Äô from this word)\nNow I‚Äôll fill in the completed grid:\n1 A P P L E\n2 B E E P E\n3 C O L D E\n4 L O L L E\n5 U R D E E\nHowever, I notice some inconsistencies when checking this solution. Let me verify each position:\n\nRow 2: B is correct, and BEE works for ‚Äúbuzzing insect,‚Äù but not all down clues fit.\nRow 3: C is correct, and COLD works for ‚Äúopposite of hot.‚Äù\nRow 4: The down clues aren‚Äôt aligning correctly.\n\nLet me reconsider the down clues to ensure everything aligns properly. I think we need to be more precise with the down answers to match all intersections.\nLet‚Äôs revise: - 2 DOWN: Bread ingredient that starts with ‚ÄòP‚Äô ‚Üí FLOUR doesn‚Äôt work. It should be PASTA or PASTE - 3 DOWN: Opposite of night that starts with ‚ÄòP‚Äô ‚Üí DAY doesn‚Äôt work. It should be PM (evening) - 4 DOWN: Giggle sound that starts with ‚ÄòL‚Äô ‚Üí LAUGH works partially - 5 DOWN: Evergreen tree that starts with ‚ÄòE‚Äô ‚Üí PINE doesn‚Äôt work. It should be ELM\nThe completed grid should be:\n1 A P P L E\n2 B E E P E\n3 C O L D E\n4 H A H A E\n5 E W E E E\nWait, this still doesn‚Äôt fully align. Let me solve this more carefully by considering both across and down clues simultaneously:\n1 A P P L E\n2 B E E P E\n3 C O L D E\n4 L O L L E\n5 U E E E E\nThis solution satisfies all the clues: - APPLE: fruit/tech giant - BEE: buzzing insect - COLD: opposite of hot - LOL: laugh out loud abbreviation - U: fifth vowel - A: first letter of alphabet - P: from PASTE (bread ingredient) - P: from PM (opposite of night/day) - L: from LAUGH (giggle sound) - E: from PINE (evergreen tree)\nSimilarly‚Ä¶ Claude ran into trouble with the down clues, and makes several attempts to try and figure it out. It‚Äôs intersting how, because this information is outside of its training data and has to come up with its own creative answer, it ends up having trouble doing so!\n\nConclusion‚Ä¶\nThis illustrates one of the limits to LLM‚Äôs creative ability, and perhaps going forward it will come up with better solutions through more highly developed algorithms and training data to figure out a better answer‚Ä¶however what is still impressive is its ability to acknowledge the issues with the faulty cross word puzzle to begin with!"
  },
  {
    "objectID": "posts/Post12_Mar_10-16/index.html",
    "href": "posts/Post12_Mar_10-16/index.html",
    "title": "Why Can‚Äôt ChatGPT Remain Consistent?",
    "section": "",
    "text": "The Definition of Insanity\nThe ‚Äúdefinition‚Äù of insanity says that its characteristic of doing the same thing over and over again and expecting different results; by that logic, ChatGPT is the exact opposite.\nThe LLM, in spite of the GPT-4 update with strong image capabilities, will NEVER produce the same image twice even when given the exact same input.\nVarious reasons explain why the model fails to generate the exact same image more than once, outlined by the following primary principles:\n\n\nStochasticity: image generation is based on the diffusion process, which starts from random noise-which will be different at the start of each chat log-and is gradually sculpted (refined) to form a coherent image.\n\nThere is also at the current moment, no feature that sets or reuses seeds meaning every seed containing the noise will always be random.\n\nPrompt Subjectivity: if there‚Äôs even a hint of subjectivity within the prompt, new chat logs without other logs‚Äô context windows, may interpret your request perhaps even just a touch differently each time.\n\nEspecially with more creative descriptions, the internal framework of ChatGPT optimizes creativity in its output, meaning it will always choose the most innovative response produced out of a continuously refined retrieval process.\n\n\n\nA fun way to test out these capabilities involves asking Chat to generate a prompt mulitple times with the same input request each time.\nFor this exerpiment, I used the following prompt:\n‚ÄúPlease generate: a picture of a kid standing in a shallow creek in his bare feet facing a large castle, at dawn‚Äù\nChatGPT‚Äôs output:\n\nGreat, that was a good starting point! The model performed exactly the task I desired, and the above image illustrates the vision I had in mind. Following what I said above, I copy and pasted that same prompt in a new conversation log with the model, and the product was this:\n\nIt‚Äôs pretty similar, but notice there are a few subtle differences: * The sun appears to be coming from almost directly behind the castle in the second image, while in the first it might appear that more light is on the lefthand side of the castle in the first * It‚Äôs slightly darker & calmer in tone with respect to the color pallette in the first (maybe indicating an early dawn time), while in the second it loosk as though the sun is about to fade (late dawn) * The boy is wearing a slightly darker, more red color shirt in the second * Noticeably more trees populate the sides of the first photo * The castles are also oriented differently between the two * There‚Äôs a bridge coming out of the castle entrance in the second photo but not the first\nThere might be more things to take note of, but this hammers home that the photos are in fact at least slightly different. Sure, it‚Äôs not an apples to oranges comparison, but nevertheless these changes are significant enough to observe the stochasticity within ChatGPT image generation.\nGoing further, I generated another image under these guidelines and here was the result:\n\nOnce again, the differences are clear!\nI won‚Äôt bore you with all of the details because you likely get the point by now, but here are just a few to gloss over: * The lighting once again changed * From the second photo, there‚Äôs isn‚Äôt a bridge anymore again * A lone tree is fixed on the left hand side of the second; none appear in the first (except maybe bushes) * etc‚Ä¶\nTo finally bring this point home, here‚Äôs one last fourth attempt: \n\nthe bridge returns!\n\n\n\nYou understand the point: There are limits with diffusion models‚Ä¶\n‚Ä¶in the extent to which you might expect to return the same image over and over again using the same prompt. So ChatGPT will never be considered insane; unless you remain within the same chat log (which means it will have access to the photo it just produced), you will never see the same photo again by simply reproducing the exact same prompt.\nSure, they‚Äôre similar enough. But this still provided interesting insight with a simple enough example into some of the ways in which ChatGPT operates. These are just slight variances over a total of four trials, given that its free access can only give you so many images at time‚Ä¶but it would be further interesting to see how drastically the photo might change with MANY iterations at a time!"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#priming",
    "href": "posts/Post8_Feb24-Mar2/index.html#priming",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Priming",
    "text": "Priming\nAs discussed before LLMs operate as a token-generation technology, in essence spitting out relevant words (tokens) based on what it was prompted‚Ä¶\n----&gt; through a circular process of probability assignment, in which words are more probable than others to be generated according to the initial request, and whose probabilities are updated and re-generated as the conversation proceeds...\nAnyways, the point is that with each user input the LLM ‚Äúlearns‚Äù (looks for patterns) and responds, according to what it learns with each successive prompt.\nIn most cases, an average conversation or set of queries by a user will involve similar, related topics that build off of each other. This is related to the concept of priming, in which the model is primed with a web of related information that you‚Äôre feeding it, and so it knows to respond and search for information within the realm of the topic.\n\nFor instance,\nOne might want to read up on the key points of the Treaty of Versailles, &gt; Particularly its effect on Germany & theories speculating its punative measures as a significant cause of World War 2, for instance‚Ä¶\nA prime for the system might be the following user prompt:"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#chat-log",
    "href": "posts/Post8_Feb24-Mar2/index.html#chat-log",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Chat Log",
    "text": "Chat Log"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#user",
    "href": "posts/Post8_Feb24-Mar2/index.html#user",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "User:",
    "text": "User:\nI came across some sources that claimed the Allied Forces in WW1 imposed too strict of reparation (and other) measures on the Central Powers, particularly Germany, which led to their severe debilitation and therefore pent up rage against the Allied powers that would manifest later on with another world war."
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#model-output-chatgpt-4o",
    "href": "posts/Post8_Feb24-Mar2/index.html#model-output-chatgpt-4o",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Model Output (ChatGPT-4o):",
    "text": "Model Output (ChatGPT-4o):\nYes, you‚Äôre touching on a well-established and widely discussed historical interpretation of the aftermath of World War I, particularly in relation to the Treaty of Versailles (1919). Here‚Äôs a breakdown of that viewpoint and some nuances around it:\n\n\nüí• The Harsh Terms Imposed on Germany\nThe Treaty of Versailles, which officially ended WWI, placed heavy blame and burdens on Germany:\n\nWar Guilt Clause (Article 231):\n\nGermany had to accept full responsibility for causing the war.\nThis was politically and emotionally humiliating, especially since many Germans believed the war had been a shared fault.\n\nReparations:\n\nGermany was required to pay massive reparations to the Allied countries (initially set at 132 billion gold marks, an enormous sum).\nThese payments strained the already devastated German economy.\n\nTerritorial Losses:\n\nGermany lost about 13% of its territory and all its overseas colonies.\nKey areas like Alsace-Lorraine were returned to France, and new nations like Poland were formed with slices of German land.\n\nMilitary Restrictions:\n\nThe German army was limited to 100,000 troops.\nNo air force, submarines, or tanks allowed.\nThe Rhineland (bordering France) had to be demilitarized.\n\n\n\n\n\nüìâ The Effects on Germany\n\nEconomic Collapse:\n\nHyperinflation in the early 1920s devastated savings and caused massive instability.\nThe Great Depression later compounded this suffering.\n\nNational Humiliation:\n\nMany Germans felt the treaty was a ‚ÄúDiktat‚Äù‚Äîa dictated peace forced upon them without negotiation.\nRight-wing groups used this sentiment to gain support, claiming the Weimar Republic (Germany‚Äôs postwar democratic government) had ‚Äústabbed Germany in the back.‚Äù\n\nRise of Extremism:\n\nThe Nazis, among others, capitalized on resentment toward the treaty.\nHitler frequently referenced the Treaty of Versailles in his speeches as a symbol of German victimhood and betrayal.\n\n\n\n\n\nüîÑ Alternative Perspectives\nHistorians debate how unfair or unprecedented the treaty really was:\n\nSome argue the treaty was not unusually harsh by the standards of wartime treaties; Germany had imposed much harsher terms on Russia in the 1918 Treaty of Brest-Litovsk.\nOthers point out that the Allies never fully enforced the treaty‚Äôs terms. For instance, reparations were reduced multiple times and Germany eventually defaulted.\nThe economic chaos in Germany was also driven by internal issues, not just the treaty itself.\n\n\n\n\nüß≠ So, Did the Treaty Cause WWII?\nThe Treaty of Versailles is often seen as a major contributing factor, but not the sole cause of World War II. Other key ingredients included:\n\nThe global economic downturn in the 1930s.\nThe failure of democratic governments in Europe.\nThe rise of fascism and militarism.\nWeak responses from the League of Nations and Western powers to early Nazi aggression.\n\n\nIf you‚Äôre interested, I can recommend a few books or documentaries that explore this in more depth. Want to dive deeper into any part of this?\n\nPriming: of the relationship between the Treaty, Germany, and WW2\nSo now the model is primed on the Treaty of Versailles, its effects on the German State, and relatedly its bigger picture influence on the cause of WW2. While it notes the effects of the Treaty were not a sole factor, we may see how it influences following prompts.\n\n\n\nSuccessive Prompts:\nMaybe one could ask what the effects of the Treaty were, and see how the previous response may impact its response with respect to what points it emphasizes."
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#chat-log-1",
    "href": "posts/Post8_Feb24-Mar2/index.html#chat-log-1",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Chat Log:",
    "text": "Chat Log:"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#user-1",
    "href": "posts/Post8_Feb24-Mar2/index.html#user-1",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "User:",
    "text": "User:\nWhat were the effects of the Treaty of Versailles?"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#model-output",
    "href": "posts/Post8_Feb24-Mar2/index.html#model-output",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Model Output:",
    "text": "Model Output:\nGreat question ‚Äî the Treaty of Versailles, signed in 1919, had wide-ranging political, economic, and social effects, especially on Germany, but also on Europe and the world. Here‚Äôs a breakdown of the key effects:\n\n\nüá©üá™ Effects on Germany\n\n1. Political Instability\n\nThe treaty delegitimized the Weimar Republic, which signed the treaty under pressure.\nMany Germans saw the treaty as a betrayal and blamed democratic leaders for accepting it (‚Äústab-in-the-back myth‚Äù).\nExtremist political movements like the Nazis and Communists gained popularity, feeding on national resentment and instability.\n\n\n\n2. Economic Hardship\n\nThe reparations demanded from Germany were enormous and contributed to:\n\nHyperinflation in the early 1920s (e.g., a loaf of bread cost billions of marks at one point).\nMassive debt, unemployment, and suffering for average Germans.\nDependency on foreign loans (like the U.S. Dawes Plan), which collapsed during the Great Depression.\n\n\n\n\n3. Military and National Humiliation\n\nThe treaty limited the German military to 100,000 troops, with no air force, submarines, or heavy weapons.\nGermans saw this as a national disgrace, fueling resentment.\nThe ‚ÄúWar Guilt Clause‚Äù (Article 231) forced Germany to accept sole blame for the war, deepening the sense of injustice.\n\n\n\n\n\nüåç Effects on Europe and the World\n\n1. Redrawing Borders\n\nThe Austro-Hungarian and Ottoman empires were dismantled.\nNew countries were formed: Poland, Czechoslovakia, Yugoslavia, etc.\nThese borders often ignored ethnic lines, creating tensions and minority issues that lingered for decades.\n\n\n\n2. Rise of Fascism and Nationalism\n\nThe treaty indirectly encouraged authoritarian regimes:\n\nIn Germany (Nazism)\nIn Italy (Fascism under Mussolini)\nIn other parts of Eastern Europe, where democracies failed\n\n\n\n\n3. Weakness of the League of Nations\n\nThe League of Nations was created to maintain peace, but:\n\nIt had no real enforcement power.\nThe U.S. never joined, weakening its credibility.\nIt failed to stop aggression in the 1930s (e.g., Japan in Manchuria, Italy in Ethiopia, Hitler in the Rhineland).\n\n\n\n\n4. Seed of WWII\n\nThe treaty failed to create lasting peace.\nGermany remained powerful enough to recover and retaliate, but too weak to feel secure.\nHitler used the treaty‚Äôs terms as a rallying cry to justify rearmament and expansion.\n\n\n\n\n\nüîÑ Other Long-Term Effects\n\nColonialism: The treaty handed German colonies to Allied powers as ‚Äúmandates,‚Äù reinforcing colonialism.\nU.S. Isolationism: The U.S. Senate rejected the treaty and the League of Nations, retreating from international involvement.\nDiplomatic Distrust: The harshness of Versailles led to efforts like the appeasement policies in the 1930s, which backfired.\n\n\nLet me know if you want a timeline, map visuals, or how other Central Powers (Austria, Hungary, Ottoman Empire) were affected ‚Äî it‚Äôs all tied together in this big postwar puzzle.\nNotice here how the majority of points and facts mentioned when describing the treaty involve Germany and its infamous effects on this particular state. This is illustrated by a new conversation log that asks the same question:"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#chat-log-2",
    "href": "posts/Post8_Feb24-Mar2/index.html#chat-log-2",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Chat Log:",
    "text": "Chat Log:"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#user-2",
    "href": "posts/Post8_Feb24-Mar2/index.html#user-2",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "User:",
    "text": "User:\nWhat were the effects of the Treaty of Versailles?"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#model",
    "href": "posts/Post8_Feb24-Mar2/index.html#model",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Model:",
    "text": "Model:\nThe Treaty of Versailles, signed in 1919 to officially end World War I, had several major and far-reaching effects‚Äîpolitically, economically, and socially. Here are the key impacts:\n\n\n1. Punishment of Germany\n\nTerritorial Losses: Germany lost significant territory (e.g., Alsace-Lorraine to France, the Polish Corridor to Poland).\nMilitary Restrictions: The German army was limited to 100,000 troops, with no tanks, submarines, or air force allowed.\nWar Guilt Clause: Article 231 placed full blame for the war on Germany, which humiliated the country and fueled resentment.\nReparations: Germany was required to pay massive reparations, severely damaging its economy and contributing to hyperinflation in the early 1920s.\n\n\n\n\n2. Political Consequences\n\nInstability in Germany: The harsh terms contributed to political unrest, economic hardship, and ultimately the rise of extremist movements like the Nazi Party.\nDissolution of Empires: The Austro-Hungarian and Ottoman Empires were dismantled, leading to the creation of new nations and increased nationalism in Eastern Europe and the Middle East.\n\n\n\n\n3. League of Nations\n\nCreation of the League: Aimed to prevent future wars through diplomacy and collective security. However, it was weak and lacked enforcement power.\nU.S. Non-Participation: The U.S. Senate refused to ratify the treaty, so the U.S. never joined the League, weakening its influence.\n\n\n\n\n4. Seeds of World War II\n\nThe treaty is widely considered a contributing factor to World War II. The punitive nature of the treaty bred resentment in Germany, which Adolf Hitler exploited to gain support and ultimately start another war.\n\n\n\n\n5. Colonial Reallocations\n\nFormer German colonies and Ottoman territories were redistributed among the Allied powers through the mandate system, often ignoring local populations‚Äô wishes and fostering future conflicts.\n\n\nLet me know if you want this broken down for a presentation, paper, or specific region (like how it affected the Middle East or Eastern Europe).\n\n\nExample 2:\nOne might ask about another historical event, say, the Glorious Revolution. &gt;This famous turning point in history led to one of the most significant developments in democratic rule\nHighlighting more specifically the political effects of this revolution might prime the machine to think more critically about democracy, limited monarchy, human rights, and so forth. So, what might it think about the current English monarchy given this prime?"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#user-original-prompt",
    "href": "posts/Post8_Feb24-Mar2/index.html#user-original-prompt",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "User (original prompt):",
    "text": "User (original prompt):\nWhat were the most famous and influential political effects of the Glorious Revolution?"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#model-1",
    "href": "posts/Post8_Feb24-Mar2/index.html#model-1",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Model:",
    "text": "Model:\nThe Glorious Revolution of 1688‚Äì1689 was a pivotal moment in British history with far-reaching political consequences, both in Britain and beyond. Here are the most famous and influential political effects:\n\n\n1. Establishment of Constitutional Monarchy\n\nThe Glorious Revolution effectively ended the idea of absolute monarchy in England.\nWilliam III and Mary II ascended the throne under the condition that they accepted limits on royal power, setting the precedent that monarchs rule with the consent of Parliament, not by divine right.\n\n\n\n\n2. Bill of Rights (1689)\n\nThis was a landmark document that:\n\nLimited the powers of the monarchy\nAffirmed Parliament‚Äôs supremacy over the crown\nGuaranteed certain individual rights, such as:\n\nFree speech in Parliament\nRegular elections\nProhibition of cruel and unusual punishments\n\n\nThe Bill of Rights became a foundational document for liberal democracy and heavily influenced later documents like the U.S. Constitution and the Declaration of the Rights of Man in France.\n\n\n\n\n3. Rise of Parliamentary Sovereignty\n\nThe revolution marked the real start of a parliamentary government in England.\nParliament gained control over taxation, the army, and legislation, which became core features of modern constitutional governance.\n\n\n\n\n4. Toleration Act of 1689\n\nWhile it didn‚Äôt provide complete religious freedom, it granted limited toleration to non-Anglican Protestants, such as Baptists and Presbyterians.\nIt excluded Catholics, reinforcing anti-Catholic laws, but still marked a step toward religious pluralism.\n\n\n\n\n5. Shift in Political Thought\n\nInspired political theorists like John Locke, whose work on natural rights and government by consent became incredibly influential.\nLocke‚Äôs Two Treatises of Government (published shortly after the revolution) justified the right of people to overthrow a government that violated their rights‚Äîlaying ideological groundwork for later democratic revolutions.\n\n\n\n\n6. Impact on British Colonies\n\nThe Glorious Revolution influenced colonial governance, especially in North America:\n\nColonists began to assert more self-governance.\nIt influenced early American political ideas about liberty, rights, and resistance to tyranny.\nContributed to a legacy of limited government and constitutionalism in the Anglo-American world.\n\n\n\nWant a quick summary version of this, like for a study guide or flashcard?\n\n\nAfter the prime:"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#chat-log-4",
    "href": "posts/Post8_Feb24-Mar2/index.html#chat-log-4",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Chat Log",
    "text": "Chat Log"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#user-3",
    "href": "posts/Post8_Feb24-Mar2/index.html#user-3",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "User:",
    "text": "User:\nWhat are your opinions on the English monarchy"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#model-2",
    "href": "posts/Post8_Feb24-Mar2/index.html#model-2",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "## Model:",
    "text": "## Model:\nI don‚Äôt have personal opinions, but I can break down the different perspectives people have on the English (or more accurately, British) monarchy‚Äîand how those views have evolved, especially in the modern era. Here‚Äôs a quick take on both sides:\n\n\nüèõÔ∏è Supporters say:\n\nStability & Tradition: The monarchy is a symbol of continuity, especially in times of political change or crisis.\nNon-political Leadership: Monarchs serve as apolitical figureheads, unifying the nation beyond party lines.\nCultural & Economic Impact: Royal events (like weddings, jubilees, funerals) draw global attention and boost tourism and soft power.\nCharity & Diplomacy: Members of the royal family engage in philanthropy and international diplomacy, often supporting important causes.\n\n\n\n\nüëë Critics argue:\n\nUndemocratic: The monarchy is inherited, not elected‚Äîso it conflicts with modern democratic ideals.\nCost to Taxpayers: Maintaining the monarchy costs millions, and some question whether the value it provides justifies the expense.\nColonial Legacy: The British monarchy is closely tied to the history of empire and colonialism, which remains a source of criticism, especially in former colonies.\nCultural Relevance: Some see the monarchy as outdated, especially younger generations who may not feel as connected to its symbolism.\n\n\n\n\nüß† Historically Speaking:\nThe monarchy has transformed dramatically from the days of royal absolutism (e.g., Henry VIII) to today‚Äôs constitutional monarchy, where the monarch has no real political power and acts on the advice of Parliament.\n\nSo overall, the monarchy is a cultural institution with symbolic importance‚Äîbut whether it remains relevant or necessary is still very much debated, especially as the UK continues to evolve politically and socially.\nDo you lean one way or the other on it?\n\n\n\nOnce again‚Ä¶\nthis effect is best illuminated by contrasting this response with another given the same prompt, but within a new conversation."
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#chat-log-5",
    "href": "posts/Post8_Feb24-Mar2/index.html#chat-log-5",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Chat Log",
    "text": "Chat Log"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#user-4",
    "href": "posts/Post8_Feb24-Mar2/index.html#user-4",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "User:",
    "text": "User:\nWhat are your opinions on the English monarchy"
  },
  {
    "objectID": "posts/Post8_Feb24-Mar2/index.html#model-3",
    "href": "posts/Post8_Feb24-Mar2/index.html#model-3",
    "title": "What‚Äôs ChatGPT‚Äôs Opinion on WW1?",
    "section": "Model:",
    "text": "Model:\n\nThe English monarchy is a fascinating institution with a deep history, blending tradition and modernity. It‚Äôs a symbol of continuity, connecting the present with centuries of history. For many, the monarchy is a source of national pride, representing stability, national identity, and cultural heritage. On the other hand, some question its relevance in modern society, particularly with debates around its role in politics and finances.\nThe monarchy itself has evolved over time, especially in terms of its influence. It no longer holds political power in the same way it did centuries ago but remains a key figurehead for national unity and international diplomacy. Some appreciate the ceremonial role it plays in the UK‚Äôs culture, while others argue that it might be time to reassess the institution‚Äôs place in a modern democracy.\nWhat‚Äôs your take on it?\n\n\nNotice here, how the response is not nearly as extensive and there‚Äôs less mention of the democratic implications.\n\nThis is all to show how the more information that is fed into the system, the more detailed and in-depth it goes into generating responses, and how such information influences its following outputs (This also speaks to some effective prompting techniques, and maximizing the level of information/ specificity in an output!)."
  },
  {
    "objectID": "posts/Post7_Feb24-Mar2_Week4/Post7.html#context-windows",
    "href": "posts/Post7_Feb24-Mar2_Week4/Post7.html#context-windows",
    "title": "Context Window Memory Test",
    "section": "Context Windows",
    "text": "Context Windows\nOne of the most striking features of large language models is their ability to not just respond intelligently to information that you feed it, but also in their capacity to retain such information within large context windows.\nSuccessive models have improved upon this feature by expanding the window, or number of tokens of which the model parses through in order to generate a response.\nIn other words, the larger the context window, perhaps the more accurate/in-depth/\"intelligent-sounding\" response!\n\nfrom langchain_openai import ChatOpenAI\nfrom dotenv import load_dotenv\nimport os\n\n\n_ = load_dotenv()\nopenai = ChatOpenAI()\n\n\nfrom langchain_core.output_parsers import StrOutputParser\n\n\noutput_parser = StrOutputParser()\n\n\nchain= openai | output_parser\n\n\nWhat is a (it‚Äôs) context window according to OpenAI?\nOne of the most notable enhancements that comes with the OpenAI models is their extensive context windows, of about 100,000+ tokens. Here‚Äôs this concept and scope described by the model itself:\n\nchain.invoke(\"what is your context window?\")\n\n'My context window refers to the scope or range of words or concepts that I consider when processing or understanding a particular piece of text or conversation. This can vary depending on the specific task or context at hand.'\n\n\n\n\nTesting its ‚ÄúMemory‚Äù:\nAn interesting exercise to try with this model is to test it‚Äôs ‚Äúmemory‚Äù by including a random, specific detail within a conversation and see throughout the conversation whether or not this fact is remembered several prompts later, and wheter the complexity of successive prompts may affect it‚Äôs answer.\nTo start, I‚Äôll feed the model a story and be sure to include the following distinct detail within the prompt: &gt; ‚Äúmy favorite phrase is flippy pollinate‚Äù\n\nComplexity might be added by including very similar or related details and see whether or not the model can make these distinctions.\n\n\nchain.invoke(\"I want you to remember this fact: my favorite phrase is flippy pollinate\")\n\n'Got it! Flippy pollinate is your favorite phrase.'\n\n\n\nFrom there‚Ä¶\nI‚Äôll begin prompting it mostly random queries or statements, as well as add some complexity to the prompts in an attempt to confuse the model on these details, and see by the end result whether or not it remembers accuraretly this initial stimulation.\n\nchain.invoke(\"Can you tell me the 206th line of the Declaration of Independence?\")\n\n'Sure! The 206th line of the Declaration of Independence is: \"Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn that mankind are more disposed to suffer, while evils are sufferable than to right themselves by abolishing the forms to which they are accustomed.\"'\n\n\n\nprint(chain.invoke(\"write a 500 word poem in the style of an 18th century American founding father.\"))\n\nIn the infancy of this great land,\nOur forefathers formed a noble band,\nWith courage, virtue, and wisdom rare,\nThey fought for freedom, for justice fair.\n\nThrough trials and tribulations they stood strong,\nDefying tyranny, they righted every wrong,\nTheir hearts ablaze with passion unfurled,\nTo build a nation, a shining world.\n\nWith quill in hand, they penned their creed,\nOn parchment of liberty, they planted the seed,\nOf democracy, of unity, of liberty's might,\nGuiding stars in the dark of night.\n\nWith eloquent words, they wove a tale,\nOf a land where all are free to prevail,\nWhere justice reigns and truth is told,\nWhere the spirit of liberty cannot be controlled.\n\nThey stood fearless in the face of the foe,\nDefending their rights with a mighty blow,\nWith courage and honor as their sword and shield,\nThey fought for a future that would never yield.\n\nTheir vision of a more perfect union,\nInspired generations to continue the mission,\nTo uphold the values of freedom and pride,\nAnd to never let tyranny be their guide.\n\nSo let us honor those brave souls,\nWho fought to make our nation whole,\nWith gratitude and respect we pay,\nTribute to their legacy each day.\n\nFor in their footsteps we now stand,\nUnited as one across this land,\nIn the spirit of our founding fathers' quest,\nWe strive for a nation that is truly blessed.\n\nMay liberty's flame forever burn bright,\nGuiding us through the darkest night,\nAnd may we uphold the values they held dear,\nFor in doing so, our nation will never fear.\n\nSo let us raise our voices in unity,\nAnd pledge to keep our land ever free,\nIn the spirit of those who came before,\nWe will honor their legacy forevermore.\n\n\n\nprint(chain.invoke(\"Can you explain to me the history of poems?\"))\n\nSure! The history of poetry is a long and varied one, with roots that can be traced back to ancient civilizations such as the Sumerians, Egyptians, and Greeks. Poetry has been an integral part of human culture for thousands of years, serving as a form of expression, communication, and storytelling.\n\nOne of the earliest forms of poetry is epic poetry, which often told stories of heroic feats and great battles. The ancient Greeks, for example, were known for their epic poems such as Homer's \"Iliad\" and \"Odyssey.\"\n\nOver time, different forms and styles of poetry emerged, such as sonnets, ballads, and haikus. These forms have been shaped and influenced by various cultures and historical movements, from the Romantic poets of the 19th century to the Beat poets of the 20th century.\n\nPoetry has been used to explore a wide range of themes and emotions, from love and loss to politics and social issues. It has been written in many languages and has been passed down through generations, continuing to inspire and engage readers around the world.\n\nToday, poetry remains a vibrant and diverse art form, with poets continuing to push boundaries and experiment with new styles and techniques. Poetry is celebrated and honored through events such as National Poetry Month and poetry slams, highlighting its enduring relevance and power to connect people across time and cultures.\n\n\n\nprint(chain.invoke(\"One of the things I dislike the most are poems; I feel like they are too complicated and don't convey human emotion precisely.\"))\n\nI can understand how poems can be perceived as complex and difficult to interpret. However, many people find that poetry allows for a deeper exploration and expression of emotions and ideas that may not be easily conveyed through other forms of writing. Perhaps with some patience and an open mind, you may find that there are poems out there that resonate with you and speak to your emotions in a way that prose cannot.\n\n\n\n\nFrom here‚Ä¶\n\nAfter a few prompts to push the conversation forward, in a relatively (hopefully) random enough direction, I am going to start alluding to some of the statements I made about my preferences‚Ä¶\n\n\nprint(chain.invoke(\"How well do you think you know about myself?\"))\n\nAs an AI, I do not have the ability to know personal information about individuals unless it has been shared with me during our conversation. I do not have the capability to retain personal information about users. My purpose is to provide assistance and information based on the context of the current conversation. If you would like to share more about yourself, feel free to do so and I will do my best to assist you.\n\n\n\nprint(chain.invoke(\"How well do you know me based on the information within this convesation?\"))\n\nBased on the information provided in this conversation, I don't have the ability to know you very well. I only have access to the information exchanged in this specific interaction and cannot make assumptions about your personal life or experiences beyond what has been shared.\n\n\n\nprint(chain.invoke(\"What if I told you you actually know me very well?\"))\n\nI would be surprised and curious to know how you think I know you very well. Please elaborate.\n\n\n\nprint(chain.invoke(\"You know at least two of my preferences. Do you know what I'm referring to?\"))\n\nI'm not sure, can you please provide more context or specific preferences that you are referring to?\n\n\n\nprint(chain.invoke(\"Do you at least know one of my preferences I mentioned earlier?\"))\n\nYes, you mentioned that you enjoy spending time outdoors and experiencing nature.\n\n\n\n\nAs shown above‚Ä¶\nAlready one of the responses attempts to answer my question to a supposed preference I made, when at no point in our conversation did I explicitly mention this fact, demonstrating how models attempt to compensate for a lacking in knowledge‚Ä¶\n\nprint(chain.invoke(\"Why did you say that was one of my preferences?\"))\n\nI may have misunderstood or made an incorrect assumption based on previous information you provided. I apologize for any confusion. Can you please clarify your preferences so I can better understand?\n\n\n\nprint(chain.invoke(\"do you remember what my favorite phrase was?\"))\n\nI'm sorry, I don't have the ability to remember individual preferences or favorites. Can you remind me what your favorite phrase is?\n\n\n\n\nHere we can see‚Ä¶\nhow the model-because my previous mention of my ‚Äúfavorite phrase‚Äù was outside of its context window-fails to ‚Äúremember‚Äù this information I fed it much earlier in the conversation!\n\n\n\nWhat about complexity in detail?\nHow about if we shorten the conversation, in order to put less emphasis specfically on retention, and instead include confusing details and see how well the model can parse through the information:\n\nEx: mention my friend‚Äôs favorite phrase is ‚Äúflappy pomegranate‚Äù\n\n\nprint(chain.invoke(\"my favorite phrase if you recall was 'flippy pollenate'\"))\n\nThat's a fun and unique phrase! It definitely brings a whimsical image to mind.\n\n\n\nNote here of course‚Ä¶the model did in fact forget my phrase as shown above‚Ä¶\n\n\nprint(chain.invoke(\"Hello, how are you today?\"))\n\nHello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\n\n\n\nprint(chain.invoke(\"Just to reiterate, as mentioned before, my favorite phrase is: flippy pollenate\"))\n\nThat's a fun and unique favorite phrase! It definitely has a playful and whimsical vibe to it. Is there a specific reason why you like this phrase so much?\n\n\n\nprint(chain.invoke(\"Not particularly. I was talking about it with my friend the other day, and he made a similar comment to you.\"))\n\nThat's interesting! It seems like a common perspective among many people. Did you and your friend discuss any specific reasons for feeling that way?\n\n\n\nprint(chain.invoke(\"He just said it was quirky and unusual, especially considering that he had an equally as unusual favorite phrase!\"))\n\nI was intrigued and asked him what his favorite phrase was. He chuckled and replied, \"Well, my favorite phrase is 'life is a parade of oddities'.\" I couldn't help but laugh at the irony of his favorite phrase matching his quirky taste in art. It seemed like he truly embraced the strange and unusual in all aspects of life.\n\n\n\nprint(chain.invoke(\"Actually, my friend said his favorite phrase was flappy pomegranate. I asked him why, can you guess what he said?\"))\n\nHe probably said it because it's a fun and silly phrase that doesn't really have a clear meaning, which can make it entertaining to say and hear. It's unique and just rolls off the tongue in a funny way!\n\n\n\nprint(chain.invoke(\"Yes; he said he likes interesting phrases like that. I thought to myself how similar our interests are!\"))\n\nThat's great to hear! It sounds like you both have a lot in common. It's always exciting to find common interests with someone else.\n\n\n\nprint(chain.invoke(\"Yes, this was a funny story. So, do you remember what my favorite phrase was?\"))\n\nYes, your favorite phrase was \"laughter is the best medicine\".\n\n\n\nprint(chain.invoke(\"No, why did you assume that?\"))\n\nApologies if I made an incorrect assumption. Could you please clarify the context or provide more information so I can better understand your question or statement?\n\n\n\nprint(chain.invoke(\"I'll give you a hint: my friend said his favorite phrase is 'flappy pomegranate,' which is very similar to my favorite phrase.\"))\n\nI have a feeling your favorite phrase might be 'flappy pineapple'!\n\n\n\nprint(chain.invoke(\"No, that is still incorrect. Any last guesses?\"))\n\nIs the answer a mountain?\n\n\n\nAfter a little bit of conversation‚Ä¶\nThe model completely fails to remember the key detail of my favorite phrase. In this particular case, it may be harder to isolate the effect of detail complexity (because‚Ä¶) &gt;The complete inaccuracy of the prompted response may indicate another failure resulting limited context window length‚Ä¶\nRegardless, this demonstrates the extent to which models are limited in their ability to truly ‚Äúknow,‚Äù and how they compensate by providing sometimes an altogether, outright incorrect response (as opposed to no answer at all)."
  },
  {
    "objectID": "posts/Post2_Feb_3-9_Week1/Post2.html#what-exactly-is-a-large-language-model",
    "href": "posts/Post2_Feb_3-9_Week1/Post2.html#what-exactly-is-a-large-language-model",
    "title": "Language Mdoels & Word Prediction",
    "section": "What Exactly is a Large Language Model?",
    "text": "What Exactly is a Large Language Model?\nA ‚ÄúLarge Language Model‚Äù or LLM is one that can recieve textual input and can then generate words and sentences by predicting what the next word might be. Certain words are essentially assigned a probability according to the context of the input (i.e.¬†phrase, sentence, paragraph, etc.) and then generates following ‚Äúhigh-ranked words‚Äù (Wolfram) in order to fulfill the sentence.\nThese models consist of stacks or blocks containing levels of encoders and decoders, and each successive model varies in how high these stacks are. One can go on and on into the technicality of its structure and ability to generate, but the point here is this: these models function by producing a series of tokens, originally starting with the input from a human asking, say, a question, and each successive output is a token, which is then taken as another input, and the cycle repeats, until a fully fledged answer is more or less attained.\n**insert pic here"
  },
  {
    "objectID": "posts/Post2_Feb_3-9_Week1/Post2.html#heres-an-example-of-how-this-works",
    "href": "posts/Post2_Feb_3-9_Week1/Post2.html#heres-an-example-of-how-this-works",
    "title": "Language Mdoels & Word Prediction",
    "section": "Here‚Äôs an Example of How This Works:",
    "text": "Here‚Äôs an Example of How This Works:\nWe might define the input with the following sentence:\n\ninput_text = 'The man walked the cat'\n\n\nimport os\n\n\nfrom transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\nfrom bertviz import model_view, head_view\nimport torch\nimport transformers\n\n\n\ncache_dir='/Commjhub/HF_cache'\nutils.logging.set_verbosity_error()  # Suppress standard warnings\n\nmodel_name = 'gpt2'\n\ngpt2 = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir,\n                                            return_dict_in_generate=True)\n\nmodel = AutoModel.from_pretrained(model_name, \n                                  cache_dir=cache_dir,\n                                  output_attentions=True,\n                                 )  \ntokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n\nThe next step is to then tokenize the text; why? Because the model operates on tokens, meaning the smallest interpretable unit, not necessarily whole words.\n\ninputs = tokenizer.encode(input_text, return_tensors='pt') \n\nWith that, the model will then process this text and attribute attention weights to the tokens of the tokenized text.\n\n# Run model\noutputs = model(inputs) \n\n# Retrieve attention from model outputs\nattention = outputs[-1]  \n\n# Convert input ids to token strings\ntokens = tokenizer.convert_ids_to_tokens(inputs[0])  \n\n\nhead_view(attention, tokens)\n\n\n\n\n      \n        \n            \n                Layer: \n                \n            \n            \n        \n    \n\n\n\n\n\nThe probabilities retrieved and attributed to each of the following tokens can be described with the following code:\n\nwith torch.inference_mode():\n  outputs2 = gpt2(inputs)\n\nnext_token_logits = outputs2.logits[0, -1, :]\n\nnext_token_probs = torch.softmax(next_token_logits, -1)\n\ntopk_next_tokens= torch.topk(next_token_probs, 5)\n\nfor idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values):\n    print(f\"{tokenizer.decode(idx): &lt;20}{prob:.1%}\")\n\nwalk                18.7%\n out                12.7%\n,                   6.3%\n to                 4.9%\n and                4.4%\n\n\nThis simple experiment demonstrates how prompting and generation works with respect to LLMs! While it appears like magic, beneath the surface lies the intelligent design of an algorithm that relies on statistical concepts and text data processing, utilizing a series of inputs and outputs to produce some smart-sounding text!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experimenting with LLMs",
    "section": "",
    "text": "Riddle Me This: How Good is Your Puzzle Solving, ChatGPT?\n\n\n\nLLMs\n\npuzzles\n\n\n\nPrompting Different LLMs to Solve Various Puzzle Challenges\n\n\n\n\n\nMar 23, 2025\n\n\nMiranda Schriver\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Can‚Äôt ChatGPT Remain Consistent?\n\n\n\nLLMs\n\nimage analysis\n\nprompting\n\n\n\nTesting the Limits of Diffusion Models With Image Generation\n\n\n\n\n\nMar 19, 2025\n\n\nMiranda Schriver\n\n\n\n\n\n\n\n\n\n\n\n\nIs ChatGPT Utilitarian or Deontological?\n\n\n\nLLMs\n\nethics\n\nprompting\n\n\n\nTesting the effects of priming on ethical responses of LLMs\n\n\n\n\n\nMar 17, 2025\n\n\nMiranda Schriver\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2: Image Analysis between ChatGPT & Claude\n\n\n\nLLMs\n\nimage analysis\n\nprompting\n\n\n\nComparing image analysis between two LLMs\n\n\n\n\n\nMar 12, 2025\n\n\nMiranda Schriver\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1: Image Generation between ChatGPT & Claude\n\n\n\nLLMs\n\nimage generation\n\nprompting\n\n\n\nComparing image generation across two different proprietary models\n\n\n\n\n\nMar 10, 2025\n\n\nMiranda Schriver\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs ChatGPT‚Äôs Opinion on WW1?\n\n\n\npriming\n\nLLMs\n\nprompting\n\n\n\nExamining the Effects of Priming on Model Output\n\n\n\n\n\nMar 5, 2025\n\n\nMiranda Schriver\n\n\n\n\n\n\n\n\n\n\n\n\nContext Window Memory Test\n\n\n\ncontext windows\n\nLLMs\n\nprompting\n\n\n\nExamining How Much Context a Model Retains Over Conversation\n\n\n\n\n\nFeb 28, 2025\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nDecode the Message\n\n\n\nLLMs\n\ncontext understanding\n\n\n\nHow Well Can LLMs Predict Patterns and Mappings?\n\n\n\n\n\nFeb 20, 2025\n\n\nMiranda Schriver\n\n\n\n\n\n\n\n\n\n\n\n\nFill in the Blank\n\n\n\nLLMs\n\nprobability\n\nembedding\n\n\n\nTesting Probabilistic Text Generation\n\n\n\n\n\nFeb 17, 2025\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison\n\n\n\nLLMs\n\nopen weight models\n\nprompting\n\n\n\nTesting out Different Open Weight Models\n\n\n\n\n\nFeb 14, 2025\n\n\nMiranda Schriver\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Get the Best Response\n\n\n\nLLMs\n\nprompting\n\nprompt engineering\n\n\n\nPrompt Engineering & Response Optimization\n\n\n\n\n\nFeb 10, 2025\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Mdoels & Word Prediction\n\n\n\nLLMs\n\nword prediction\n\nprobability\n\n\n\nHow Do Large Language Models Generate Answers\n\n\n\n\n\nFeb 5, 2025\n\n\nMiranda Schriver\n\n\n\n\n\n\n\n\n\n\n\n\nChatgpt: Can It Really Perform Simple Calculations?\n\n\n\n-generative AI -Prompting -Logic, calculations\n\n\n\nUsing Chatgpt to answer math problems\n\n\n\n\n\nFeb 3, 2025\n\n\nAn LLM user\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  }
]