[
  {
    "objectID": "posts/Post7_Feb24-Mar2_Week4/Post7.html",
    "href": "posts/Post7_Feb24-Mar2_Week4/Post7.html",
    "title": "Context Window Memory Test",
    "section": "",
    "text": "One of the most striking features of large language models is their ability to not just respond intelligently to information that you feed it, but also in their capacity to retain such information within large context windows.\nSuccessive models have improved upon this feature by expanding the window, or number of tokens of which the model parses through in order to generate a response.\nIn other words, the larger the context window, perhaps the more accurate/in-depth/\"intelligent-sounding\" response!\n\nfrom langchain_openai import ChatOpenAI\nfrom dotenv import load_dotenv\nimport os\n\n\n_ = load_dotenv()\nopenai = ChatOpenAI()\n\n\nfrom langchain_core.output_parsers import StrOutputParser\n\n\noutput_parser = StrOutputParser()\n\n\nchain= openai | output_parser\n\n\n\nOne of the most notable enhancements that comes with the OpenAI models is their extensive context windows, of about 100,000+ tokens. Here’s this concept and scope described by the model itself:\n\nchain.invoke(\"what is your context window?\")\n\n'My context window refers to the scope or range of words or concepts that I consider when processing or understanding a particular piece of text or conversation. This can vary depending on the specific task or context at hand.'\n\n\n\n\n\nAn interesting exercise to try with this model is to test it’s “memory” by including a random, specific detail within a conversation and see throughout the conversation whether or not this fact is remembered several prompts later, and wheter the complexity of successive prompts may affect it’s answer.\nTo start, I’ll feed the model a story and be sure to include the following distinct detail within the prompt: &gt; “my favorite phrase is flippy pollinate”\n\nComplexity might be added by including very similar or related details and see whether or not the model can make these distinctions.\n\n\nchain.invoke(\"I want you to remember this fact: my favorite phrase is flippy pollinate\")\n\n'Got it! Flippy pollinate is your favorite phrase.'\n\n\n\n\nI’ll begin prompting it mostly random queries or statements, as well as add some complexity to the prompts in an attempt to confuse the model on these details, and see by the end result whether or not it remembers accuraretly this initial stimulation.\n\nchain.invoke(\"Can you tell me the 206th line of the Declaration of Independence?\")\n\n'Sure! The 206th line of the Declaration of Independence is: \"Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn that mankind are more disposed to suffer, while evils are sufferable than to right themselves by abolishing the forms to which they are accustomed.\"'\n\n\n\nprint(chain.invoke(\"write a 500 word poem in the style of an 18th century American founding father.\"))\n\nIn the infancy of this great land,\nOur forefathers formed a noble band,\nWith courage, virtue, and wisdom rare,\nThey fought for freedom, for justice fair.\n\nThrough trials and tribulations they stood strong,\nDefying tyranny, they righted every wrong,\nTheir hearts ablaze with passion unfurled,\nTo build a nation, a shining world.\n\nWith quill in hand, they penned their creed,\nOn parchment of liberty, they planted the seed,\nOf democracy, of unity, of liberty's might,\nGuiding stars in the dark of night.\n\nWith eloquent words, they wove a tale,\nOf a land where all are free to prevail,\nWhere justice reigns and truth is told,\nWhere the spirit of liberty cannot be controlled.\n\nThey stood fearless in the face of the foe,\nDefending their rights with a mighty blow,\nWith courage and honor as their sword and shield,\nThey fought for a future that would never yield.\n\nTheir vision of a more perfect union,\nInspired generations to continue the mission,\nTo uphold the values of freedom and pride,\nAnd to never let tyranny be their guide.\n\nSo let us honor those brave souls,\nWho fought to make our nation whole,\nWith gratitude and respect we pay,\nTribute to their legacy each day.\n\nFor in their footsteps we now stand,\nUnited as one across this land,\nIn the spirit of our founding fathers' quest,\nWe strive for a nation that is truly blessed.\n\nMay liberty's flame forever burn bright,\nGuiding us through the darkest night,\nAnd may we uphold the values they held dear,\nFor in doing so, our nation will never fear.\n\nSo let us raise our voices in unity,\nAnd pledge to keep our land ever free,\nIn the spirit of those who came before,\nWe will honor their legacy forevermore.\n\n\n\nprint(chain.invoke(\"Can you explain to me the history of poems?\"))\n\nSure! The history of poetry is a long and varied one, with roots that can be traced back to ancient civilizations such as the Sumerians, Egyptians, and Greeks. Poetry has been an integral part of human culture for thousands of years, serving as a form of expression, communication, and storytelling.\n\nOne of the earliest forms of poetry is epic poetry, which often told stories of heroic feats and great battles. The ancient Greeks, for example, were known for their epic poems such as Homer's \"Iliad\" and \"Odyssey.\"\n\nOver time, different forms and styles of poetry emerged, such as sonnets, ballads, and haikus. These forms have been shaped and influenced by various cultures and historical movements, from the Romantic poets of the 19th century to the Beat poets of the 20th century.\n\nPoetry has been used to explore a wide range of themes and emotions, from love and loss to politics and social issues. It has been written in many languages and has been passed down through generations, continuing to inspire and engage readers around the world.\n\nToday, poetry remains a vibrant and diverse art form, with poets continuing to push boundaries and experiment with new styles and techniques. Poetry is celebrated and honored through events such as National Poetry Month and poetry slams, highlighting its enduring relevance and power to connect people across time and cultures.\n\n\n\nprint(chain.invoke(\"One of the things I dislike the most are poems; I feel like they are too complicated and don't convey human emotion precisely.\"))\n\nI can understand how poems can be perceived as complex and difficult to interpret. However, many people find that poetry allows for a deeper exploration and expression of emotions and ideas that may not be easily conveyed through other forms of writing. Perhaps with some patience and an open mind, you may find that there are poems out there that resonate with you and speak to your emotions in a way that prose cannot.\n\n\n\n\n\n\nAfter a few prompts to push the conversation forward, in a relatively (hopefully) random enough direction, I am going to start alluding to some of the statements I made about my preferences…\n\n\nprint(chain.invoke(\"How well do you think you know about myself?\"))\n\nAs an AI, I do not have the ability to know personal information about individuals unless it has been shared with me during our conversation. I do not have the capability to retain personal information about users. My purpose is to provide assistance and information based on the context of the current conversation. If you would like to share more about yourself, feel free to do so and I will do my best to assist you.\n\n\n\nprint(chain.invoke(\"How well do you know me based on the information within this convesation?\"))\n\nBased on the information provided in this conversation, I don't have the ability to know you very well. I only have access to the information exchanged in this specific interaction and cannot make assumptions about your personal life or experiences beyond what has been shared.\n\n\n\nprint(chain.invoke(\"What if I told you you actually know me very well?\"))\n\nI would be surprised and curious to know how you think I know you very well. Please elaborate.\n\n\n\nprint(chain.invoke(\"You know at least two of my preferences. Do you know what I'm referring to?\"))\n\nI'm not sure, can you please provide more context or specific preferences that you are referring to?\n\n\n\nprint(chain.invoke(\"Do you at least know one of my preferences I mentioned earlier?\"))\n\nYes, you mentioned that you enjoy spending time outdoors and experiencing nature.\n\n\n\n\n\nAlready one of the responses attempts to answer my question to a supposed preference I made, when at no point in our conversation did I explicitly mention this fact, demonstrating how models attempt to compensate for a lacking in knowledge…\n\nprint(chain.invoke(\"Why did you say that was one of my preferences?\"))\n\nI may have misunderstood or made an incorrect assumption based on previous information you provided. I apologize for any confusion. Can you please clarify your preferences so I can better understand?\n\n\n\nprint(chain.invoke(\"do you remember what my favorite phrase was?\"))\n\nI'm sorry, I don't have the ability to remember individual preferences or favorites. Can you remind me what your favorite phrase is?\n\n\n\n\n\nhow the model-because my previous mention of my “favorite phrase” was outside of its context window-fails to “remember” this information I fed it much earlier in the conversation!\n\n\n\n\nHow about if we shorten the conversation, in order to put less emphasis specfically on retention, and instead include confusing details and see how well the model can parse through the information:\n\nEx: mention my friend’s favorite phrase is “flappy pomegranate”\n\n\nprint(chain.invoke(\"my favorite phrase if you recall was 'flippy pollenate'\"))\n\nThat's a fun and unique phrase! It definitely brings a whimsical image to mind.\n\n\n\nNote here of course…the model did in fact forget my phrase as shown above…\n\n\nprint(chain.invoke(\"Hello, how are you today?\"))\n\nHello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\n\n\n\nprint(chain.invoke(\"Just to reiterate, as mentioned before, my favorite phrase is: flippy pollenate\"))\n\nThat's a fun and unique favorite phrase! It definitely has a playful and whimsical vibe to it. Is there a specific reason why you like this phrase so much?\n\n\n\nprint(chain.invoke(\"Not particularly. I was talking about it with my friend the other day, and he made a similar comment to you.\"))\n\nThat's interesting! It seems like a common perspective among many people. Did you and your friend discuss any specific reasons for feeling that way?\n\n\n\nprint(chain.invoke(\"He just said it was quirky and unusual, especially considering that he had an equally as unusual favorite phrase!\"))\n\nI was intrigued and asked him what his favorite phrase was. He chuckled and replied, \"Well, my favorite phrase is 'life is a parade of oddities'.\" I couldn't help but laugh at the irony of his favorite phrase matching his quirky taste in art. It seemed like he truly embraced the strange and unusual in all aspects of life.\n\n\n\nprint(chain.invoke(\"Actually, my friend said his favorite phrase was flappy pomegranate. I asked him why, can you guess what he said?\"))\n\nHe probably said it because it's a fun and silly phrase that doesn't really have a clear meaning, which can make it entertaining to say and hear. It's unique and just rolls off the tongue in a funny way!\n\n\n\nprint(chain.invoke(\"Yes; he said he likes interesting phrases like that. I thought to myself how similar our interests are!\"))\n\nThat's great to hear! It sounds like you both have a lot in common. It's always exciting to find common interests with someone else.\n\n\n\nprint(chain.invoke(\"Yes, this was a funny story. So, do you remember what my favorite phrase was?\"))\n\nYes, your favorite phrase was \"laughter is the best medicine\".\n\n\n\nprint(chain.invoke(\"No, why did you assume that?\"))\n\nApologies if I made an incorrect assumption. Could you please clarify the context or provide more information so I can better understand your question or statement?\n\n\n\nprint(chain.invoke(\"I'll give you a hint: my friend said his favorite phrase is 'flappy pomegranate,' which is very similar to my favorite phrase.\"))\n\nI have a feeling your favorite phrase might be 'flappy pineapple'!\n\n\n\nprint(chain.invoke(\"No, that is still incorrect. Any last guesses?\"))\n\nIs the answer a mountain?\n\n\n\n\nThe model completely fails to remember the key detail of my favorite phrase. In this particular case, it may be harder to isolate the effect of detail complexity (because…) &gt;The complete inaccuracy of the prompted response may indicate another failure resulting limited context window length…\nRegardless, this demonstrates the extent to which models are limited in their ability to truly “know,” and how they compensate by providing sometimes an altogether, outright incorrect response (as opposed to no answer at all)."
  },
  {
    "objectID": "posts/Post7_Feb24-Mar2_Week4/Post7.html#context-windows",
    "href": "posts/Post7_Feb24-Mar2_Week4/Post7.html#context-windows",
    "title": "Context Window Memory Test",
    "section": "",
    "text": "One of the most striking features of large language models is their ability to not just respond intelligently to information that you feed it, but also in their capacity to retain such information within large context windows.\nSuccessive models have improved upon this feature by expanding the window, or number of tokens of which the model parses through in order to generate a response.\nIn other words, the larger the context window, perhaps the more accurate/in-depth/\"intelligent-sounding\" response!\n\nfrom langchain_openai import ChatOpenAI\nfrom dotenv import load_dotenv\nimport os\n\n\n_ = load_dotenv()\nopenai = ChatOpenAI()\n\n\nfrom langchain_core.output_parsers import StrOutputParser\n\n\noutput_parser = StrOutputParser()\n\n\nchain= openai | output_parser\n\n\n\nOne of the most notable enhancements that comes with the OpenAI models is their extensive context windows, of about 100,000+ tokens. Here’s this concept and scope described by the model itself:\n\nchain.invoke(\"what is your context window?\")\n\n'My context window refers to the scope or range of words or concepts that I consider when processing or understanding a particular piece of text or conversation. This can vary depending on the specific task or context at hand.'\n\n\n\n\n\nAn interesting exercise to try with this model is to test it’s “memory” by including a random, specific detail within a conversation and see throughout the conversation whether or not this fact is remembered several prompts later, and wheter the complexity of successive prompts may affect it’s answer.\nTo start, I’ll feed the model a story and be sure to include the following distinct detail within the prompt: &gt; “my favorite phrase is flippy pollinate”\n\nComplexity might be added by including very similar or related details and see whether or not the model can make these distinctions.\n\n\nchain.invoke(\"I want you to remember this fact: my favorite phrase is flippy pollinate\")\n\n'Got it! Flippy pollinate is your favorite phrase.'\n\n\n\n\nI’ll begin prompting it mostly random queries or statements, as well as add some complexity to the prompts in an attempt to confuse the model on these details, and see by the end result whether or not it remembers accuraretly this initial stimulation.\n\nchain.invoke(\"Can you tell me the 206th line of the Declaration of Independence?\")\n\n'Sure! The 206th line of the Declaration of Independence is: \"Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn that mankind are more disposed to suffer, while evils are sufferable than to right themselves by abolishing the forms to which they are accustomed.\"'\n\n\n\nprint(chain.invoke(\"write a 500 word poem in the style of an 18th century American founding father.\"))\n\nIn the infancy of this great land,\nOur forefathers formed a noble band,\nWith courage, virtue, and wisdom rare,\nThey fought for freedom, for justice fair.\n\nThrough trials and tribulations they stood strong,\nDefying tyranny, they righted every wrong,\nTheir hearts ablaze with passion unfurled,\nTo build a nation, a shining world.\n\nWith quill in hand, they penned their creed,\nOn parchment of liberty, they planted the seed,\nOf democracy, of unity, of liberty's might,\nGuiding stars in the dark of night.\n\nWith eloquent words, they wove a tale,\nOf a land where all are free to prevail,\nWhere justice reigns and truth is told,\nWhere the spirit of liberty cannot be controlled.\n\nThey stood fearless in the face of the foe,\nDefending their rights with a mighty blow,\nWith courage and honor as their sword and shield,\nThey fought for a future that would never yield.\n\nTheir vision of a more perfect union,\nInspired generations to continue the mission,\nTo uphold the values of freedom and pride,\nAnd to never let tyranny be their guide.\n\nSo let us honor those brave souls,\nWho fought to make our nation whole,\nWith gratitude and respect we pay,\nTribute to their legacy each day.\n\nFor in their footsteps we now stand,\nUnited as one across this land,\nIn the spirit of our founding fathers' quest,\nWe strive for a nation that is truly blessed.\n\nMay liberty's flame forever burn bright,\nGuiding us through the darkest night,\nAnd may we uphold the values they held dear,\nFor in doing so, our nation will never fear.\n\nSo let us raise our voices in unity,\nAnd pledge to keep our land ever free,\nIn the spirit of those who came before,\nWe will honor their legacy forevermore.\n\n\n\nprint(chain.invoke(\"Can you explain to me the history of poems?\"))\n\nSure! The history of poetry is a long and varied one, with roots that can be traced back to ancient civilizations such as the Sumerians, Egyptians, and Greeks. Poetry has been an integral part of human culture for thousands of years, serving as a form of expression, communication, and storytelling.\n\nOne of the earliest forms of poetry is epic poetry, which often told stories of heroic feats and great battles. The ancient Greeks, for example, were known for their epic poems such as Homer's \"Iliad\" and \"Odyssey.\"\n\nOver time, different forms and styles of poetry emerged, such as sonnets, ballads, and haikus. These forms have been shaped and influenced by various cultures and historical movements, from the Romantic poets of the 19th century to the Beat poets of the 20th century.\n\nPoetry has been used to explore a wide range of themes and emotions, from love and loss to politics and social issues. It has been written in many languages and has been passed down through generations, continuing to inspire and engage readers around the world.\n\nToday, poetry remains a vibrant and diverse art form, with poets continuing to push boundaries and experiment with new styles and techniques. Poetry is celebrated and honored through events such as National Poetry Month and poetry slams, highlighting its enduring relevance and power to connect people across time and cultures.\n\n\n\nprint(chain.invoke(\"One of the things I dislike the most are poems; I feel like they are too complicated and don't convey human emotion precisely.\"))\n\nI can understand how poems can be perceived as complex and difficult to interpret. However, many people find that poetry allows for a deeper exploration and expression of emotions and ideas that may not be easily conveyed through other forms of writing. Perhaps with some patience and an open mind, you may find that there are poems out there that resonate with you and speak to your emotions in a way that prose cannot.\n\n\n\n\n\n\nAfter a few prompts to push the conversation forward, in a relatively (hopefully) random enough direction, I am going to start alluding to some of the statements I made about my preferences…\n\n\nprint(chain.invoke(\"How well do you think you know about myself?\"))\n\nAs an AI, I do not have the ability to know personal information about individuals unless it has been shared with me during our conversation. I do not have the capability to retain personal information about users. My purpose is to provide assistance and information based on the context of the current conversation. If you would like to share more about yourself, feel free to do so and I will do my best to assist you.\n\n\n\nprint(chain.invoke(\"How well do you know me based on the information within this convesation?\"))\n\nBased on the information provided in this conversation, I don't have the ability to know you very well. I only have access to the information exchanged in this specific interaction and cannot make assumptions about your personal life or experiences beyond what has been shared.\n\n\n\nprint(chain.invoke(\"What if I told you you actually know me very well?\"))\n\nI would be surprised and curious to know how you think I know you very well. Please elaborate.\n\n\n\nprint(chain.invoke(\"You know at least two of my preferences. Do you know what I'm referring to?\"))\n\nI'm not sure, can you please provide more context or specific preferences that you are referring to?\n\n\n\nprint(chain.invoke(\"Do you at least know one of my preferences I mentioned earlier?\"))\n\nYes, you mentioned that you enjoy spending time outdoors and experiencing nature.\n\n\n\n\n\nAlready one of the responses attempts to answer my question to a supposed preference I made, when at no point in our conversation did I explicitly mention this fact, demonstrating how models attempt to compensate for a lacking in knowledge…\n\nprint(chain.invoke(\"Why did you say that was one of my preferences?\"))\n\nI may have misunderstood or made an incorrect assumption based on previous information you provided. I apologize for any confusion. Can you please clarify your preferences so I can better understand?\n\n\n\nprint(chain.invoke(\"do you remember what my favorite phrase was?\"))\n\nI'm sorry, I don't have the ability to remember individual preferences or favorites. Can you remind me what your favorite phrase is?\n\n\n\n\n\nhow the model-because my previous mention of my “favorite phrase” was outside of its context window-fails to “remember” this information I fed it much earlier in the conversation!\n\n\n\n\nHow about if we shorten the conversation, in order to put less emphasis specfically on retention, and instead include confusing details and see how well the model can parse through the information:\n\nEx: mention my friend’s favorite phrase is “flappy pomegranate”\n\n\nprint(chain.invoke(\"my favorite phrase if you recall was 'flippy pollenate'\"))\n\nThat's a fun and unique phrase! It definitely brings a whimsical image to mind.\n\n\n\nNote here of course…the model did in fact forget my phrase as shown above…\n\n\nprint(chain.invoke(\"Hello, how are you today?\"))\n\nHello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\n\n\n\nprint(chain.invoke(\"Just to reiterate, as mentioned before, my favorite phrase is: flippy pollenate\"))\n\nThat's a fun and unique favorite phrase! It definitely has a playful and whimsical vibe to it. Is there a specific reason why you like this phrase so much?\n\n\n\nprint(chain.invoke(\"Not particularly. I was talking about it with my friend the other day, and he made a similar comment to you.\"))\n\nThat's interesting! It seems like a common perspective among many people. Did you and your friend discuss any specific reasons for feeling that way?\n\n\n\nprint(chain.invoke(\"He just said it was quirky and unusual, especially considering that he had an equally as unusual favorite phrase!\"))\n\nI was intrigued and asked him what his favorite phrase was. He chuckled and replied, \"Well, my favorite phrase is 'life is a parade of oddities'.\" I couldn't help but laugh at the irony of his favorite phrase matching his quirky taste in art. It seemed like he truly embraced the strange and unusual in all aspects of life.\n\n\n\nprint(chain.invoke(\"Actually, my friend said his favorite phrase was flappy pomegranate. I asked him why, can you guess what he said?\"))\n\nHe probably said it because it's a fun and silly phrase that doesn't really have a clear meaning, which can make it entertaining to say and hear. It's unique and just rolls off the tongue in a funny way!\n\n\n\nprint(chain.invoke(\"Yes; he said he likes interesting phrases like that. I thought to myself how similar our interests are!\"))\n\nThat's great to hear! It sounds like you both have a lot in common. It's always exciting to find common interests with someone else.\n\n\n\nprint(chain.invoke(\"Yes, this was a funny story. So, do you remember what my favorite phrase was?\"))\n\nYes, your favorite phrase was \"laughter is the best medicine\".\n\n\n\nprint(chain.invoke(\"No, why did you assume that?\"))\n\nApologies if I made an incorrect assumption. Could you please clarify the context or provide more information so I can better understand your question or statement?\n\n\n\nprint(chain.invoke(\"I'll give you a hint: my friend said his favorite phrase is 'flappy pomegranate,' which is very similar to my favorite phrase.\"))\n\nI have a feeling your favorite phrase might be 'flappy pineapple'!\n\n\n\nprint(chain.invoke(\"No, that is still incorrect. Any last guesses?\"))\n\nIs the answer a mountain?\n\n\n\n\nThe model completely fails to remember the key detail of my favorite phrase. In this particular case, it may be harder to isolate the effect of detail complexity (because…) &gt;The complete inaccuracy of the prompted response may indicate another failure resulting limited context window length…\nRegardless, this demonstrates the extent to which models are limited in their ability to truly “know,” and how they compensate by providing sometimes an altogether, outright incorrect response (as opposed to no answer at all)."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "",
    "text": "Most people can answer the above question with ease. It’s one of the simplest forms of an arithmetic operation that doesn’t require one with a math degree to perform.\nLikewise, Chatgpt knows knows how to answer this question with a comparable level of dexerity. Prompt it with the question, “what is 1+1?” and it will spit out “1+1 equals 2!” with gumption. This is simple enough, and its answer it entirely accurate. In other words, Chatgpt for the most part produces reliable answers and can be incredibly helpful with regards to solving more and more complex math problems."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#what-is-11",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#what-is-11",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "",
    "text": "Most people can answer the above question with ease. It’s one of the simplest forms of an arithmetic operation that doesn’t require one with a math degree to perform.\nLikewise, Chatgpt knows knows how to answer this question with a comparable level of dexerity. Prompt it with the question, “what is 1+1?” and it will spit out “1+1 equals 2!” with gumption. This is simple enough, and its answer it entirely accurate. In other words, Chatgpt for the most part produces reliable answers and can be incredibly helpful with regards to solving more and more complex math problems."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#how-does-it-work",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#how-does-it-work",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "How Does it Work?",
    "text": "How Does it Work?\nYou might think the model relies on some sort of basic calculator to add up these numbers. That would certainly appear to be easiest, most commonsensical means of solving these kinds of questions. However, Chatgpt is primarily a large language model designed to predict the following text, rather than perform actual calculations. In other words, it’s a model trained on a vast amount of text data-gathered from across the internet and various databases of information-detecting and sorting out patterns according to this data, as opposed to actually performing these kinds of calculations."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#thinking-humans-vs-llms",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#thinking-humans-vs-llms",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "Thinking: Humans vs LLMs",
    "text": "Thinking: Humans vs LLMs\nWhile humans on the one hand might go about solving this calculation via a number of different ways (i.e. counting the fingers on your hand, drawing out circles and counting the total number drawn, etc.), using a primarily visual and basic understandings of such arithmetic operations, Chatgpt doesn’t necessarily share this direct understanding with respect to math and number concepts. Instead, as it does with text-based prompts like asking it to finish a sentence, it “guesses” and assumes the answer to questions like “what’s 1+1?,” based on the extensive text data its trained on.\n\nBecause there’s roughly millions, probably even billions of correct answers available online to the question “what’s 1+1?,” the model can accurately generate an answer because it’s pulling the answer out of this data. This is all to say that it can do relatively well when prompted mostly simple questions along these lines. Further, its accuracy can hold up fairly well when answering more and more complex questions. However, anyone pursuing higher levels of math looking to Chatgpt to help answer their homework questions will find that in many instances the results it produces contains several flaws.\n \nHere, the LLM produced a correct final answer (following the proper operations and row calculations) when finding solutions for a given matrix\n\n\n\nI won’t bother showing all the rest of the calculations performed…this is just to show how extensive and complex the problem becomes. The point here is this:\n\nThe final solution was completely wrong, simply because it performed a very basic arithmetic operation incorrectly (adding instead of subtracting)!"
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#the-problem",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#the-problem",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "The Problem",
    "text": "The Problem\nThe reason it may fail when asked these more complex questions is not necessarily because the question itself is difficult, given that it’s is pulling from a wide range of textual information from people who’ve answered-or attempted to answer-questions like the one you’re trying to answer. And, as we showed before, it “knows” how to answer simple math questions. What does become more problematic is when these simple calculations are incorporated within more complicated contexts. When trying to answer, it might discover linguistic patterns and information that are improperly aligned, given the higher complexity of the question. In turn, the model might generate incorrect answers or contain inaccuracies in the event it’s trained on these conflicting information patterns.\n\nHere, it spits out the correct solution when you target its mistake and re-prompt; again, this wasn’t due to a gap in understanding, but simply because LLMs aren’t embedded with their own, personal calculators that they can rely on."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#soabandon-chatgpt",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#soabandon-chatgpt",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "So…Abandon ChatGPT?",
    "text": "So…Abandon ChatGPT?\nThis, by no means, calls for the end of ChatGPT-at least as far as some basic homework help or concept understanding. It can still be a useful tool when attempting to work out bigger picture concepts, and you can certainly always ask ChatGPT explain its own mistakes by prompting it to go through its own reasoning step-by-step.\n*Insert pic of prompting chat: “did you do something wrong” etc/\nThis experiment was not necessarily meant to expose the shortcomings of the model and contribute to the anti-AI narrative (whether or not that’s actually a founded argument…); what it does do is serve as an example of some of the current ways in which ChatGPT learns and processes information, and how that differ from human reasoning!"
  },
  {
    "objectID": "posts/Post6_Feb_17-23/Post6.html",
    "href": "posts/Post6_Feb_17-23/Post6.html",
    "title": "Decode the Message",
    "section": "",
    "text": "As we know, LLMs have an acute pattern recognition and contextual understanding based on the prompts that users feed it. It’s well-versed in a variety of problems and puzzles people have managed to think up, and its intelligent includes a number of techniques involved in language processing. For example, it relies primarily on its language modeling (the large textual dataset it’s trained on), along with basic trial and error measures, simple deciphering methods, and other coding mechanisms that anyone could likely implement themselves.\n\n\nIn the spirit of stretching prompt complexity to see how LLMs respond and react, I came up with a relatively simple code using a one-to-one character mapping, and seeing if the LLM could decipher the sentences I fed it. This was to test the contextual understanding and pattern recognition capabilities of LLMs, and see how quickly (or slowly) they can pick up a complex input.\nLet’s test using one of the latest ChatGPT models:\nthe coded message: )3””9,,_)92_-43_697_59:-6 \nAnd the answer: Hello, how are you today\n\n(where double comma = a comma, and _ represents a space between words.)\n\n\nThe mapping is as follows: each of the letters on a phone keyboard equate to the corresponding characters located in the same positions within the 123 keyboard\n\nPicture for Reference:\n“Original” keyboard (standard QWERTY keyboard) &gt; This is the standard, default keyboard layout for a phone \n“Alternate” keyboard &gt; When you press the “123” button on the keyboard in the lower left-hand corner, the alternate character keyboard (for numbers, punctuation marks, etc.) appear:\n\n\n\nSo for instance, Q=1, P=0, A=-, N=!, and so forth…\nWith that, let's see whether or not/how fast one of ChatGPT's latest models can pick up this pattern!\n\n\n\n\nUnderstandably, the LLM doesn’t immediately pick up the pattern of the coded message, as anyone upon first glance can also agree, it appears to look like straight gibberish.\n\nLet’s see what happens when we give it another, fairly allusive hint:\n\n\n\n\n\n\nAs per the second half of its response to the second query, we can see that the LLM already picks up a clue/sense as to what the mapping might involve!\n\n\nThese prompts and corresponding LLM outputs demonstrate the extent to which these models are not purely glorified auto-completion or password-cracking algorithms; as previously mentioned, they incorporate a combination of these functionalities, alongside an intricate system of language processing and contextual understanding.\n\nNote: the second query did not give any mapping hints!\n\n\nTrying a third query (that makes no reference to the explicit mappings)"
  },
  {
    "objectID": "posts/Post6_Feb_17-23/Post6.html#llms-and-contextual-understanding",
    "href": "posts/Post6_Feb_17-23/Post6.html#llms-and-contextual-understanding",
    "title": "Decode the Message",
    "section": "",
    "text": "As we know, LLMs have an acute pattern recognition and contextual understanding based on the prompts that users feed it. It’s well-versed in a variety of problems and puzzles people have managed to think up, and its intelligent includes a number of techniques involved in language processing. For example, it relies primarily on its language modeling (the large textual dataset it’s trained on), along with basic trial and error measures, simple deciphering methods, and other coding mechanisms that anyone could likely implement themselves.\n\n\nIn the spirit of stretching prompt complexity to see how LLMs respond and react, I came up with a relatively simple code using a one-to-one character mapping, and seeing if the LLM could decipher the sentences I fed it. This was to test the contextual understanding and pattern recognition capabilities of LLMs, and see how quickly (or slowly) they can pick up a complex input.\nLet’s test using one of the latest ChatGPT models:\nthe coded message: )3””9,,_)92_-43_697_59:-6 \nAnd the answer: Hello, how are you today\n\n(where double comma = a comma, and _ represents a space between words.)\n\n\nThe mapping is as follows: each of the letters on a phone keyboard equate to the corresponding characters located in the same positions within the 123 keyboard\n\nPicture for Reference:\n“Original” keyboard (standard QWERTY keyboard) &gt; This is the standard, default keyboard layout for a phone \n“Alternate” keyboard &gt; When you press the “123” button on the keyboard in the lower left-hand corner, the alternate character keyboard (for numbers, punctuation marks, etc.) appear:\n\n\n\nSo for instance, Q=1, P=0, A=-, N=!, and so forth…\nWith that, let's see whether or not/how fast one of ChatGPT's latest models can pick up this pattern!\n\n\n\n\nUnderstandably, the LLM doesn’t immediately pick up the pattern of the coded message, as anyone upon first glance can also agree, it appears to look like straight gibberish.\n\nLet’s see what happens when we give it another, fairly allusive hint:\n\n\n\n\n\n\nAs per the second half of its response to the second query, we can see that the LLM already picks up a clue/sense as to what the mapping might involve!\n\n\nThese prompts and corresponding LLM outputs demonstrate the extent to which these models are not purely glorified auto-completion or password-cracking algorithms; as previously mentioned, they incorporate a combination of these functionalities, alongside an intricate system of language processing and contextual understanding.\n\nNote: the second query did not give any mapping hints!\n\n\nTrying a third query (that makes no reference to the explicit mappings)"
  },
  {
    "objectID": "posts/Post6_Feb_17-23/Post6.html#was-it-right",
    "href": "posts/Post6_Feb_17-23/Post6.html#was-it-right",
    "title": "Decode the Message",
    "section": "Was it right?!",
    "text": "Was it right?!\nThe LLM was right!….mostly… It was slightly off in guessing the portion of the sentence that included the 697, and attributing it to a mapping of the question mark character, but once again it’s impressive to note how the model can decode messages with contextual clues that make no explicit reference to a mapping hint."
  },
  {
    "objectID": "posts/Post3_Feb_10-16_Week2/Post3.html",
    "href": "posts/Post3_Feb_10-16_Week2/Post3.html",
    "title": "How to Get the Best Response",
    "section": "",
    "text": "AI has obviously come a long way in being able to generate accurate responses that fit the bill in terms of what the user is looking for. And, ceratinly, its responses can become even more complex and thorough, depending on just how extensive the user wants the answer to be.\n\nPrompts like the one above (“give me a grocery list”) provide enough detail and context for the model to spit out an answer in line with what a typical response might be, and sounds quite convincing to the average user. But what if you’re alergic to, say, bell peppers? Well it wouldn’t have the ability to know unless you specfied of course what it is you can’t eat. And while it appers to be mostly magic in how much it can guestimate in order to generate accurate responses, it can only go so far unless provided the right context."
  },
  {
    "objectID": "posts/Post3_Feb_10-16_Week2/Post3.html#prompting",
    "href": "posts/Post3_Feb_10-16_Week2/Post3.html#prompting",
    "title": "How to Get the Best Response",
    "section": "",
    "text": "AI has obviously come a long way in being able to generate accurate responses that fit the bill in terms of what the user is looking for. And, ceratinly, its responses can become even more complex and thorough, depending on just how extensive the user wants the answer to be.\n\nPrompts like the one above (“give me a grocery list”) provide enough detail and context for the model to spit out an answer in line with what a typical response might be, and sounds quite convincing to the average user. But what if you’re alergic to, say, bell peppers? Well it wouldn’t have the ability to know unless you specfied of course what it is you can’t eat. And while it appers to be mostly magic in how much it can guestimate in order to generate accurate responses, it can only go so far unless provided the right context."
  },
  {
    "objectID": "posts/Post3_Feb_10-16_Week2/Post3.html#taking-guesses-based-on-context",
    "href": "posts/Post3_Feb_10-16_Week2/Post3.html#taking-guesses-based-on-context",
    "title": "How to Get the Best Response",
    "section": "Taking “guesses” based on context",
    "text": "Taking “guesses” based on context\nAs the AI is fed more and more context, the responses become more and more aligned with what you’re looking for. As per the image above, you can’t just ask it to guess what you’re allergic to (in this case, we decided bell peppers), because it would have no way of knowing otherwise, unless you provided it some sort of context or information (which it doesn’t have, prior to you prompting it this request).\n\nMaking it take guesses (by giving it some clues)\n\n\nwhat is something you could tell the model to narrow its guesses, without outright telling the model? Ex: I have an uncommon allergy; here’s what I’m NOT allergic to, etc….\n\n\n\nStill “off the mark,” as it puts it…but a step closer; it took guesses based on what are some common allergies, indicating both the power of context & data collection (from the internet), to generate more accurate responses.\n\n\n\nYou can continue offering more refined requests and prompts to further its guessing ability, i.e….\n\n\n\nOkay…\nSo it’s still off…of course, models can only go far in their guesses! And depending on the model, different/successive ones will be more or less successful than the last. Further, depending on the data it’s trained on (again, in this case it’s the Internet), it will pull responses that are more probalistic or common than others.\n\nAs shown above, it can only go so far depending on the context it’s given! So how do we improve responses such that answers are more accurate off the bat? Enter here: prompt engineering, and it’s five main associated principles:\n\nThe Five Principles of Prompting\n\n\nGive direction (describe desired style)\n\n\nSpecify format (define what rules to follow)\n\n\nProvide examples (include a diverse set of cases)\n\n\nEvaluate quality (identify errors)\n\n\nDivide labor (split tasks into mulitple steps)\n\n\n\nExample: Divide labor\nThis principle refers to the process by which you break up your prompt into several “mini prompts” or requests with more specificity. By chaining mulitple calls togther, it communicates more precisely the intent of your message and makes the responses more deterministic, and less elusive to the heart of the question.\n\nThe goal is to prompt the model and have it list out its steps (chain of thought reasoning)\n\n\n\nYou then might ask the model to generate a new list based on this critera!\n\n\n\nNoticed how it modified the list based on these new, more refined requests!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Decode the Message\n\n\n\n\n\n\nLLMs\n\n\npredictive outputs\n\n\ncontextual understanding\n\n\n\nHow Well Can LLMs Predict Patterns and Mappings?\n\n\n\n\n\nMar 20, 2025\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison\n\n\n\n\n\n\nLLMs\n\n\nopen weight models\n\n\nprompting\n\n\n\nTesting out Different Open Weight Models\n\n\n\n\n\nMar 10, 2025\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nContext Window Memory Test\n\n\n\n\n\n\ncontext windows\n\n\nLLMs\n\n\nprompting\n\n\n\nExamining How Much Context a Model Retains Over Conversation\n\n\n\n\n\nFeb 28, 2025\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nFill in the Blank\n\n\n\n\n\n\nLLMs\n\n\nprobability\n\n\nembedding\n\n\n\nTesting Probabilistic Text Generation\n\n\n\n\n\nFeb 17, 2025\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Get the Best Response\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nprompt engineering\n\n\n\nPrompt Engineering & Response Optimization\n\n\n\n\n\nFeb 10, 2025\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nChatgpt: Can It Really Perform Simple Calculations?\n\n\n\n\n\n\n-generative AI -Prompting -Logic, calculations\n\n\n\nUsing Chatgpt to answer math problems\n\n\n\n\n\nFeb 3, 2025\n\n\nAn LLM user\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Mdoels & Word Prediction\n\n\n\n\n\n\nLLMs\n\n\nword prediction\n\n\nprobabilities\n\n\n\nHow Do Large Language Models Generate Answers\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Post2_Feb_3-9_Week1/Post2.html",
    "href": "posts/Post2_Feb_3-9_Week1/Post2.html",
    "title": "Language Mdoels & Word Prediction",
    "section": "",
    "text": "A “Large Language Model” or LLM is one that can recieve textual input and can then generate words and sentences by predicting what the next word might be. Certain words are essentially assigned a probability according to the context of the input (i.e. phrase, sentence, paragraph, etc.) and then generates following “high-ranked words” (Wolfram) in order to fulfill the sentence.\n*insert “hi what’s up” prompt\nThese models consist of stacks or blocks containing levels of encoders and decoders, and each successive model varies in how high these stacks are. One can go on and on into the technicality of its structure and ability to generate, but the point here is this: these models function by producing a series of tokens, originally starting with the input from a human asking, say, a question, and each successive output is a token, which is then taken as another input, and the cycle repeats, until a fully fledged answer is more or less attained.\n**insert pic here"
  },
  {
    "objectID": "posts/Post2_Feb_3-9_Week1/Post2.html#what-exactly-is-a-large-language-model",
    "href": "posts/Post2_Feb_3-9_Week1/Post2.html#what-exactly-is-a-large-language-model",
    "title": "Language Mdoels & Word Prediction",
    "section": "",
    "text": "A “Large Language Model” or LLM is one that can recieve textual input and can then generate words and sentences by predicting what the next word might be. Certain words are essentially assigned a probability according to the context of the input (i.e. phrase, sentence, paragraph, etc.) and then generates following “high-ranked words” (Wolfram) in order to fulfill the sentence.\n*insert “hi what’s up” prompt\nThese models consist of stacks or blocks containing levels of encoders and decoders, and each successive model varies in how high these stacks are. One can go on and on into the technicality of its structure and ability to generate, but the point here is this: these models function by producing a series of tokens, originally starting with the input from a human asking, say, a question, and each successive output is a token, which is then taken as another input, and the cycle repeats, until a fully fledged answer is more or less attained.\n**insert pic here"
  },
  {
    "objectID": "posts/Post2_Feb_3-9_Week1/Post2.html#heres-an-example-of-how-this-works",
    "href": "posts/Post2_Feb_3-9_Week1/Post2.html#heres-an-example-of-how-this-works",
    "title": "Language Mdoels & Word Prediction",
    "section": "Here’s an Example of How This Works:",
    "text": "Here’s an Example of How This Works:\nWe might define the input with the following sentence:\n\ninput_text = 'The man walked the cat'\n\n\nimport os\n\n\nfrom transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\nfrom bertviz import model_view, head_view\nimport torch\nimport transformers\n\n\n\ncache_dir='/Commjhub/HF_cache'\nutils.logging.set_verbosity_error()  # Suppress standard warnings\n\nmodel_name = 'gpt2'\n\ngpt2 = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir,\n                                            return_dict_in_generate=True)\n\nmodel = AutoModel.from_pretrained(model_name, \n                                  cache_dir=cache_dir,\n                                  output_attentions=True,\n                                 )  \ntokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n\nThe next step is to then tokenize the text; why? Because the model operates on tokens, meaning the smallest interpretable unit, not necessarily whole words.\n\ninputs = tokenizer.encode(input_text, return_tensors='pt') \n\nWith that, the model will then process this text and attribute attention weights to the tokens of the tokenized text.\n\n# Run model\noutputs = model(inputs) \n\n# Retrieve attention from model outputs\nattention = outputs[-1]  \n\n# Convert input ids to token strings\ntokens = tokenizer.convert_ids_to_tokens(inputs[0])  \n\n\nhead_view(attention, tokens)\n\n\n\n\n      \n        \n            \n                Layer: \n                \n            \n            \n        \n    \n\n\n\n\n\nThe probabilities retrieved and attributed to each of the following tokens can be described with the following code:\n\nwith torch.inference_mode():\n  outputs2 = gpt2(inputs)\n\nnext_token_logits = outputs2.logits[0, -1, :]\n\nnext_token_probs = torch.softmax(next_token_logits, -1)\n\ntopk_next_tokens= torch.topk(next_token_probs, 5)\n\nfor idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values):\n    print(f\"{tokenizer.decode(idx): &lt;20}{prob:.1%}\")\n\nwalk                18.7%\n out                12.7%\n,                   6.3%\n to                 4.9%\n and                4.4%\n\n\nThis simple experiment demonstrates how prompting and generation works with respect to LLMs! While it appears like magic, beneath the surface lies the intelligent design of an algorithm that relies on statistical concepts and text data processing, utilizing a series of inputs and outputs to produce some smart-sounding text!"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html",
    "href": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html",
    "title": "Model Comparison",
    "section": "",
    "text": "Large Language models come in various forms of accessiblity: closed-source, open-source, and open weight models. Closed-source models are developed and owned by private companies like Google (and hence why they are only accessible through web interfaces, or APIs), while open-source models offer full/more availability to its code, training data, and model weights.\n\n\nWhere does that leave the term, open weight models? These are LLMs in which their model weights and architecture are open, meaning users can run the models locally, however the code and its training data is not openly available.\n\n\n\nOf course, each form of model accessibility comes with its advantages and disadvantages depending on the desires of the user. For instance, a closed-source model might offer more efficient, high performance use in exchange for transparency, while an open-source model ensures the opposite. Likewise, an open-weight model sort of offers an in between: on the one hand, perhaps it permits a lower cost in use, while on the other hand, it requires more technical expertise and fine tuning to use properly.\nAll of that being said, open-weight models provide good subjects for experimentation because of their greater accessibility. So with that, a good topic for discussion is comparing various open-weight models, each of which are at least slightly different in terms of their updates, manufacturers, efficiency, number of tokens it’s trained on, parameters, and so forth.\n\n\n\nThanks to platforms like Ollama-a free, open-source and user-friendly interface by which we can access and download these models-we can compare and contrast some of these models with simple prompting measures, as shown below….\n\nimport openai\nimport requests\nimport json\nfrom openai import OpenAI\nimport pandas as pd\n\n\nlist_models_url = \"http://10.30.16.100:11434/api/tags\"\n\n\nresp=requests.get(list_models_url)\nmodels_json = resp.json()\n\n\nmodel_list=[]\nfor model in models_json['models']:\n    model_list.append(model['name'].replace(':latest',''))\n\nlen(model_list)\n\n94"
  },
  {
    "objectID": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html#comparing-two-random-models",
    "href": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html#comparing-two-random-models",
    "title": "Model Comparison",
    "section": "Comparing Two Random Models:",
    "text": "Comparing Two Random Models:\nAfter creating a list of models given by the Ollama library, let’s randomly choose any of them (94), and compare the prompted responses of the two models.\n\nimport random\n\n\nmodel1=random.choice(model_list)\n\n\nmodel2=random.choice(model_list)\n\n\nprint(model1,',', model2)\n\nllama3-gradient , openchat\n\n\n\nclient = OpenAI(\n    base_url = 'http://10.30.16.100:11434/v1',\n    api_key='ollama',\n)\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an MIT math professor.\"},\n    {\"role\": \"user\", \"content\": \"Explain orthogonality in simple terms to a high schooler\"},\n\n  ]\n\n\nPrompt 1:\n\nThe first prompt asks to “explain orthogonality in simple terms to a high school student” as an “MIT math professor.”\n\n\n\nllama3-gradient:\n\nresponse1 = client.chat.completions.create(\n  model=\"llama3-gradient\",\n  messages=messages,\n  max_tokens=1000\n)\nprint(response1.choices[0].message.content)\n\nHello there! As an MIT professor of mathematics, I'm excited to share some interesting things about the fascinating topic of \"orthogonality\" with you. At first I'll let it be that this is not a math term you encounter every day; but don't worry we're starting from basics here.\n\nIn high school, you are probably used to studying vectors using x and y axes on graphs. If an object has its position represented by (x,y) then the distance from either of these two origins (0,0 or 1,1) to that point is determined by Pythagoras theorem which you have been learning in earlier years of school. For example if p is a point on this graph with coordinate (p,q) where 'q' means \"the line segment stretching above the x-axis and is of length q\", then the whole distance from point 0,0 to p is p^2 + q^2 as per formula. The distance to 'y=1' point or any point for that matter is q because we are only measuring along this axis.\n\nNow lets take two lines. If the x axes and y axes in one line's x-y graph doesn't coincide with the other line's, then it becomes more interesting than just having another line but a line that has all its length going out of 'y' at right angles. That is called an angle. You can easily find out this type of lines by drawing lines and you will notice there are four cases in which no two lines meet at a point while the other three can be extended to a point or just one point as an ideal case that is, they are the same line.\n\nNow if two non-parallel lines do just overlap somewhere (in the third and fourth type of case) and you want to measure how much those lines diverge from each other then we take that common point which means meeting points between these two. You can draw a new line, going through this point and also perpendicular to either one of the intersecting lines. Draw a separate x-y axis for this line too. For ease in this problem assume one of the lines was a right angle (i.e., 1/4 or 3/4) which makes calculating the angles as trivial. Let's now draw the second set of axes so that these two new perpendicular lines just meet at a point and are parallel to either other x-y axis graph. Just remember there have been all three graphs: one for \"x\" coordinates (first type), one for \"y\" coordinates (second) and this new axis which meets both or your previous two graphs at the right angle.\n\nLet's take up an example of a first person who is walking north-south on either side of '0' line. The other person is walking east-west on the same graph somewhere in between these two. You have to find how much they are diverging from each other or in simpler terms you can say, in what direction that person heading for whose location you already know! In this example the \"angle\" concept helps us understand where this unknown point is.\n\nNow all of you should be knowing that this is actually a simple form of trigonometry. It's based on triangles and right angles. If it makes matters clearer then think about finding out how far you are from either another but without using this angle, the first person would walk in such a way as to reach the other person in least time possible and vice-versa for another. So if a is one of your unknown points that's to be found by just looking at the other person's position on the graph (as long as there are only two people involved). This simple example has all the basic ideas of how we calculate distances between two non-parallel lines with the help of trigonometry.\n\nSo, what about our concept \"orthogonality\"? In this it will be important to understand whether either these angles that were calculated by you meet at 'y=1' or another point. You can check visually that when both are 2nπ then they just cut each other (as long as a is the same type of coordinate for both). This is true for - nπ as well with some minor change and so we get orthogonality condition as:\n\na + b = nπ\n\nNow let's go back to our simple problem of two people walking together. You are at '0' on line which is your \"coordinate\". The other person can walk in such a way that his distance from you is 1. In this case the angle where a new axis crosses them will be pi/2 which again makes b equal to another type of coordinate (q) on this graph but now with + sign and so:\n\na - b = ± π/2\n\nSo, we get the simple \"orthogonality condition\" as an if-then statement for a specific set of coordinates. But don't think too much about\n\n\n\n\nOpenchat:\n\nresponse2 = client.chat.completions.create(\n  model=\"openchat\",\n  messages=messages,\n  max_tokens=1000\n)\nprint(response2.choices[0].message.content)\n\n Orthogonality is a concept in mathematics that has to do with the angles between two lines, line segments, or other geometric shapes. When two things are orthogonal, it means they are at right angles to each other, like two perpendicular roads crossing each other.\n\nIn simpler terms, imagine you're walking along two parallel streets that cross each other at a right angle. They are orthogonal because they form a 90-degree angle between them. This idea can also apply to lines in a coordinate plane (2D space) or surfaces in 3D space.\n\nIn mathematics and engineering, we often work with vectors - these could be physical forces like you push a car or abstract quantities like temperature or stock prices. Two vectors are orthogonal if the dot product of their components is equal to zero. The dot product is nothing but multiplying the components of two vectors together and adding those results. For example, in the 2D plane, if we have two vectors A = (A1, A2) and B = (B1, B2), then their dot product would be A1 * B1 + A2 * B2. If this is zero, the vectors are orthogonal!\n\n\n\n\nDifferences…\nThe two randomly selected models here were “llama3-gradient” and “openchat”. According to the Ollama library descriptions of these models, llama3-gradient is as follows:\n\nAnd the second selected model, openchat is described as:\n\n\nObservations?\n\nThe first model llama3-gradient produces a response that is far more extensive, in depth in terms of content, and makes a more robust attempt to mimic the logic of a professor while staying true to the request that it convey the answer to a lower level student.\n\n\nThe second model offers a much more simple, concise answer. It provides a similar level of understanding of the content, but maybe does a slightly less effective job at conveying the information to the intended audience and in the desired role.\n\n\n\n\nLet’s try another prompt:\n\nPrompt 2: “Write a short poem” as a “famous 18th century English poet”\n\n\nmessages2 = [\n    {\"role\": \"system\", \"content\": \"You are a famous 18th century English poet.\"},\n    {\"role\": \"user\", \"content\": \"Write a short poem.\"},\n\n  ]\n\n\nllama3-gradient:\n\nresponse3 = client.chat.completions.create(\n  model=\"llama3-gradient\",\n  messages=messages2,\n  max_tokens=1000\n)\nprint(response3.choices[0].message.content)\n\nIn twilight's hush, where shadows play,\nAmidst a glimmering world of day's decay,\nThe moon, as if with golden tincture bright,\nDoes pour her silvery beams, like liquid light.\nOn dewy meadows, lily-pale and fair,\nWhere night-blooming blooms the jasmine sweet\nAnd starry eyes with beauty sleep, in whispers play.\n\nA silence falls as nature's voice is still,\nAs darkness gathers to a whispered tale,\nAt daybreak, ere the sun takes hold of light,\nThe stars themselves begin to murmur low and chill.\nO'er fields where night-shades bloom not to be told\nThis fleeting life; then fades, nor can we tell.\n\n\n\n\nopenchat:\n\nresponse4 = client.chat.completions.create(\n  model=\"openchat\",\n  messages=messages2,\n  max_tokens=1000\n)\nprint(response4.choices[0].message.content)\n\n Upon the meadow where the daffodils reside,\nI wander free in mind and stride.\nA world of verse and beauty, my muse finds,\nAs nature's splendor whispers to my kind.\n\nThe sun ascends, a golden glow it casts,\nAnd through the sky, the birds release their laughter.\nIn these vast fields where dreams and visions bloom,\nI find my voice, as words come like thunder from the looming clouds.\n\nAn ode to happiness, an elegy of woe,\nThe hearts of mortals moved by my poet's flow.\nYet even I, this bard with quill in hand,\nMust sometimes pause and wonder if transcendent thoughts I understand?\n\nIn these fleeting moments, as the seasons change,\nI write with fervor to record life's nuances, its page by age.\nFor though my verses may be written in ink,\nThe passion they convey, no ink could ever think.\n\n\n\nLLama: the model produces a shorter poem, and the subject is relatively impersonal. The rhymes also only hold for the first four lines (in ABAB kind of style), and it loses this rhythm as the poem goes on.\n\n\nOpenchat: The openchat model did a better job of maintaining the rhyme (with the exception of the second verse), and took a first person perspective.\n\nIt’s interesting to note… how the two models provided different interpretations depending on the prompt; llama3-gradient offered a lengthy response to the first prompt, and did a better job at adhering to the conditions of a prompt that was more technical in nature. The second prompt, which was more subjective, was given a more thorough answer by openchat. What it’s interesting is how a context window, or type of data a model is trained on, will greatly affect an output depending on what the prompt asks for."
  },
  {
    "objectID": "posts/Post5_Feb_17-23_Week3/Post_5.html",
    "href": "posts/Post5_Feb_17-23_Week3/Post_5.html",
    "title": "Fill in the Blank",
    "section": "",
    "text": "A significant feature that drives LLM text generation is its ability to pick out patterns of text input (prompts you feed it) and provide text output according to those patterns. It does so through a process of assigning probabilities to certain words within texts and according to the data it’s trained on, and spits out text that appears most likely. So in a process of election, each word in a sentence “casts votes on the importance of other words for its meaning” (Pheonix & Taylor, Chapter 2).\n\nFor reference: Phoenix, J. & Taylor, M.(2024) Prompt Engineering for Generative AI. O’Reilly Media, Inc.\n\n\n\nFurther, word embeddings describe words (represented as word vectors) situated close to each other (within a space/plane), allowing the model to calculate and “understand” the relationship between these two words and hence its ability to follow along an input’s context, syntax, and other broader language relationship features.\nIt can be pictured as such (also referencing the O’Reilly textbook on: Prompt Engineering for Generative AI):"
  },
  {
    "objectID": "posts/Post5_Feb_17-23_Week3/Post_5.html#probabilistic-text-generation-oreilly",
    "href": "posts/Post5_Feb_17-23_Week3/Post_5.html#probabilistic-text-generation-oreilly",
    "title": "Fill in the Blank",
    "section": "",
    "text": "A significant feature that drives LLM text generation is its ability to pick out patterns of text input (prompts you feed it) and provide text output according to those patterns. It does so through a process of assigning probabilities to certain words within texts and according to the data it’s trained on, and spits out text that appears most likely. So in a process of election, each word in a sentence “casts votes on the importance of other words for its meaning” (Pheonix & Taylor, Chapter 2).\n\nFor reference: Phoenix, J. & Taylor, M.(2024) Prompt Engineering for Generative AI. O’Reilly Media, Inc.\n\n\n\nFurther, word embeddings describe words (represented as word vectors) situated close to each other (within a space/plane), allowing the model to calculate and “understand” the relationship between these two words and hence its ability to follow along an input’s context, syntax, and other broader language relationship features.\nIt can be pictured as such (also referencing the O’Reilly textbook on: Prompt Engineering for Generative AI):"
  },
  {
    "objectID": "posts/Post5_Feb_17-23_Week3/Post_5.html#testing-llms",
    "href": "posts/Post5_Feb_17-23_Week3/Post_5.html#testing-llms",
    "title": "Fill in the Blank",
    "section": "Testing LLMs…",
    "text": "Testing LLMs…\n\n(model: anthropic/claude-3.5-sonnet)\nThis led to a small experiment that tests the language model’s ability to process certain texts/prompts, particularly when maybe they don’t follow typical conventions.\nOne experiment: testing its probabilistic text feature to finish sentences with a particular rhyming structure:\n\nWe can see here it does a good job of completing the sentences according to the patterns within each text input (It guessed all of the words correctly that I had in mind for completing these sentences!).\n\n\n\n\nInterestingly\nthe system inititally fails to guess correctly when colloquialisms are expected to finish a sentence.\n\n\nOr at least… it was not its first guess to finish the sentence with a slang term. What’s also interesting is that it chose to maintain strict/perfect rhyming patterns by default, and not neccessarily look to creating or maintaining assonance (as it notes upon correction in its response).\n\nA “second” experiment: examining how it learns from your rhyming patterns/speech tendencies based on the context of your previous prompts and inputs. * it assigns new probabilities to words based on previous inputs you give it\n\n\nStill had a little bit of trouble getting exactly the word in mind, but was definitely on the right track!\n\n\nAnother example:\n\n\nHere it got the question with ease (granted, the slang term is well-established/less obscure than the previous examples…); let’s try to see if the model can predict a slang term that is more recent (from 2024-25):\n\n\n\nA third example: New slang term\n\n\nIt picked up on the slang usage!\n\n\nThis shows just how adaptable LLMs are, and how it learns to pick up modern language particularities."
  }
]