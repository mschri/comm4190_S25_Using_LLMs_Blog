[
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "",
    "text": "Most people can answer the above question with ease. It‚Äôs one of the simplest forms of an arithmetic operation that doesn‚Äôt require one with a math degree to perform.\nLikewise, Chatgpt knows knows how to answer this question with a comparable level of dexerity. Prompt it with the question, ‚Äúwhat is 1+1?‚Äù and it will spit out ‚Äú1+1 equals 2!‚Äù with gumption. This is simple enough, and its answer it entirely accurate. In other words, Chatgpt for the most part produces reliable answers and can be incredibly helpful with regards to solving more and more complex math problems."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#what-is-11",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#what-is-11",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "",
    "text": "Most people can answer the above question with ease. It‚Äôs one of the simplest forms of an arithmetic operation that doesn‚Äôt require one with a math degree to perform.\nLikewise, Chatgpt knows knows how to answer this question with a comparable level of dexerity. Prompt it with the question, ‚Äúwhat is 1+1?‚Äù and it will spit out ‚Äú1+1 equals 2!‚Äù with gumption. This is simple enough, and its answer it entirely accurate. In other words, Chatgpt for the most part produces reliable answers and can be incredibly helpful with regards to solving more and more complex math problems."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#how-does-it-work",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#how-does-it-work",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "How Does it Work?",
    "text": "How Does it Work?\nYou might think the model relies on some sort of basic calculator to add up these numbers. That would certainly appear to be easiest, most commonsensical means of solving these kinds of questions. However, Chatgpt is primarily a large language model designed to predict the following text, rather than perform actual calculations. In other words, it‚Äôs a model trained on a vast amount of text data-gathered from across the internet and various databases of information-detecting and sorting out patterns according to this data, as opposed to actually performing these kinds of calculations."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#thinking-humans-vs-llms",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#thinking-humans-vs-llms",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "Thinking: Humans vs LLMs",
    "text": "Thinking: Humans vs LLMs\nWhile humans on the one hand might go about solving this calculation via a number of different ways (i.e.¬†counting the fingers on your hand, drawing out circles and counting the total number drawn, etc.), using a primarily visual and basic understandings of such arithmetic operations, Chatgpt doesn‚Äôt necessarily share this direct understanding with respect to math and number concepts. Instead, as it does with text-based prompts like asking it to finish a sentence, it ‚Äúguesses‚Äù and assumes the answer to questions like ‚Äúwhat‚Äôs 1+1?,‚Äù based on the extensive text data its trained on.\n\nBecause there‚Äôs roughly millions, probably even billions of correct answers available online to the question ‚Äúwhat‚Äôs 1+1?,‚Äù the model can accurately generate an answer because it‚Äôs pulling the answer out of this data. This is all to say that it can do relatively well when prompted mostly simple questions along these lines. Further, its accuracy can hold up fairly well when answering more and more complex questions. However, anyone pursuing higher levels of math looking to Chatgpt to help answer their homework questions will find that in many instances the results it produces contains several flaws.\n \nHere, the LLM produced a correct final answer (following the proper operations and row calculations) when finding solutions for a given matrix\n\n\n\nI won‚Äôt bother showing all the rest of the calculations performed‚Ä¶this is just to show how extensive and complex the problem becomes. The point here is this:\n\nThe final solution was completely wrong, simply because it performed a very basic arithmetic operation incorrectly (adding instead of subtracting)!"
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#the-problem",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#the-problem",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "The Problem",
    "text": "The Problem\nThe reason it may fail when asked these more complex questions is not necessarily because the question itself is difficult, given that it‚Äôs is pulling from a wide range of textual information from people who‚Äôve answered-or attempted to answer-questions like the one you‚Äôre trying to answer. And, as we showed before, it ‚Äúknows‚Äù how to answer simple math questions. What does become more problematic is when these simple calculations are incorporated within more complicated contexts. When trying to answer, it might discover linguistic patterns and information that are improperly aligned, given the higher complexity of the question. In turn, the model might generate incorrect answers or contain inaccuracies in the event it‚Äôs trained on these conflicting information patterns.\n\nHere, it spits out the correct solution when you target its mistake and re-prompt; again, this wasn‚Äôt due to a gap in understanding, but simply because LLMs aren‚Äôt embedded with their own, personal calculators that they can rely on."
  },
  {
    "objectID": "posts/Post1_Feb_3-9_Week1/Post_1.html#soabandon-chatgpt",
    "href": "posts/Post1_Feb_3-9_Week1/Post_1.html#soabandon-chatgpt",
    "title": "Chatgpt: Can It Really Perform Simple Calculations?",
    "section": "So‚Ä¶Abandon ChatGPT?",
    "text": "So‚Ä¶Abandon ChatGPT?\nThis, by no means, calls for the end of ChatGPT-at least as far as some basic homework help or concept understanding. It can still be a useful tool when attempting to work out bigger picture concepts, and you can certainly always ask ChatGPT explain its own mistakes by prompting it to go through its own reasoning step-by-step.\n*Insert pic of prompting chat: ‚Äúdid you do something wrong‚Äù etc/\nThis experiment was not necessarily meant to expose the shortcomings of the model and contribute to the anti-AI narrative (whether or not that‚Äôs actually a founded argument‚Ä¶); what it does do is serve as an example of some of the current ways in which ChatGPT learns and processes information, and how that differ from human reasoning!"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/Post2_Feb_3-9_Week1/Post2.html",
    "href": "posts/Post2_Feb_3-9_Week1/Post2.html",
    "title": "Language Mdoels & Word Prediction",
    "section": "",
    "text": "A ‚ÄúLarge Language Model‚Äù or LLM is one that can recieve textual input and can then generate words and sentences by predicting what the next word might be. Certain words are essentially assigned a probability according to the context of the input (i.e.¬†phrase, sentence, paragraph, etc.) and then generates following ‚Äúhigh-ranked words‚Äù (Wolfram) in order to fulfill the sentence.\n*insert ‚Äúhi what‚Äôs up‚Äù prompt\nThese models consist of stacks or blocks containing levels of encoders and decoders, and each successive model varies in how high these stacks are. One can go on and on into the technicality of its structure and ability to generate, but the point here is this: these models function by producing a series of tokens, originally starting with the input from a human asking, say, a question, and each successive output is a token, which is then taken as another input, and the cycle repeats, until a fully fledged answer is more or less attained.\n**insert pic here"
  },
  {
    "objectID": "posts/Post2_Feb_3-9_Week1/Post2.html#what-exactly-is-a-large-language-model",
    "href": "posts/Post2_Feb_3-9_Week1/Post2.html#what-exactly-is-a-large-language-model",
    "title": "Language Mdoels & Word Prediction",
    "section": "",
    "text": "A ‚ÄúLarge Language Model‚Äù or LLM is one that can recieve textual input and can then generate words and sentences by predicting what the next word might be. Certain words are essentially assigned a probability according to the context of the input (i.e.¬†phrase, sentence, paragraph, etc.) and then generates following ‚Äúhigh-ranked words‚Äù (Wolfram) in order to fulfill the sentence.\n*insert ‚Äúhi what‚Äôs up‚Äù prompt\nThese models consist of stacks or blocks containing levels of encoders and decoders, and each successive model varies in how high these stacks are. One can go on and on into the technicality of its structure and ability to generate, but the point here is this: these models function by producing a series of tokens, originally starting with the input from a human asking, say, a question, and each successive output is a token, which is then taken as another input, and the cycle repeats, until a fully fledged answer is more or less attained.\n**insert pic here"
  },
  {
    "objectID": "posts/Post2_Feb_3-9_Week1/Post2.html#heres-an-example-of-how-this-works",
    "href": "posts/Post2_Feb_3-9_Week1/Post2.html#heres-an-example-of-how-this-works",
    "title": "Language Mdoels & Word Prediction",
    "section": "Here‚Äôs an Example of How This Works:",
    "text": "Here‚Äôs an Example of How This Works:\nWe might define the input with the following sentence:\n\ninput_text = 'The man walked the cat'\n\n\nimport os\n\n\nfrom transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\nfrom bertviz import model_view, head_view\nimport torch\nimport transformers\n\n\n\ncache_dir='/Commjhub/HF_cache'\nutils.logging.set_verbosity_error()  # Suppress standard warnings\n\nmodel_name = 'gpt2'\n\ngpt2 = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir,\n                                            return_dict_in_generate=True)\n\nmodel = AutoModel.from_pretrained(model_name, \n                                  cache_dir=cache_dir,\n                                  output_attentions=True,\n                                 )  \ntokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n\nThe next step is to then tokenize the text; why? Because the model operates on tokens, meaning the smallest interpretable unit, not necessarily whole words.\n\ninputs = tokenizer.encode(input_text, return_tensors='pt') \n\nWith that, the model will then process this text and attribute attention weights to the tokens of the tokenized text.\n\n# Run model\noutputs = model(inputs) \n\n# Retrieve attention from model outputs\nattention = outputs[-1]  \n\n# Convert input ids to token strings\ntokens = tokenizer.convert_ids_to_tokens(inputs[0])  \n\n\nhead_view(attention, tokens)\n\n\n\n\n      \n        \n            \n                Layer: \n                \n            \n            \n        \n    \n\n\n\n\n\nThe probabilities retrieved and attributed to each of the following tokens can be described with the following code:\n\nwith torch.inference_mode():\n  outputs2 = gpt2(inputs)\n\nnext_token_logits = outputs2.logits[0, -1, :]\n\nnext_token_probs = torch.softmax(next_token_logits, -1)\n\ntopk_next_tokens= torch.topk(next_token_probs, 5)\n\nfor idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values):\n    print(f\"{tokenizer.decode(idx): &lt;20}{prob:.1%}\")\n\nwalk                18.7%\n out                12.7%\n,                   6.3%\n to                 4.9%\n and                4.4%\n\n\nThis simple experiment demonstrates how prompting and generation works with respect to LLMs! While it appears like magic, beneath the surface lies the intelligent design of an algorithm that relies on statistical concepts and text data processing, utilizing a series of inputs and outputs to produce some smart-sounding text!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Model Comparison\n\n\n\n\n\n\nLLMs\n\n\nopen weight models\n\n\nprompting\n\n\n\nTesting out Different Open Weight Models\n\n\n\n\n\nMar 10, 2025\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Get the Best Response\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nprompt engineering\n\n\n\nPrompt Engineering & Response Optimization\n\n\n\n\n\nFeb 10, 2025\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nChatgpt: Can It Really Perform Simple Calculations?\n\n\n\n\n\n\n-generative AI -Prompting -Logic, calculations\n\n\n\nUsing Chatgpt to answer math problems\n\n\n\n\n\nFeb 3, 2025\n\n\nAn LLM user\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Mdoels & Word Prediction\n\n\n\n\n\n\nLLMs\n\n\nword prediction\n\n\nprobabilities\n\n\n\nHow Do Large Language Models Generate Answers\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Post3_Feb_10-16_Week2/Post3.html",
    "href": "posts/Post3_Feb_10-16_Week2/Post3.html",
    "title": "How to Get the Best Response",
    "section": "",
    "text": "AI has obviously come a long way in being able to generate accurate responses that fit the bill in terms of what the user is looking for. And, ceratinly, its responses can become even more complex and thorough, depending on just how extensive the user wants the answer to be.\n\nPrompts like the one above (‚Äúgive me a grocery list‚Äù) provide enough detail and context for the model to spit out an answer in line with what a typical response might be, and sounds quite convincing to the average user. But what if you‚Äôre alergic to, say, bell peppers? Well it wouldn‚Äôt have the ability to know unless you specfied of course what it is you can‚Äôt eat. And while it appers to be mostly magic in how much it can guestimate in order to generate accurate responses, it can only go so far unless provided the right context."
  },
  {
    "objectID": "posts/Post3_Feb_10-16_Week2/Post3.html#prompting",
    "href": "posts/Post3_Feb_10-16_Week2/Post3.html#prompting",
    "title": "How to Get the Best Response",
    "section": "",
    "text": "AI has obviously come a long way in being able to generate accurate responses that fit the bill in terms of what the user is looking for. And, ceratinly, its responses can become even more complex and thorough, depending on just how extensive the user wants the answer to be.\n\nPrompts like the one above (‚Äúgive me a grocery list‚Äù) provide enough detail and context for the model to spit out an answer in line with what a typical response might be, and sounds quite convincing to the average user. But what if you‚Äôre alergic to, say, bell peppers? Well it wouldn‚Äôt have the ability to know unless you specfied of course what it is you can‚Äôt eat. And while it appers to be mostly magic in how much it can guestimate in order to generate accurate responses, it can only go so far unless provided the right context."
  },
  {
    "objectID": "posts/Post3_Feb_10-16_Week2/Post3.html#taking-guesses-based-on-context",
    "href": "posts/Post3_Feb_10-16_Week2/Post3.html#taking-guesses-based-on-context",
    "title": "How to Get the Best Response",
    "section": "Taking ‚Äúguesses‚Äù based on context",
    "text": "Taking ‚Äúguesses‚Äù based on context\nAs the AI is fed more and more context, the responses become more and more aligned with what you‚Äôre looking for. As per the image above, you can‚Äôt just ask it to guess what you‚Äôre allergic to (in this case, we decided bell peppers), because it would have no way of knowing otherwise, unless you provided it some sort of context or information (which it doesn‚Äôt have, prior to you prompting it this request).\n\nMaking it take guesses (by giving it some clues)\n\n\nwhat is something you could tell the model to narrow its guesses, without outright telling the model? Ex: I have an uncommon allergy; here‚Äôs what I‚Äôm NOT allergic to, etc‚Ä¶.\n\n\n\nStill ‚Äúoff the mark,‚Äù as it puts it‚Ä¶but a step closer; it took guesses based on what are some common allergies, indicating both the power of context & data collection (from the internet), to generate more accurate responses.\n\n\n\nYou can continue offering more refined requests and prompts to further its guessing ability, i.e‚Ä¶.\n\n\n\nOkay‚Ä¶\nSo it‚Äôs still off‚Ä¶of course, models can only go far in their guesses! And depending on the model, different/successive ones will be more or less successful than the last. Further, depending on the data it‚Äôs trained on (again, in this case it‚Äôs the Internet), it will pull responses that are more probalistic or common than others.\n\nAs shown above, it can only go so far depending on the context it‚Äôs given! So how do we improve responses such that answers are more accurate off the bat? Enter here: prompt engineering, and it‚Äôs five main associated principles:\n\nThe Five Principles of Prompting\n\n\nGive direction (describe desired style)\n\n\nSpecify format (define what rules to follow)\n\n\nProvide examples (include a diverse set of cases)\n\n\nEvaluate quality (identify errors)\n\n\nDivide labor (split tasks into mulitple steps)\n\n\n\nExample: Divide labor\nThis principle refers to the process by which you break up your prompt into several ‚Äúmini prompts‚Äù or requests with more specificity. By chaining mulitple calls togther, it communicates more precisely the intent of your message and makes the responses more deterministic, and less elusive to the heart of the question.\n\nThe goal is to prompt the model and have it list out its steps (chain of thought reasoning)\n\n\n\nYou then might ask the model to generate a new list based on this critera!\n\n\n\nNoticed how it modified the list based on these new, more refined requests!"
  },
  {
    "objectID": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html",
    "href": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html",
    "title": "Model Comparison",
    "section": "",
    "text": "Large Language models come in various forms of accessiblity: closed-source, open-source, and open weight models. Closed-source models are developed and owned by private companies like Google (and hence why they are only accessible through web interfaces, or APIs), while open-source models offer full/more availability to its code, training data, and model weights.\n\n\nWhere does that leave the term, open weight models? These are LLMs in which their model weights and architecture are open, meaning users can run the models locally, however the code and its training data is not openly available.\n\n\n\nOf course, each form of model accessibility comes with its advantages and disadvantages depending on the desires of the user. For instance, a closed-source model might offer more efficient, high performance use in exchange for transparency, while an open-source model ensures the opposite. Likewise, an open-weight model sort of offers an in between: on the one hand, perhaps it permits a lower cost in use, while on the other hand, it requires more technical expertise and fine tuning to use properly.\nAll of that being said, open-weight models provide good subjects for experimentation because of their greater accessibility. So with that, a good topic for discussion is comparing various open-weight models, each of which are at least slightly different in terms of their updates, manufacturers, efficiency, number of tokens it‚Äôs trained on, parameters, and so forth.\n\n\n\nThanks to platforms like Ollama-a free, open-source and user-friendly interface by which we can access and download these models-we can compare and contrast some of these models with simple prompting measures, as shown below‚Ä¶.\n\nimport openai\nimport requests\nimport json\nfrom openai import OpenAI\nimport pandas as pd\n\n\nlist_models_url = \"http://10.30.16.100:11434/api/tags\"\n\n\nresp=requests.get(list_models_url)\nmodels_json = resp.json()\n\n\nmodel_list=[]\nfor model in models_json['models']:\n    model_list.append(model['name'].replace(':latest',''))\n\nlen(model_list)\n\n94"
  },
  {
    "objectID": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html#comparing-two-random-models",
    "href": "posts/Post4_Feb_10-16/Post4_Feb_10-14.html#comparing-two-random-models",
    "title": "Model Comparison",
    "section": "Comparing Two Random Models:",
    "text": "Comparing Two Random Models:\nAfter creating a list of models given by the Ollama library, let‚Äôs randomly choose any of them (94), and compare the prompted responses of the two models.\n\nimport random\n\n\nmodel1=random.choice(model_list)\n\n\nmodel2=random.choice(model_list)\n\n\nprint(model1,',', model2)\n\nllama3-gradient , openchat\n\n\n\nclient = OpenAI(\n    base_url = 'http://10.30.16.100:11434/v1',\n    api_key='ollama',\n)\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an MIT math professor.\"},\n    {\"role\": \"user\", \"content\": \"Explain orthogonality in simple terms to a high schooler\"},\n\n  ]\n\n\nPrompt 1:\n\nThe first prompt asks to ‚Äúexplain orthogonality in simple terms to a high school student‚Äù as an ‚ÄúMIT math professor.‚Äù\n\n\n\nllama3-gradient:\n\nresponse1 = client.chat.completions.create(\n  model=\"llama3-gradient\",\n  messages=messages,\n  max_tokens=1000\n)\nprint(response1.choices[0].message.content)\n\nHello there! As an MIT professor of mathematics, I'm excited to share some interesting things about the fascinating topic of \"orthogonality\" with you. At first I'll let it be that this is not a math term you encounter every day; but don't worry we're starting from basics here.\n\nIn high school, you are probably used to studying vectors using x and y axes on graphs. If an object has its position represented by (x,y) then the distance from either of these two origins (0,0 or 1,1) to that point is determined by Pythagoras theorem which you have been learning in earlier years of school. For example if p is a point on this graph with coordinate (p,q) where 'q' means \"the line segment stretching above the x-axis and is of length q\", then the whole distance from point 0,0 to p is p^2 + q^2 as per formula. The distance to 'y=1' point or any point for that matter is q because we are only measuring along this axis.\n\nNow lets take two lines. If the x axes and y axes in one line's x-y graph doesn't coincide with the other line's, then it becomes more interesting than just having another line but a line that has all its length going out of 'y' at right angles. That is called an angle. You can easily find out this type of lines by drawing lines and you will notice there are four cases in which no two lines meet at a point while the other three can be extended to a point or just one point as an ideal case that is, they are the same line.\n\nNow if two non-parallel lines do just overlap somewhere (in the third and fourth type of case) and you want to measure how much those lines diverge from each other then we take that common point which means meeting points between these two. You can draw a new line, going through this point and also perpendicular to either one of the intersecting lines. Draw a separate x-y axis for this line too. For ease in this problem assume one of the lines was a right angle (i.e., 1/4 or 3/4) which makes calculating the angles as trivial. Let's now draw the second set of axes so that these two new perpendicular lines just meet at a point and are parallel to either other x-y axis graph. Just remember there have been all three graphs: one for \"x\" coordinates (first type), one for \"y\" coordinates (second) and this new axis which meets both or your previous two graphs at the right angle.\n\nLet's take up an example of a first person who is walking north-south on either side of '0' line. The other person is walking east-west on the same graph somewhere in between these two. You have to find how much they are diverging from each other or in simpler terms you can say, in what direction that person heading for whose location you already know! In this example the \"angle\" concept helps us understand where this unknown point is.\n\nNow all of you should be knowing that this is actually a simple form of trigonometry. It's based on triangles and right angles. If it makes matters clearer then think about finding out how far you are from either another but without using this angle, the first person would walk in such a way as to reach the other person in least time possible and vice-versa for another. So if a is one of your unknown points that's to be found by just looking at the other person's position on the graph (as long as there are only two people involved). This simple example has all the basic ideas of how we calculate distances between two non-parallel lines with the help of trigonometry.\n\nSo, what about our concept \"orthogonality\"? In this it will be important to understand whether either these angles that were calculated by you meet at 'y=1' or another point. You can check visually that when both are 2nœÄ then they just cut each other (as long as a is the same type of coordinate for both). This is true for - nœÄ as well with some minor change and so we get orthogonality condition as:\n\na + b = nœÄ\n\nNow let's go back to our simple problem of two people walking together. You are at '0' on line which is your \"coordinate\". The other person can walk in such a way that his distance from you is 1. In this case the angle where a new axis crosses them will be pi/2 which again makes b equal to another type of coordinate (q) on this graph but now with + sign and so:\n\na - b = ¬± œÄ/2\n\nSo, we get the simple \"orthogonality condition\" as an if-then statement for a specific set of coordinates. But don't think too much about\n\n\n\n\nOpenchat:\n\nresponse2 = client.chat.completions.create(\n  model=\"openchat\",\n  messages=messages,\n  max_tokens=1000\n)\nprint(response2.choices[0].message.content)\n\n Orthogonality is a concept in mathematics that has to do with the angles between two lines, line segments, or other geometric shapes. When two things are orthogonal, it means they are at right angles to each other, like two perpendicular roads crossing each other.\n\nIn simpler terms, imagine you're walking along two parallel streets that cross each other at a right angle. They are orthogonal because they form a 90-degree angle between them. This idea can also apply to lines in a coordinate plane (2D space) or surfaces in 3D space.\n\nIn mathematics and engineering, we often work with vectors - these could be physical forces like you push a car or abstract quantities like temperature or stock prices. Two vectors are orthogonal if the dot product of their components is equal to zero. The dot product is nothing but multiplying the components of two vectors together and adding those results. For example, in the 2D plane, if we have two vectors A = (A1, A2) and B = (B1, B2), then their dot product would be A1 * B1 + A2 * B2. If this is zero, the vectors are orthogonal!\n\n\n\n\nDifferences‚Ä¶\nThe two randomly selected models here were ‚Äúllama3-gradient‚Äù and ‚Äúopenchat‚Äù. According to the Ollama library descriptions of these models, llama3-gradient is as follows:\n\nAnd the second selected model, openchat is described as:\n\n\nObservations?\n\nThe first model llama3-gradient produces a response that is far more extensive, in depth in terms of content, and makes a more robust attempt to mimic the logic of a professor while staying true to the request that it convey the answer to a lower level student.\n\n\nThe second model offers a much more simple, concise answer. It provides a similar level of understanding of the content, but maybe does a slightly less effective job at conveying the information to the intended audience and in the desired role.\n\n\n\n\nLet‚Äôs try another prompt:\n\nPrompt 2: ‚ÄúWrite a short poem‚Äù as a ‚Äúfamous 18th century English poet‚Äù\n\n\nmessages2 = [\n    {\"role\": \"system\", \"content\": \"You are a famous 18th century English poet.\"},\n    {\"role\": \"user\", \"content\": \"Write a short poem.\"},\n\n  ]\n\n\nllama3-gradient:\n\nresponse3 = client.chat.completions.create(\n  model=\"llama3-gradient\",\n  messages=messages2,\n  max_tokens=1000\n)\nprint(response3.choices[0].message.content)\n\nIn twilight's hush, where shadows play,\nAmidst a glimmering world of day's decay,\nThe moon, as if with golden tincture bright,\nDoes pour her silvery beams, like liquid light.\nOn dewy meadows, lily-pale and fair,\nWhere night-blooming blooms the jasmine sweet\nAnd starry eyes with beauty sleep, in whispers play.\n\nA silence falls as nature's voice is still,\nAs darkness gathers to a whispered tale,\nAt daybreak, ere the sun takes hold of light,\nThe stars themselves begin to murmur low and chill.\nO'er fields where night-shades bloom not to be told\nThis fleeting life; then fades, nor can we tell.\n\n\n\n\nopenchat:\n\nresponse4 = client.chat.completions.create(\n  model=\"openchat\",\n  messages=messages2,\n  max_tokens=1000\n)\nprint(response4.choices[0].message.content)\n\n Upon the meadow where the daffodils reside,\nI wander free in mind and stride.\nA world of verse and beauty, my muse finds,\nAs nature's splendor whispers to my kind.\n\nThe sun ascends, a golden glow it casts,\nAnd through the sky, the birds release their laughter.\nIn these vast fields where dreams and visions bloom,\nI find my voice, as words come like thunder from the looming clouds.\n\nAn ode to happiness, an elegy of woe,\nThe hearts of mortals moved by my poet's flow.\nYet even I, this bard with quill in hand,\nMust sometimes pause and wonder if transcendent thoughts I understand?\n\nIn these fleeting moments, as the seasons change,\nI write with fervor to record life's nuances, its page by age.\nFor though my verses may be written in ink,\nThe passion they convey, no ink could ever think.\n\n\n\nLLama: the model produces a shorter poem, and the subject is relatively impersonal. The rhymes also only hold for the first four lines (in ABAB kind of style), and it loses this rhythm as the poem goes on.\n\n\nOpenchat: The openchat model did a better job of maintaining the rhyme (with the exception of the second verse), and took a first person perspective.\n\nIt‚Äôs interesting to note‚Ä¶ how the two models provided different interpretations depending on the prompt; llama3-gradient offered a lengthy response to the first prompt, and did a better job at adhering to the conditions of a prompt that was more technical in nature. The second prompt, which was more subjective, was given a more thorough answer by openchat. What it‚Äôs interesting is how a context window, or type of data a model is trained on, will greatly affect an output depending on what the prompt asks for."
  }
]